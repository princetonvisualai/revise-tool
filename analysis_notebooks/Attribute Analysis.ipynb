{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute Analysis\n",
    "The following notebook can analyze a given attribute with any number of expected values (2+), and will perform metrics att_siz, att_cnt, att_dis, att_clu, att_scn generalized to any attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "1. Fill in the dataset in section 1.1\n",
    "\n",
    "2. Run all cells\n",
    "\n",
    "3. Look at the summary pdf generated AND/OR explore each metric below.\n",
    "    - All metrics are identified by a short keyword, and consist of a \"Setup\" and \"Analyses\" portion. The \"Setup\" portion contains code that does not need to be modified unless customization is needed, and the \"Analyses\" portion provides an interactive display of the results.\n",
    "    \n",
    "## Table of Contents\n",
    "1. [Initial Setup](#setup) <br/>\n",
    "    1.1 [Dataset](#dataset) <br/>\n",
    "2. att_siz Metric: [Distance from center, size, attribute label inference](#att_size)<br/>\n",
    "    2.1 [Setup](#att_size_setup)<br/>\n",
    "    2.2 [Analyses](#att_size_analyses)\n",
    "3. att_cnt Metric: [Object occurrences and cooccurrences](#att_cnt)<br/>\n",
    "    3.1 [Setup](#att_cnt_setup)<br/>\n",
    "    3.2 [Analyses](#att_cnt_analyses)\n",
    "4. att_dis Metric: [Distance from object as proxy for interaction](#att_dis)<br/>\n",
    "    4.1 [Setup](#att_dis_setup)<br/>\n",
    "    4.2 [Analyses](#att_dis_analyses)\n",
    "5. att_clu Metric: [Linearly seperate objects by attribute](#att_clu)<br/>\n",
    "    5.1 [Setup](#att_clu_setup)<br/>\n",
    "    5.2 [Analyses](#att_clu_analyses)\n",
    "6. att_scn Metric [Scenes](#att_scn)<br/>\n",
    "    6.1 [Setup](#att_scn_setup)<br/>\n",
    "    6.2 [Analyses](#att_scn_analyses)\n",
    "7. [Setting up summary pdf](#summarypdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "<a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import argparse\n",
    "import os\n",
    "try: # only change dir if necessary\n",
    "    import datasets\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir(os.pardir)\n",
    "    import datasets\n",
    "import pickle\n",
    "import itertools\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import PIL.Image\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from math import sqrt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "from scipy.spatial.distance import squareform\n",
    "import pycountry\n",
    "from geonamescache import GeonamesCache\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "import textwrap\n",
    "import matplotlib.patches as mpatches\n",
    "import operator\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import imageio\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, HBox, Layout\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "from matplotlib.transforms import Bbox\n",
    "from IPython.display import clear_output\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = sns.color_palette('Set2')\n",
    "SAME_EXTENT = (-0.5, 6.5, -0.5, 6.5)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not os.path.exists(\"dataloader_files\"):\n",
    "    os.mkdir(\"dataloader_files\")\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook\n",
    "def hide_toggle(for_next=False, toggle_text='Toggle show/hide'):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide helper functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder(num, folder):\n",
    "    if not os.path.exists(\"results/{0}/{1}\".format(folder, num)):\n",
    "        os.mkdir(\"results/{0}/{1}\".format(folder, num))\n",
    "    file = open(\"results/{0}/{1}/results.txt\".format(folder, num), \"w\")\n",
    "    return file\n",
    "\n",
    "# Projecting a set of features into a lower-dimensional subspace with PCA\n",
    "def project(features, dim):\n",
    "    standardized = StandardScaler().fit_transform(features)\n",
    "    pca = PCA(n_components=dim)\n",
    "    principalComponents = pca.fit_transform(X=standardized)\n",
    "    return principalComponents\n",
    "\n",
    "# Calculating the binomial proportion confidence interval\n",
    "def wilson(p, n, z = 1.96):\n",
    "    denominator = 1 + z**2/n\n",
    "    centre_adjusted_probability = p + z*z / (2*n)\n",
    "    adjusted_standard_deviation = sqrt((p*(1 - p) + z*z / (4*n)) / n)\n",
    "    \n",
    "    lower_bound = (centre_adjusted_probability - z*adjusted_standard_deviation) / denominator\n",
    "    upper_bound = (centre_adjusted_probability + z*adjusted_standard_deviation) / denominator\n",
    "    return (lower_bound, upper_bound)\n",
    "\n",
    "def country_to_iso3(country):\n",
    "    missing = {'South+Korea': 'KOR',\n",
    "            'North+Korea': 'PRK',\n",
    "            'Laos': 'LAO',\n",
    "            'Caribbean+Netherlands': 'BES',\n",
    "            'St.+Lucia': 'LCA',\n",
    "            'East+Timor': 'TLS',\n",
    "            'Democratic+Republic+of+Congo': 'COD',\n",
    "            'Swaziland': 'SWZ',\n",
    "            'Cape+Verde': 'CPV',\n",
    "            'C%C3%B4te+d%C2%B4Ivoire': 'CIV',\n",
    "            'Ivory+Coast': 'CIV',\n",
    "            'Channel+Islands': 'GBR'\n",
    "            }\n",
    "    try:\n",
    "        iso3 = pycountry.countries.search_fuzzy(country.replace('+', ' '))[0].alpha_3\n",
    "    except LookupError:\n",
    "        try:\n",
    "            iso3 = missing[country]\n",
    "        except KeyError:\n",
    "            iso3 = None\n",
    "    return iso3\n",
    "\n",
    "def full_extent(ax, pad=0.0):\n",
    "    \"\"\"Get the full extent of an axes, including axes labels, tick labels, and\n",
    "    titles.\"\"\"\n",
    "    # For text objects, we need to draw the figure first, otherwise the extents\n",
    "    # are undefined.\n",
    "    ax.figure.canvas.draw()\n",
    "    items = ax.get_xticklabels() + ax.get_yticklabels() \n",
    "    items += [ax, ax.title]\n",
    "    bbox = Bbox.union([item.get_window_extent() for item in items])\n",
    "\n",
    "    return bbox.expanded(1.0 + pad, 1.0 + pad)\n",
    "\n",
    "def display_filepaths(filepaths, width=100, height=100):\n",
    "    try: \n",
    "        sidebyside = widgets.HBox([widgets.Image(value=open(filepath, 'rb').read(), format='png', width=width, height=height) for filepath in filepaths], layout=Layout(height='{}px'.format(height)))\n",
    "        display(sidebyside)\n",
    "    except FileNotFoundError: \n",
    "        print('Filepath not found. If using CocoDatasetNoImages Class, some functionality is not available.')\n",
    "\n",
    "def dec_to_show(p):\n",
    "    if p < .001:\n",
    "        return '{:0.3e}'.format(p)\n",
    "    else:\n",
    "        return round(p, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Fill in below with dataset and file path names\n",
    "<a id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([ \n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "dataset = datasets.CoCoDataset(transform_train)\n",
    "folder_name = 'coco_example'\n",
    "\n",
    "# dataset = datasets.OpenImagesDataset(transform_train)\n",
    "# folder_name = 'openimages_supp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = '1_pager_attr'\n",
    "os.system(\"rm -r results/{0}/{1}\".format(folder_name, save_loc))\n",
    "file = folder(save_loc, folder_name)\n",
    "first_pass = True\n",
    "to_write = {}\n",
    "if not os.path.exists(\"checkpoints/{}\".format(folder_name)):\n",
    "    os.mkdir(\"checkpoints/{}\".format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = None\n",
    "\n",
    "distances = pickle.load(open(\"results/{}/att_dis.pkl\".format(folder_name), \"rb\"))\n",
    "sample_file = distances[0][0][0][3]\n",
    "if not os.path.exists(sample_file):\n",
    "    assert data_folder is not None, \"initialize data_folder with folder path of your data\"\n",
    "    dataset.init_folder_path(data_folder)\n",
    "    print(\"overwriting from_path() function\")\n",
    "    dataset.from_path = dataset.from_path_prerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = dataset.attribute_names\n",
    "num_attrs = len(attr_names)\n",
    "ordinal = dataset.ordinal\n",
    "axis = []\n",
    "if ordinal:\n",
    "    axis = dataset.axis\n",
    "    assert len(axis) != 0, \"ensure self.axis is defined in dataset class\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# att_siz Metric: Distance from center, size, attribute label inference\n",
    "<a id=\"att_siz\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"att_siz_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide att_siz code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pickle.load(open(\"results/{}/att_siz.pkl\".format(folder_name), \"rb\"))\n",
    "sizes = [[y[0] for y in x] for x in info['sizes']]\n",
    "sizes_paths = [[y[1] for y in x] for x in info['sizes']]\n",
    "dists = [[y[0] for y in x] for x in info['distances']]\n",
    "dists_paths = [[y[1] for y in x] for x in info['distances']]\n",
    "tiny_sizes = [[y[0] for y in x] for x in info['tiny_sizes']]\n",
    "tiny_paths = [[y[1] for y in x] for x in info['tiny_sizes']]\n",
    "no_faces = [[y[0] for y in x] for x in info['noface_sizes']]\n",
    "no_paths = [[y[1] for y in x] for x in info['noface_sizes']]\n",
    "\n",
    "scenes = [None]*num_attrs\n",
    "for attr in range(num_attrs):\n",
    "    try:\n",
    "        scenes[attr]=np.array(list(itertools.chain.from_iterable([chunk[2] for chunk in no_faces[attr]])) + list(itertools.chain.from_iterable([chunk[1] for chunk in tiny_sizes[attr]])))\n",
    "    except TypeError:\n",
    "        if len(tiny_sizes)==0:\n",
    "            print(\"There are no images with faces too small to label for group: {0}\".format(attr_names[attr]))\n",
    "        else:\n",
    "            print(\"There are no images where a face is not detected for group: {0}\".format(attr_names[attr]))\n",
    "    tiny_sizes[attr] = [chunk[0] for chunk in tiny_sizes[attr]]\n",
    "    no_faces[attr] = [chunk[0] for chunk in no_faces[attr]]\n",
    "    \n",
    "info = pickle.load(open('util_files/places_scene_info.pkl', 'rb'))\n",
    "idx_to_scene = info['idx_to_scene']\n",
    "idx_to_scenegroup = info['idx_to_scenegroup']\n",
    "sceneidx_to_scenegroupidx = info['sceneidx_to_scenegroupidx']\n",
    "\n",
    "xaxis = [idx_to_scenegroup[i] for i in range(len(idx_to_scenegroup))]\n",
    "xaxis = ['\\n'.join(textwrap.wrap(chunk, width=30)) for chunk in xaxis]\n",
    "barWidth = .4\n",
    "fontsize = 15\n",
    "\n",
    "r1 = np.arange(len(idx_to_scenegroup))\n",
    "r1 = r1 * ((barWidth * num_attrs) + .2)\n",
    "\n",
    "scenes = [np.bincount(scenes[i]) for i in range(num_attrs)]\n",
    "total_images = np.sum(scenes)\n",
    "scenes_ratio = [scenes[i]/dataset.num_attribute_images[i] for i in range(num_attrs)]\n",
    "all_sizes = [tiny_sizes[i]+no_faces[i]+sizes[i] for i in range(num_attrs)]\n",
    "all_paths = [tiny_paths[i]+no_paths[i]+sizes_paths[i] for i in range(num_attrs)]\n",
    "\n",
    "def numbers_where_attribute_inferred():\n",
    "    tiny = [len(tiny_sizes[i]) for i in range(num_attrs)]\n",
    "    noface = [len(no_faces[i]) for i in range(num_attrs)]\n",
    "    original = [tiny[i]+noface[i]+len(sizes[i]) for i in range(num_attrs)]\n",
    "    \n",
    "    total_original = np.sum(original)\n",
    "    if total_original >0:\n",
    "        print(\"Total labelled images: {0},\".format(total_original))\n",
    "    for i in range(num_attrs):\n",
    "        if original[i]>0:\n",
    "            print(\"{0} were {1}\".format(original[i], attr_names[i]))\n",
    "      \n",
    "    max_original = 0\n",
    "    max_attribute = -1\n",
    "    for attr in range(num_attrs):\n",
    "        if not math.isnan(original[attr]/total_original) and original[attr]/total_original > max_original:\n",
    "            max_original = original[attr]/total_original\n",
    "            max_attribute = attr\n",
    "    if max_attribute > -1:\n",
    "        print(\"{0} is assigned to {1}% labelled images in the dataset, and is the most commonly assigned label\".format(attr_names[max_attribute], round(max_original, 4)*100))  \n",
    "    print()\n",
    "    \n",
    "    tiny_total = np.sum(tiny)\n",
    "    if tiny_total > 0:\n",
    "        print(\"Discarded {0} images for being too small,\".format(tiny_total))\n",
    "    for i in range(num_attrs):\n",
    "        if tiny[i]>0:\n",
    "            print(\"{0} were {1}\".format(tiny[i], attr_names[i]))\n",
    "        \n",
    "    max_original = 0\n",
    "    max_attribute = -1\n",
    "    for attr in range(num_attrs):\n",
    "        if not math.isnan(tiny[attr]/tiny_total) and tiny[attr]/tiny_total > max_original:\n",
    "            max_original = tiny[attr]/tiny_total\n",
    "            max_attribute = attr\n",
    "    if max_attribute > -1:\n",
    "        print(\"{0} is assigned to {1}% labelled images where a person is too small to properly see, and is the most commonly assigned label among such images\".format(attr_names[max_attribute], round(max_original, 4)*100))\n",
    "    print()\n",
    "    \n",
    "    noface_total = np.sum(noface)\n",
    "    if noface_total >0:\n",
    "        print(\"Discarded {0} images for having no face detected,\".format(noface_total))\n",
    "    for i in range(num_attrs):\n",
    "        if noface[i] >0:\n",
    "            print(\"{0} were {1}\".format(noface[i], attr_names[i]))\n",
    "        \n",
    "    max_original = 0\n",
    "    max_attribute = -1\n",
    "    for attr in range(num_attrs):\n",
    "        if not math.isnan(noface[attr]/noface_total) and noface[attr]/noface_total > max_original:\n",
    "            max_original = noface[attr]/noface_total\n",
    "            max_attribute = attr\n",
    "    if max_attribute > -1:\n",
    "        print(\"{0} is assigned to {1}% labelled images where a face is not detected, and is the most commonly assigned label among such images\".format(attr_names[max_attribute], round(max_original, 4)*100))\n",
    "       \n",
    "    labelled = [tiny[i]+noface[i] for i in range(num_attrs)]\n",
    "    max_labelled = np.argmax(labelled)\n",
    "    labelled_others = np.sum(labelled)\n",
    "    prob = labelled[max_labelled] / labelled_others\n",
    "    prob_statement = \"Probability image is labeled {0} when it should not be, i.e. given there's no face detected or person is too small: {1}\".format(attr_names[max_labelled], round(prob, 4))\n",
    "    if (prob < .45 or prob > .55) and first_pass:\n",
    "        to_write[0] = [\"(att_siz) \" + prob_statement]\n",
    "    print()\n",
    "    print(prob_statement)\n",
    "    \n",
    "def scenes_where_no_face(disp_plt=False):\n",
    "    barWidths = [barWidth * i for i in range(num_attrs)]\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    r = [r1]\n",
    "    for i in range(1, num_attrs+1):\n",
    "        r.append([x + barWidth for x in r[len(r)-1]])\n",
    "        \n",
    "    scene_attr_ratios = [0 for i in range(len(scenes[0]))]\n",
    "    max_scenes = [0 for i in range(len(scenes[0]))]\n",
    "    for scene in range(len(scenes[0])):\n",
    "        max_scene = 0\n",
    "        min_scene = 1\n",
    "        max_attr = -1\n",
    "        for attr in range(num_attrs):\n",
    "            if scenes_ratio[attr][scene] > max_scene:\n",
    "                max_scene = scenes_ratio[attr][scene]\n",
    "                max_attr = attr\n",
    "            if scenes_ratio[attr][scene] < min_scene:\n",
    "                min_scene = scenes_ratio[attr][scene]\n",
    "        scene_attr_ratios[scene] = max_scene / min_scene\n",
    "        max_scenes[scene] = max_attr\n",
    "    \n",
    "    if disp_plt:\n",
    "        p_trend, scene_ord_options = [], []\n",
    "        for scene in range(len(scenes[0])):\n",
    "            scene_trend = np.array(scenes_ratio)[axis, scene]\n",
    "            _, _, _, p, _ = stats.linregress([x for x in range(len(axis))], scene_trend)\n",
    "            p_trend.append(p)\n",
    "            scene_ord_options.append((f\"{xaxis[scene]}, p={dec_to_show(p)}\", scene))\n",
    "        \n",
    "        scene_ord_options = [x for _, x in sorted(zip(p_trend, scene_ord_options))]\n",
    "        return scene_ord_options\n",
    "    else:\n",
    "        order = np.argsort(scene_attr_ratios)\n",
    "        biggest_diff_scenes = []\n",
    "        if first_pass and scene_attr_ratios[order[-1]] > 1.:\n",
    "            biggest_diff_scenes.append(\"{0} is the scene where the label of {1} is most likely to be picked over that of others\".format(xaxis[order[-1]], attr_names[max_scenes[order[-1]]]))\n",
    "        if first_pass and scene_attr_ratios[order[0]] < 1.:\n",
    "            biggest_diff_scenes.append(\"{0} is the scene where the label of {1} is most likely to be picked over that of others\".format(xaxis[order[0]], attr_names[max_scenes[order[0]]]))\n",
    "        if len(biggest_diff_scenes) > 0:\n",
    "            to_write[1] = biggest_diff_scenes\n",
    "\n",
    "        for i in range(num_attrs-1, -1, -1):\n",
    "            plt.barh(r[i], scenes_ratio[i][order], height=barWidth, color=COLORS[i], edgecolor='white', label=attr_names[i])\n",
    "        ticks = r[0]+(num_attrs/2)*barWidth\n",
    "        plt.yticks(ticks, np.array(xaxis)[order], fontsize=fontsize)\n",
    "        plt.xticks(fontsize=fontsize)\n",
    "        plt.ylabel('Scene', fontsize=fontsize)\n",
    "        plt.xlabel('Proportion of Labelled Images Discarded with this Scene', fontsize=fontsize)\n",
    "        plt.legend(loc='best', prop={'size': fontsize})\n",
    "        plt.title(\"Scenes where image was labeled when it should not have been\", fontsize=fontsize)\n",
    "        plt.tight_layout()\n",
    "        plt.gcf().subplots_adjust(bottom=0.18)\n",
    "        plt.gcf().subplots_adjust(left=0.4)\n",
    "        plt.show()\n",
    "    \n",
    "comparisons_widget = widgets.Dropdown(options=['Sizes', 'Distances', 'All sizes', 'Sizes where no face was detected'], value='Sizes')\n",
    "\n",
    "def compare_sizedist(metric):\n",
    "    def mean_and_std(data, data_type):\n",
    "        mean = [np.mean(data[i]) for i in range(num_attrs)]\n",
    "        std = [np.std(data[i]) for i in range(num_attrs)]\n",
    "\n",
    "        min_p = 100\n",
    "        one = -1\n",
    "        two = -1\n",
    "        for i in range(num_attrs):\n",
    "            for a in range(i+1, num_attrs):\n",
    "                t, p = stats.ttest_ind(data[i], data[a])\n",
    "                if p < min_p:\n",
    "                    min_p=p\n",
    "                    one = i\n",
    "                    two = a\n",
    "        p = min_p\n",
    "        \n",
    "        to_save = False\n",
    "        if metric == 'first_pass' and p < .05 and first_pass:\n",
    "            data_descrip = ''\n",
    "            if data_type == 'dists':\n",
    "                data_descrip = 'Distance from center'\n",
    "            if data_type == 'sizes':\n",
    "                data_descrip = 'Fraction of image taken up by a person'\n",
    "            to_write[2] = [\"(att_siz) {0} is different between the attributes with a p-value of {1} for the most significant pair ({2} and {3}), distribution shown below\".format(data_descrip, dec_to_show(p), attr_names[one], attr_names[two])]\n",
    "            to_save = True\n",
    "            \n",
    "        if metric == 'first_pass' or metric != 'first_pass':\n",
    "            for i in range(num_attrs):\n",
    "                histogram_a, bins_a = np.histogram(data[i], bins='auto')\n",
    "                bin_centers_a = 0.5*(bins_a[1:] + bins_a[:-1])\n",
    "                area_a = np.trapz(histogram_a, x=bin_centers_a)\n",
    "                plt.plot(bin_centers_a, histogram_a/area_a, alpha=.75, label=attr_names[i], color=COLORS[i])\n",
    "\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.xlabel('Distances' if data_type == 'dists' else 'Sizes')\n",
    "            plt.ylabel('Frequency')\n",
    "            if to_save and first_pass:\n",
    "                plt.savefig(\"results/{0}/{1}/0.png\".format(folder_name, save_loc))\n",
    "                plt.close()\n",
    "            elif metric == 'first_pass':\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "        if metric != 'first_pass':\n",
    "            for i in range(num_attrs):\n",
    "                print(\"{0}: {1} +- {2}\\n\".format(attr_names[i], round(mean[i], 4), round(std[i], 4)))\n",
    "            print(\"The smallest P value, which is between the groups {} and {}: {}\\n\".format(attr_names[one],attr_names[two], '{:0.3e}'.format(p)))\n",
    "\n",
    "    if metric == 'Sizes':\n",
    "        mean_and_std(sizes, 'sizes')\n",
    "    elif metric == 'All sizes':\n",
    "        mean_and_std(all_sizes, 'all_sizes')\n",
    "    elif metric == 'Sizes where no face was detected':\n",
    "        mean_and_std(no_faces, 'no_faces')  \n",
    "    elif metric == 'Distances':\n",
    "        mean_and_std(dists, 'dists')\n",
    "    elif metric == 'first_pass' and first_pass:\n",
    "        mean_and_std(sizes, 'sizes')\n",
    "        mean_and_std(dists, 'dists')\n",
    "\n",
    "def ordinal_sizedist(metric, perc=None, disp_plt=True):\n",
    "    def ordinal_plot(data, data_type):\n",
    "        mean_data = []\n",
    "        median_data = []\n",
    "        \n",
    "        u, n, n_i_sq, n_i_mult = 0, 0, 0, 0\n",
    "        for idx_i, i in enumerate(axis):\n",
    "            a_data = np.array(data[i])\n",
    "            \n",
    "            mean_data.append(np.mean(a_data))\n",
    "            median_data.append(np.median(a_data))\n",
    "            n += len(a_data)\n",
    "            n_i_sq += len(a_data)**2\n",
    "            n_i_mult += len(a_data)**2 * (2 * len(a_data) + 3)\n",
    "            \n",
    "            if (idx_i < len(axis)):\n",
    "                for idx_j, j in enumerate(axis[idx_i + 1:]):\n",
    "                    b_data = np.array(data[j])\n",
    "                    for dat in a_data:\n",
    "                        u += (b_data > dat).sum()\n",
    "        \n",
    "        E_U = (n**2 - n_i_sq)/4\n",
    "        Var_U = (n**2 * (2 * n + 3) - n_i_mult)/72\n",
    "        Z = (u - E_U)/(Var_U ** 0.5)\n",
    "        p_trend = stats.norm.sf(abs(Z))\n",
    "        \n",
    "        fig=plt.figure()\n",
    "        mean_ax=fig.add_subplot(111, label='mean')\n",
    "        med_ax=fig.add_subplot(111, label='median', frame_on=False)\n",
    "\n",
    "        plt_mean = mean_ax.plot(np.array(attr_names)[axis], mean_data, color=\"C0\")[0]\n",
    "        mean_ax.set_xlabel(\"Attribute\")\n",
    "        mean_ax.set_ylabel(\"Mean\")\n",
    "\n",
    "        plt_med = med_ax.plot(np.array(attr_names)[axis], median_data, color=\"C1\")[0]\n",
    "        med_ax.yaxis.tick_right()\n",
    "        med_ax.set_ylabel(\"Median\")\n",
    "        med_ax.yaxis.set_label_position(\"right\")\n",
    "        \n",
    "        mean_ax.legend([plt_mean, plt_med], [\"Mean\", \"Median\"])\n",
    "        plt.title(f\"Mean/Median {data_type} by Attribute, p={dec_to_show(p_trend)}\");\n",
    "        plt.show()\n",
    "                \n",
    "    def ordinal_image(data, metric, paths, perc):\n",
    "        perc_paths = [[] for i in axis]\n",
    "        perc_data = [[] for i in axis]\n",
    "        \n",
    "        for idx_i, i in enumerate(axis):\n",
    "            a_data = np.array(data[i])\n",
    "            arg = np.argsort(a_data)\n",
    "            if (len(arg) > 0):\n",
    "                idxs = arg[np.array([(min(int(len(arg) * k/10), len(arg) - 1)) for k in range(0, 11)])]\n",
    "                perc_data[idx_i].append(a_data[idxs])\n",
    "                perc_paths[idx_i].append(np.array(paths[i])[idxs])\n",
    "            else:\n",
    "                perc_data[idx_i].append(-1)\n",
    "                perc_paths[idx_i].append(-1)\n",
    "\n",
    "        fig = plt.figure(figsize=(16, 8))\n",
    "        perc = int(perc * 10)\n",
    "        fig.suptitle(\"Images in the {}-th Percentile of {} (Attribute, {})\".format(perc * 10, metric, metric), fontsize=25, y=1.1)\n",
    "\n",
    "        columns = 3\n",
    "        rows = math.ceil(len(axis)/columns)\n",
    "        for i in range(1, len(axis) + 1):\n",
    "            if perc_paths[i-1][0] != -1:\n",
    "                image, anns = dataset.from_path(perc_paths[i-1][0][perc][0])\n",
    "                image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "                data = perc_data[i-1][0][perc]\n",
    "                ax = fig.add_subplot(rows, columns, i)\n",
    "\n",
    "                ann = anns[1][1][int(perc_paths[i-1][0][perc][1])]\n",
    "                ann_0 = (ann[0]*image.shape[1], ann[2]*image.shape[0])\n",
    "                ann_w = (ann[1]-ann[0])*image.shape[1]\n",
    "                ann_h = (ann[3]-ann[2])*image.shape[0]\n",
    "                rect = patches.Rectangle(ann_0,ann_w, ann_h, linewidth=2,edgecolor='b',facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "                if metric=='Distances':\n",
    "                    pt = patches.Circle((image.shape[1]/2, image.shape[0]/2), 4, fill=True, color='r')\n",
    "                    ax.add_patch(pt)\n",
    "                    pt = patches.Circle((ann_0[0] + ann_w/2, ann_0[1] + ann_h/2), 4, fill=True, color='r')\n",
    "                    ax.add_patch(pt)\n",
    "\n",
    "                title = f\"{attr_names[axis[i - 1]]}, {data.round(3)}\"\n",
    "                ax.set_title(title, fontsize=15)\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "                im = ax.imshow(image, alpha=.66)\n",
    "        plt.tight_layout()\n",
    "            \n",
    "    if metric == 'Sizes':\n",
    "        if disp_plt:\n",
    "            ordinal_plot(sizes, 'sizes')\n",
    "        else:\n",
    "            ordinal_image(sizes, metric, sizes_paths, perc)\n",
    "    elif metric == 'All sizes':\n",
    "        if disp_plt:\n",
    "            ordinal_plot(all_sizes, 'all_sizes')\n",
    "        else:\n",
    "            ordinal_image(all_sizes, metric, all_paths, perc)\n",
    "    elif metric == 'Sizes where no face was detected':\n",
    "        if disp_plt:\n",
    "            ordinal_plot(no_faces, 'no_faces')  \n",
    "        else:\n",
    "            ordinal_image(no_faces, metric, no_paths, perc)\n",
    "    elif metric == 'Distances':\n",
    "        if disp_plt:\n",
    "            ordinal_plot(dists, 'dists')\n",
    "        else:\n",
    "            ordinal_image(dists, metric, dists_paths, perc)\n",
    "\n",
    "def disp_ordinal_siz_scn(value):\n",
    "    def plot_ordinal_scene(scene):\n",
    "        fig=plt.figure();\n",
    "        ax=fig.add_subplot(111, label='mean');\n",
    "\n",
    "        _ = ax.plot(np.array(attr_names)[axis], np.array(scenes_ratio)[axis][:, scene], color=\"C0\")[0];\n",
    "        ax.set_xlabel(\"Attribute\");\n",
    "        ax.set_ylabel(\"Fraction\");\n",
    "\n",
    "        plt.title(f\"Fraction of Discarded Attribute Labels in '{xaxis[scene]}'\");\n",
    "        plt.show();\n",
    "        \n",
    "    clear_output()\n",
    "    toggle = widgets.ToggleButtons(options=[('Categorical Plot', False), ('Ordinal Graph', True)], \n",
    "                                   value = value['new'],\n",
    "                                   tooltips=['Display plot with counts of each scene by attribute',\n",
    "                                             'Display graph showing a statisically significant trend along an ordinal axis'])\n",
    "    ui = HBox([toggle])\n",
    "    display(ui)\n",
    "    \n",
    "    if not value['new']:\n",
    "        numbers_where_attribute_inferred()\n",
    "    order = scenes_where_no_face(toggle.value)\n",
    "    if order is not None:\n",
    "        ord_selection_widget = widgets.Dropdown(options=order)\n",
    "        ui_graph = HBox([ord_selection_widget])\n",
    "        out_graph = widgets.interactive_output(plot_ordinal_scene, {'scene': ord_selection_widget})\n",
    "        display(ui_graph, out_graph)\n",
    "    toggle.observe(disp_ordinal_siz_scn, 'value')\n",
    "\n",
    "def disp_discrete_siz():\n",
    "    if first_pass:\n",
    "        compare_sizedist('first_pass')\n",
    "    all_things = [comparisons_widget]\n",
    "    ui = HBox(all_things)\n",
    "    out = widgets.interactive_output(compare_sizedist, {'metric': comparisons_widget})\n",
    "    display(ui, out)\n",
    "\n",
    "def disp_ordinal_siz(value):\n",
    "    clear_output()\n",
    "    toggle = widgets.ToggleButtons(options=[('Categorical Graph', False), ('Ordinal Graph', True)], \n",
    "                                   value = value['new'],\n",
    "                                   tooltips=['Display graph with counts of each category by attribute',\n",
    "                                             'Display graph showing a statisically significant trend along an ordinal axis'])\n",
    "    ui = HBox([toggle])\n",
    "    display(ui)\n",
    "\n",
    "    if value['new']:\n",
    "        ui = HBox([comparisons_widget])\n",
    "        out = widgets.interactive_output(ordinal_sizedist, {'metric': comparisons_widget})\n",
    "        display(ui, out)\n",
    "        \n",
    "        ordinal_widget = widgets.FloatSlider(min=0, max=1.0, step=.1, value=.5, description=\"Percentile\")\n",
    "        ui = HBox([ordinal_widget])\n",
    "        out = widgets.interactive_output(ordinal_sizedist, {'metric': comparisons_widget, 'perc': ordinal_widget, 'disp_plt': fixed(False)})\n",
    "        display(ui, out)\n",
    "    else: \n",
    "        disp_discrete_siz()\n",
    "        \n",
    "    toggle.observe(disp_ordinal_siz, 'value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses \n",
    "<a id=\"att_siz_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics on how many attribute labels were inferred when they shouldn't have been because the person was either too small, or no face was detected. The scenes where this happens are shown to investigate if perhaps annotators are relying on contextual clues to make this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ordinal:\n",
    "    disp_ordinal_siz_scn({'new': False})\n",
    "else:\n",
    "    numbers_where_attribute_inferred()\n",
    "    scenes_where_no_face()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution by attribute of sizes and distances, both after removing images where attribute was unlikely to be able to be labeled, all sizes before any images were removed, and the sizes of people where no face was detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ordinal:\n",
    "    disp_ordinal_siz({'new': False})    \n",
    "else:\n",
    "    disp_discrete_siz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# att_cnt Metric: Object occurrences and cooccurrences\n",
    "<a id=\"att_cnt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"att_cnt_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide att_cnt code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categories = dataset.categories\n",
    "names = dataset.labels_to_names\n",
    "counts = pickle.load(open(\"results/{}/att_cnt.pkl\".format(folder_name), \"rb\"))\n",
    "topn = 10\n",
    "counts_mat = [np.zeros((len(categories), len(categories))) for i in range(num_attrs)]\n",
    "for key in counts[0].keys():\n",
    "    a, b = key.split('-')\n",
    "    a, b = int(a), int(b)\n",
    "    for attr in range(num_attrs):\n",
    "        counts_mat[attr][b][a] = counts[attr][key]\n",
    "        counts_mat[attr][a][b] = counts[attr][key]\n",
    "instance_counts = [np.diagonal(counts_mat[i]) for i in range(num_attrs)]\n",
    "normalized = [np.divide(counts_mat[i], instance_counts[i]) for i in range(num_attrs)]\n",
    "            \n",
    "def make_dist(counts, total):\n",
    "    samples = np.zeros(total)\n",
    "    samples[:int(counts)] = 1\n",
    "    return samples\n",
    "\n",
    "norm_instance_counts = [instance_counts[i]/ dataset.num_attribute_images[i] for i in range(num_attrs)]\n",
    "xaxis = [names[i] for i in categories]\n",
    "barWidth = .4\n",
    "\n",
    "# Looking at if the number of times an object appears with different attributes is statistically significant for all pairs of attributes\n",
    "p_values = []\n",
    "attribute_mapping = {}\n",
    "for i in range(len(instance_counts[0])):\n",
    "    for a in range(num_attrs):\n",
    "        for b in range(a+1, num_attrs):\n",
    "            p_values.append(stats.ttest_ind(make_dist(instance_counts[b][i], dataset.num_attribute_images[b]), make_dist(instance_counts[a][i], dataset.num_attribute_images[a]))[1])\n",
    "            attribute_mapping[p_values[-1]] = [a, b, xaxis[i]]\n",
    "    \n",
    "# Graphs the counts of each supercategory by attribute\n",
    "def supercategory_by_attribute(ordinal_plot=False):\n",
    "    if dataset.group_mapping is not None:\n",
    "        fontsize = 15\n",
    "        supercategory_counts = [np.zeros(len(dataset.supercategories_to_names)) for i in range(num_attrs)]\n",
    "        for i in range(len(categories)):\n",
    "            supercat = dataset.group_mapping(categories[i])\n",
    "            for a in range(num_attrs):\n",
    "                supercategory_counts[a][supercat] += norm_instance_counts[a][i]\n",
    "        for a in range(num_attrs):\n",
    "            supercategory_counts[a] = supercategory_counts[a][1:]\n",
    "        supercategory_counts = [np.array(supercategory_counts[i]) for i in range(num_attrs)]\n",
    "        \n",
    "        if ordinal_plot:\n",
    "            p_trend, cat_ord_options = [], []\n",
    "            for idx, count in enumerate(np.array(supercategory_counts).T):\n",
    "                cat_trend = np.array(count)[axis]\n",
    "                _, _, _, p, _ = stats.linregress([x for x in range(len(axis))], cat_trend)\n",
    "                p_trend.append(p)\n",
    "                cat_ord_options.append((f\"{dataset.supercategories_to_names[idx + 1]}, p={dec_to_show(p)}\", idx))\n",
    "            cat_ord_options = [x for _, x in sorted(zip(p_trend, cat_ord_options))]    \n",
    "            return cat_ord_options, np.array(supercategory_counts)[axis].T\n",
    "        else:\n",
    "            fig = plt.figure(figsize=(10, 6))\n",
    "            r1 = np.arange(len(dataset.supercategories_to_names)-1)\n",
    "            r1 = [r1[i]+i*num_attrs*barWidth for i in range(len(r1))]\n",
    "            r = [r1]\n",
    "            for i in range(1, num_attrs+1):\n",
    "                r.append([x + barWidth for x in r[len(r)-1]])\n",
    "\n",
    "            category_attr_ratios = [0 for i in range(len(supercategory_counts[0]))]\n",
    "            max_categories = [0 for i in range(len(supercategory_counts[0]))]\n",
    "            for category in range(len(supercategory_counts[0])):\n",
    "                max_category = 0\n",
    "                min_category = 1\n",
    "                max_attr = -1\n",
    "                for attr in range(num_attrs):\n",
    "                    if supercategory_counts[attr][category] > max_category:\n",
    "                        max_category = supercategory_counts[attr][category]\n",
    "                        max_attr = attr\n",
    "                    if supercategory_counts[attr][category] < min_category:\n",
    "                        min_category = supercategory_counts[attr][category]\n",
    "                category_attr_ratios[category] = max_category / min_category\n",
    "                max_categories[category] = max_attr\n",
    "\n",
    "            order = np.argsort(category_attr_ratios)\n",
    "            r = np.array(r)\n",
    "            for i in range(num_attrs-1, -1, -1):\n",
    "                plt.barh(r[i], supercategory_counts[i][order], height=barWidth, color=COLORS[i], edgecolor='white', label=attr_names[i])\n",
    "\n",
    "            ticks = [r[0]+(num_attrs/2)*barWidth for i in range(len(r1))][0]\n",
    "            plt.yticks(ticks, np.array([dataset.supercategories_to_names[i+1] for i in range(len(r1))])[order], fontsize=fontsize)\n",
    "            plt.xticks(fontsize=fontsize)\n",
    "            plt.legend(loc='best', fontsize=fontsize)\n",
    "            plt.ylabel('Object Category', fontsize=fontsize, labelpad=20)\n",
    "            plt.xlabel('Fraction of Labelled Images that contain this Category', fontsize=fontsize, labelpad=20, x=.3)\n",
    "            plt.tight_layout()\n",
    "            plt.gcf().subplots_adjust(bottom=0.18)\n",
    "            plt.gcf().subplots_adjust(left=0.28)\n",
    "            if first_pass:\n",
    "                to_write[3] = ['(att_cnt) Distribution of object categories each attribute appears with, sorted by ratio between the max count attribute and min count attribute.']\n",
    "                plt.savefig(\"results/{0}/{1}/1.png\".format(folder_name, save_loc))\n",
    "            plt.show()\n",
    "\n",
    "# Graphs the ratio of instance counts if they are statistically significant\n",
    "indices_to_keep = [i for i in range(len(instance_counts[0])) if categories[i] not in dataset.people_labels]\n",
    "pspecific_indices = []\n",
    "values_forobject = int((num_attrs)*(num_attrs-1)/2)\n",
    "        \n",
    "for ind in indices_to_keep:\n",
    "    for i in range(ind*(values_forobject),ind*(values_forobject)+(values_forobject)):\n",
    "        pspecific_indices.append(i)\n",
    "\n",
    "instance_sig = [norm_instance_counts[i][indices_to_keep] for i in range(num_attrs)]\n",
    "xaxis_sig = np.array(xaxis)[indices_to_keep]\n",
    "pvalues_sig = np.array(p_values)[pspecific_indices]\n",
    "count_attr_ratio = [0 for i in range(len(instance_sig[0]))]\n",
    "max_counts = [0 for i in range(len(instance_sig[0]))]\n",
    "for i in range(len(instance_sig[0])):\n",
    "    max_count = 0\n",
    "    min_count = 0\n",
    "    max_attr = -1\n",
    "    sum_cat = 0\n",
    "    for attr in range(num_attrs):\n",
    "        sum_cat += instance_sig[attr][i]\n",
    "        if instance_sig[attr][i] > max_count:\n",
    "            max_count = instance_sig[attr][i]\n",
    "            max_attr = attr\n",
    "    if sum_cat >0:\n",
    "        count_attr_ratio[i] = float(max_count) / float(sum_cat)\n",
    "    else:\n",
    "        count_attr_ratio[i] = 0\n",
    "    max_counts[i] = max_attr\n",
    "all_ratios = np.array(count_attr_ratio)\n",
    "max_counts = np.array(max_counts)\n",
    "\n",
    "def show_instance_ratios(sort_by, topn):\n",
    "    infinities = np.concatenate([np.where(all_ratios == -np.inf)[0], np.where(all_ratios == np.inf)[0]], axis=None)\n",
    "    infinite_categories = max_counts[infinities]\n",
    "    \n",
    "    all_ratios[all_ratios == -np.inf] = 0\n",
    "    all_ratios[all_ratios == np.inf] = 0\n",
    "\n",
    "    to_save = False\n",
    "    if topn is None:\n",
    "        topn = 5\n",
    "        to_save = True\n",
    "\n",
    "    if sort_by == 'pvalue':\n",
    "        top_indices = np.argsort(pvalues_sig)[:topn][::-1]\n",
    "        if to_save:\n",
    "            for i in reversed(range(topn)):\n",
    "                if pvalues_sig[top_indices[i]] >= .05:\n",
    "                    top_indices.pop()\n",
    "            topn = len(top_indices)\n",
    "            if len(top_indices) == 0:\n",
    "                return\n",
    "                to_save = False\n",
    "\n",
    "        instance_sig_topn = []\n",
    "        xaxis_sig_topn = []\n",
    "        all_ratios_topn = []\n",
    "        max_c = []\n",
    "        for pval in pvalues_sig[top_indices]:\n",
    "            info = attribute_mapping[pval]\n",
    "            index = np.where(xaxis_sig==info[2])\n",
    "            if instance_sig[info[0]][index][0]*dataset.num_attribute_images[info[0]] > instance_sig[info[1]][index][0]*dataset.num_attribute_images[info[1]]:\n",
    "                instance_sig_topn.append(instance_sig[info[0]][index][0])\n",
    "            else:\n",
    "                instance_sig_topn.append(instance_sig[info[1]][index][0])\n",
    "            xaxis_sig_topn.append(info[2])\n",
    "            all_ratios_topn.append(all_ratios[index][0])\n",
    "            max_c.append(max_counts[index][0])\n",
    "\n",
    "    elif sort_by == 'ratio':\n",
    "        top_indices = np.argsort(np.absolute(all_ratios))[-topn:]\n",
    "        instance_sig_topn = [instance_sig[i][top_indices] for i in range(num_attrs)]\n",
    "        xaxis_sig_topn = xaxis_sig[top_indices]\n",
    "        all_ratios_topn = all_ratios[top_indices]\n",
    "        max_c = max_counts[top_indices]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, max(2, topn // 3)))\n",
    "    fontsize = 10\n",
    "    pltbar = plt.barh(np.arange(topn), np.absolute(all_ratios_topn))\n",
    "\n",
    "    for bar in range(len(pltbar)):\n",
    "        pltbar[bar].set_color(COLORS[max_c[bar]])\n",
    "\n",
    "    plt.yticks(np.arange(topn), xaxis_sig_topn[:topn], rotation='horizontal', fontsize=fontsize)\n",
    "    ax = plt.gca()\n",
    "    ax.tick_params(axis=\"x\", bottom=True, top=True, labelbottom=True, labeltop=True)\n",
    "    ax.tick_params(axis=\"y\", left=False, right=True, labelleft=False, labelright=True)\n",
    "    plt.ylabel('Categories', fontsize=fontsize)\n",
    "    plt.xlabel('Counts Ratio', fontsize=fontsize)\n",
    "    patches = [mpatches.Patch(color=COLORS[i], label=attr_names[i]) for i in range(num_attrs)]\n",
    "    handles = patches\n",
    "    fontP = FontProperties()\n",
    "    fontP.set_size(12)\n",
    "    lgd = plt.legend(handles=handles, prop=fontP, loc='best')\n",
    "    plt.tight_layout()\n",
    "    if to_save:\n",
    "        to_write[4] = ['(att_cnt) Objects that are most statistically significantly represented with one attribute over the other.']\n",
    "        plt.savefig(\"results/{0}/{1}/2.png\".format(folder_name, save_loc))\n",
    "        plt.close()\n",
    "        return\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    if len(infinities) > 0:\n",
    "        print(\"Categories where one attribute had 0 occurrences with that object:\\n\")\n",
    "\n",
    "    # Categories where all attributes had 0 occurrences with that object\n",
    "    for index in infinities:\n",
    "        print(\"{0} had 0 occurrences with all values of the attribute\".format(xaxis_sig_topn[index]))\n",
    "\n",
    "    print(\"The most common attribute associated each category in the graph is:\\n\")\n",
    "    for index in reversed(range(topn)):\n",
    "        attr = int(max_c[index])\n",
    "        if sort_by == 'pvalue':\n",
    "            instance_sig_top = instance_sig_topn[index]\n",
    "        elif sort_by == 'ratio':\n",
    "            instance_sig_top = instance_sig_topn[attr][index]\n",
    "        print(\"{0}: {1}={2}, proportion of total instances: {3}\\n\".format(xaxis_sig_topn[index], attr_names[attr], math.ceil(instance_sig_top*dataset.num_attribute_images[attr]), round(np.absolute(all_ratios_topn[index]), 4)))\n",
    "\n",
    "p_values = np.zeros_like(counts_mat[0])\n",
    "attributes = np.array([[[-1,-1] for a in range(len(counts_mat[0][0]))] for b in range(len(counts_mat[0]))])\n",
    "mi = [np.zeros_like(counts_mat[0]) for a in range(num_attrs)]\n",
    "mi_wilson = [np.zeros_like(counts_mat[0]) for a in range(num_attrs)]\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(categories)):\n",
    "        if categories[i] in dataset.people_labels or categories[j] in dataset.people_labels:\n",
    "            p_values[i][j] = -1\n",
    "        else:\n",
    "            min_p = float('inf')\n",
    "            attr1 = -1\n",
    "            attr2 = -1\n",
    "            for a in range(num_attrs):\n",
    "                for b in range(a+1, num_attrs):\n",
    "                    len_a = instance_counts[a][j] + instance_counts[a][i] - counts_mat[a][i][j]\n",
    "                    an = np.zeros(int(len_a))\n",
    "                    an[:int(counts_mat[a][i][j])] = 1\n",
    "                    mi[a][i][j] = np.mean(an)\n",
    "                    mi_wilson[a][i][j] = wilson(np.mean(an), len_a)[0]\n",
    "                    \n",
    "                    len_b = instance_counts[b][j] + instance_counts[b][i] - counts_mat[b][i][j]\n",
    "                    bn = np.zeros(int(len_b))\n",
    "                    bn[:int(counts_mat[b][i][j])] = 1\n",
    "                    mi[b][i][j] = np.mean(bn)\n",
    "                    mi_wilson[b][i][j] = wilson(np.mean(bn), len_b)[0]\n",
    "                    p = stats.ttest_ind(an, bn)[1]\n",
    "                    if p < min_p:\n",
    "                        min_p = p\n",
    "                        attr1 = a\n",
    "                        attr2 = b\n",
    "            p_values[i][j] = p\n",
    "            attributes[i][j][0] = attr1\n",
    "            attributes[i][j][1] = attr2\n",
    "flat_p = p_values.flatten()\n",
    "flat_p[flat_p!=flat_p] = float(\"inf\")\n",
    "flat_p[flat_p == -1] = float(\"inf\")\n",
    "\n",
    "flat_attributes = attributes.flatten()\n",
    "attributes_dict = {}\n",
    "for i in range(len(flat_p)):\n",
    "    #Per 2 attributes in a ttest\n",
    "    attributes_dict[flat_p[i]] = [flat_attributes[2*i], flat_attributes[2*i+1]]\n",
    "    \n",
    "normalized_indices = np.argsort(flat_p)\n",
    "\n",
    "def cooccurrence_counts_mi(topn):\n",
    "    print(\"Statistically significant mutual information:\\n\")\n",
    "    i, j = 0, 0\n",
    "    while j < topn:\n",
    "        index = normalized_indices[i]\n",
    "        a, b = index % len(categories), index // len(categories)\n",
    "        if a < b:\n",
    "            attr1 = attributes_dict[flat_p[index]][0]\n",
    "            attr2 = attributes_dict[flat_p[index]][1]\n",
    "            print(\"{0} - {1}: {2}\".format(names[categories[a]], names[categories[b]], '{:0.3e}'.format(flat_p[index])))\n",
    "            print(\"{0}: {1}, {2}: {3}\".format(attr_names[attr1], round(mi[attr1].flatten()[index], 4), attr_names[attr2], round(mi[attr2].flatten()[index], 4)))\n",
    "            print()\n",
    "            j += 1\n",
    "        i += 1\n",
    "        \n",
    "def disp_ordinal_cnt_supercat(value):\n",
    "    def plot_ordinal_supercat(cat, supercat_ord):\n",
    "        fig=plt.figure();\n",
    "        ax=fig.add_subplot(111, label='mean');\n",
    "        _ = ax.plot(np.array(attr_names)[axis], np.array(supercat_ord)[cat], color=\"C0\")[0];\n",
    "        ax.set_xlabel(\"Attribute\");\n",
    "        ax.set_ylabel(\"Fraction\");\n",
    "\n",
    "        plt.title(f\"Fraction of Images Containing '{dataset.supercategories_to_names[cat + 1]}'\");\n",
    "        plt.show();\n",
    "        \n",
    "    clear_output()\n",
    "    toggle = widgets.ToggleButtons(options=[('Bar Plot', False), ('Ordinal Graph', True)], \n",
    "                                   value=value['new'],\n",
    "                                   tooltips=['Display bar plot with counts of each category by attribute',\n",
    "                                             'Display graph showing a statisically significant trend along an ordinal axis'])\n",
    "    ui = HBox([toggle])\n",
    "    display(ui)\n",
    "    if value['new']:\n",
    "        order, supercategory_ord = supercategory_by_attribute(toggle.value)\n",
    "        if order is not None:\n",
    "            ord_selection_widget = widgets.Dropdown(options=order)\n",
    "            ui_graph = HBox([ord_selection_widget])\n",
    "            out_graph = widgets.interactive_output(plot_ordinal_supercat, {'cat': ord_selection_widget, 'supercat_ord': fixed(supercategory_ord)})\n",
    "            display(ui_graph, out_graph)\n",
    "    else:\n",
    "        supercategory_by_attribute()\n",
    "\n",
    "    toggle.observe(disp_ordinal_cnt_supercat, 'value')\n",
    "    \n",
    "def disp_ordinal_cnt_cat(value):\n",
    "    def plot_ordinal_cat(cat, count):\n",
    "        fig=plt.figure();\n",
    "        ax=fig.add_subplot(111, label='mean');\n",
    "        _ = ax.plot(np.array(attr_names)[axis], np.array(count)[axis, cat], color=\"C0\")[0];\n",
    "        ax.set_xlabel(\"Attribute\");\n",
    "        ax.set_ylabel(\"Fraction\");\n",
    "\n",
    "        plt.title(f\"Fraction of Objects Coocurring with '{xaxis[cat]}'\");\n",
    "        plt.show();\n",
    "        \n",
    "    clear_output()\n",
    "    toggle = widgets.ToggleButtons(options=[('Bar Plot', False), ('Ordinal Graph', True)], \n",
    "                                   value=value['new'],\n",
    "                                   tooltips=['Display bar plot with counts of each category by attribute',\n",
    "                                             'Display graph showing a statisically significant trend along an ordinal axis'])\n",
    "    ui = HBox([toggle])\n",
    "    display(ui)\n",
    "    if value['new']:\n",
    "        norm_ord_counts = np.array(norm_instance_counts)\n",
    "        cat_ordinal_options, p_trend = [], []\n",
    "        for i in range(norm_ord_counts.shape[1]):\n",
    "            count = norm_ord_counts[axis, i]\n",
    "            _, _, _, p, _ = stats.linregress([x for x in range(len(axis))], count)\n",
    "            p_trend.append(p)\n",
    "            cat_ordinal_options.append((f\"{xaxis[i]}, p={dec_to_show(p)}\", i))\n",
    "            \n",
    "        cat_ordinal_options = [x for _, x in sorted(zip(p_trend, cat_ordinal_options))]\n",
    "        ord_selection_widget = widgets.Dropdown(options=cat_ordinal_options)\n",
    "        ui_graph = HBox([ord_selection_widget])\n",
    "        out_graph = widgets.interactive_output(plot_ordinal_cat, {'cat': ord_selection_widget, 'count':fixed(norm_ord_counts)})\n",
    "        display(ui_graph, out_graph)\n",
    "    else:\n",
    "        disp_discrete_cnt()\n",
    "\n",
    "    toggle.observe(disp_ordinal_cnt_cat, 'value')\n",
    "\n",
    "def disp_discrete_cnt():\n",
    "    instanceratio_slider = widgets.IntSlider(min=5, max=50, step=1, value=10)\n",
    "    instanceratio_sortby = widgets.Dropdown(options=['pvalue', 'ratio'], value='pvalue')\n",
    "\n",
    "    if first_pass:\n",
    "        show_instance_ratios('pvalue', None)\n",
    "\n",
    "    all_things = [instanceratio_sortby, instanceratio_slider]\n",
    "    ui = HBox(all_things)\n",
    "    out = widgets.interactive_output(show_instance_ratios, {'sort_by': instanceratio_sortby, 'topn': instanceratio_slider})\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "<a id=\"att_cnt_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of object categories by attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if dataset.group_mapping is not None:\n",
    "    if ordinal:\n",
    "        disp_ordinal_cnt_supercat({'new': False})\n",
    "    else:\n",
    "        supercategory_by_attribute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio between how often an object is represented with each attribute (normalized), sorted by p-value or ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if dataset.group_mapping is not None:\n",
    "    if ordinal:\n",
    "        disp_ordinal_cnt_cat({'new': False})\n",
    "    else:\n",
    "        disp_discrete_cnt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most statistically significant object cooccurrences, measured by mutual information, between the attributes. The p-value is shown for the difference of the distributions, and the numbers for each specific attribute value indicate the mutual information for each attribute on the object pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(cooccurrence_counts_mi, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# att_dis Metric : Distance from object as proxy for interaction\n",
    "<a id=\"att_dis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"att_dis_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide att_dis code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset.categories\n",
    "names = dataset.labels_to_names\n",
    "distances = pickle.load(open(\"results/{}/att_dis.pkl\".format(folder_name), \"rb\"))\n",
    "topn = 10\n",
    "\n",
    "attr_dists = []\n",
    "attr_distr_dists = []\n",
    "attr_file_paths = []\n",
    "attr_indices = []\n",
    "attr_person = []\n",
    "p_values = []\n",
    "ratio_idxs = []\n",
    "xaxis = []\n",
    "i_list = []\n",
    "p_vals = []\n",
    "cat_ordinal_options = []\n",
    "mean_dists = [[0 for j in range(len(axis))] for i in range(len(categories))]\n",
    "median_dists = [[0 for j in range(len(axis))] for i in range(len(categories))]\n",
    "dists_ord, paths_ord, ann_indices_ord, person_indices_ord = {}, {}, {}, {}\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    largest_ratio = -1\n",
    "    ratio_idx = -1\n",
    "    for j in range(num_attrs):\n",
    "        a_dist = np.array([ (chunk[0] / np.sqrt(chunk[1]*chunk[2])) for chunk in distances[i][j] if chunk[2] != 0 and chunk[1] != 0])\n",
    "        a_file = np.array([ chunk[3] for chunk in distances[i][j] if chunk[2] != 0 and chunk[1] != 0])\n",
    "        a_index = np.array([ chunk[4] for chunk in distances[i][j] if chunk[2] != 0 and chunk[1] != 0])\n",
    "        a_person = np.array([ chunk[5] for chunk in distances[i][j] if chunk[2] != 0 and chunk[1] != 0])\n",
    "        b_dist, b_file, b_index, b_person = [], [], [], []\n",
    "        for k in range(num_attrs):\n",
    "            b_dist.extend([(chunk[0] / np.sqrt(chunk[1]*chunk[2])) for chunk in distances[i][k] if chunk[2] != 0 and chunk[1] != 0 and j != k])\n",
    "            b_file.extend([chunk[3] for chunk in distances[i][k] if chunk[2] != 0 and chunk[1] != 0 and j != k])\n",
    "            b_index.extend([chunk[4] for chunk in distances[i][k] if chunk[2] != 0 and chunk[1] != 0 and j != k])\n",
    "            b_person.extend([chunk[5] for chunk in distances[i][k] if chunk[2] != 0 and chunk[1] != 0 and j != k])\n",
    "        \n",
    "        t, p = stats.ttest_ind(a_dist, b_dist, equal_var=False)\n",
    "        a_mean = np.mean(a_dist)\n",
    "        b_mean = np.mean(b_dist)\n",
    "        if not np.isnan(a_mean) and not np.isnan(b_mean) and len(a_dist) > 5 and len(b_dist) > 5:\n",
    "            if (p < 0.05):\n",
    "                attr_dists.append([a_mean, b_mean])\n",
    "                attr_distr_dists.append([a_dist, b_dist])\n",
    "                attr_file_paths.append([a_file, b_file])\n",
    "                attr_indices.append([a_index, b_index])\n",
    "                attr_person.append([a_person, b_person])\n",
    "                p_values.append(p)\n",
    "                ratio_idxs.append(j)\n",
    "                xaxis.append(names[categories[i]])\n",
    "                i_list.append(i)\n",
    "        \n",
    "        if (num_attrs == 2):\n",
    "            break\n",
    "        \n",
    "    if (ordinal and num_attrs > 2 and categories[i] not in dataset.people_labels):\n",
    "        u, n, n_i_sq, n_i_mult = 0, 0, 0, 0\n",
    "        accum_dists = []\n",
    "        for idx_j, j in enumerate(axis):\n",
    "            a_dist = np.array([ (chunk[0] / np.sqrt(chunk[1]*chunk[2])) for chunk in distances[i][j] if chunk[2] != 0 and chunk[1] != 0])\n",
    "            accum_dists.append(a_dist)\n",
    "            if (len(a_dist) == 0):\n",
    "                continue\n",
    "            mean_dists[i][idx_j] = np.mean(a_dist)\n",
    "            median_dists[i][idx_j] = np.median(a_dist)\n",
    "            n += len(a_dist)\n",
    "            n_i_sq += len(a_dist)**2\n",
    "            n_i_mult += len(a_dist)**2 * (2 * len(a_dist) + 3)\n",
    "            \n",
    "            if (idx_j < len(axis)):\n",
    "                for idx_k, k in enumerate(axis[idx_j + 1:]):\n",
    "                    b_dist = np.array([(chunk[0] / np.sqrt(chunk[1]*chunk[2])) for chunk in distances[i][k] if chunk[2] != 0 and chunk[1] != 0])\n",
    "                    for dist in a_dist:\n",
    "                        u += (b_dist > dist).sum()\n",
    "\n",
    "        E_U = (n**2 - n_i_sq)/4\n",
    "        Var_U = (n**2 * (2 * n + 3) - n_i_mult)/72\n",
    "        Z = (u - E_U)/(Var_U ** 0.5)\n",
    "        p_trend = stats.norm.sf(abs(Z))\n",
    "        \n",
    "        p_vals.append(p_trend)\n",
    "        cat_ordinal_options.append(((names[categories[i]], p_trend), i))\n",
    "        if i not in dists_ord:\n",
    "            dists_ord[i], paths_ord[i], ann_indices_ord[i], person_indices_ord[i] = [], [], [], []\n",
    "        for idx, dist in zip(axis, accum_dists):\n",
    "            arg = np.argsort(dist)\n",
    "            if len(arg) > 0:\n",
    "                idxs = arg[np.array([(min(int(len(arg) * k/10), len(arg) - 1)) for k in range(0, 11)])]\n",
    "                dists_ord[i].append(dist[idxs])\n",
    "                paths_ord[i].append(np.array([chunk[3] for chunk in distances[i][idx] if chunk[2] != 0 and chunk[1] != 0])[idxs])\n",
    "                ann_indices_ord[i].append(np.array([chunk[4] for chunk in distances[i][idx] if chunk[2] != 0 and chunk[1] != 0])[idxs])\n",
    "                person_indices_ord[i].append(np.array([chunk[5] for chunk in distances[i][idx] if chunk[2] != 0 and chunk[1] != 0])[idxs])\n",
    "            else:\n",
    "                dists_ord[i].append(-1)\n",
    "                paths_ord[i].append(-1)\n",
    "                ann_indices_ord[i].append(-1)\n",
    "                person_indices_ord[i].append(-1)\n",
    "                \n",
    "cat_ordinal_options.sort(key=lambda x:x[0][1])\n",
    "cat_ordinal_options[:] = [(f\"{x}, p={dec_to_show(y)}\", z) for (x, y), z in cat_ordinal_options]\n",
    "mean_dists = np.array(mean_dists)\n",
    "# Only retaining information for distances that are not people\n",
    "indices_to_keep = [i for i in range(len(attr_dists)) if categories[i_list[i]] not in dataset.people_labels]\n",
    "attr_sig_dists = np.array(attr_dists)[indices_to_keep]\n",
    "attr_distr_dists = np.array(attr_distr_dists)[indices_to_keep]\n",
    "attr_file_paths = np.array(attr_file_paths)[indices_to_keep]\n",
    "attr_indices = np.array(attr_indices)[indices_to_keep]\n",
    "attr_person = np.array(attr_person)[indices_to_keep]\n",
    "xaxis_sig = np.array(xaxis)[indices_to_keep]\n",
    "pvalues_sig = np.array(p_values)[indices_to_keep]\n",
    "ratio_idxs = np.array(ratio_idxs)[indices_to_keep]\n",
    "\n",
    "all_ratios = []\n",
    "for i in range(len(attr_sig_dists)):\n",
    "    in_count = attr_sig_dists[i][0]\n",
    "    out_count = attr_sig_dists[i][1]\n",
    "    if in_count > out_count:\n",
    "        all_ratios.append(in_count/out_count)\n",
    "    else:\n",
    "        all_ratios.append(out_count/-in_count)\n",
    "\n",
    "all_ratios = np.array(all_ratios)\n",
    "distance_ratio_options = [(\"{0}, {3}: p-value {1}, {2}x\".format(xaxis_sig[i], '{:0.2e}'.format(pvalues_sig[i]), round(all_ratios[i], 3), attr_names[ratio_idxs[i]].lower()), i) for i in np.argsort(pvalues_sig)]\n",
    "\n",
    "def show_ordinal_trends(cat):\n",
    "    fig=plt.figure()\n",
    "    mean_ax=fig.add_subplot(111, label='mean')\n",
    "    med_ax=fig.add_subplot(111, label='median', frame_on=False)\n",
    "    \n",
    "    plt_mean = mean_ax.plot(axis, mean_dists[cat], color=\"C0\")[0]\n",
    "    mean_ax.set_xlabel(\"Attribute\")\n",
    "    mean_ax.set_ylabel(\"Mean Distance\")\n",
    "    \n",
    "    plt_med = med_ax.plot(axis, median_dists[cat], color=\"C1\")[0]\n",
    "    med_ax.yaxis.tick_right()\n",
    "    med_ax.set_ylabel(\"Median Distance\")\n",
    "    med_ax.yaxis.set_label_position(\"right\")\n",
    "    \n",
    "    mean_ax.legend([plt_mean, plt_med], [\"Mean\", \"Median\"])\n",
    "    \n",
    "    plt.title(f\"Mean/Median Distance of '{names[categories[cat]]}' by Attribute\");\n",
    "    plt.show()\n",
    "\n",
    "def show_ordinal_images(cat, perc):    \n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    perc = int(perc * 10)\n",
    "    fig.suptitle(\"Images in the {}-th Percentile of Distance (Attribute, Distance)\".format(perc * 10), fontsize=25, y=1.1)\n",
    "\n",
    "    columns = 3\n",
    "    rows = math.ceil(len(axis)/columns)\n",
    "    for i in range(1, len(axis) + 1):\n",
    "        if (paths_ord[cat][i-1] != -1):\n",
    "            image, anns = dataset.from_path(paths_ord[cat][i-1][perc])\n",
    "            image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "            ann_index = ann_indices_ord[cat][i-1][perc]\n",
    "            pers_idx = person_indices_ord[cat][i-1][perc]\n",
    "            dist = dists_ord[cat][i-1][perc]\n",
    "            ax = fig.add_subplot(rows, columns, i)\n",
    "\n",
    "            ann = anns[0][ann_index]['bbox']\n",
    "            ann_0 = (ann[0]*image.shape[1], ann[2]*image.shape[0])\n",
    "            ann_w = (ann[1]-ann[0])*image.shape[1]\n",
    "            ann_h = (ann[3]-ann[2])*image.shape[0]\n",
    "\n",
    "            per = anns[1][1][pers_idx]\n",
    "            per_0 = (per[0]*image.shape[1], per[2]*image.shape[0])\n",
    "            per_w = (per[1]-per[0])*image.shape[1]\n",
    "            per_h = (per[3]-per[2])*image.shape[0]\n",
    "            rect = patches.Rectangle(per_0,per_w, per_h, linewidth=2,edgecolor='r',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            rect = patches.Rectangle(ann_0,ann_w, ann_h, linewidth=2,edgecolor='b',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            title = f\"{attr_names[axis[i - 1]]}, {dist.round(3)}\"\n",
    "            ax.set_title(title, fontsize=15)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            im = ax.imshow(image, alpha=.66)\n",
    "    plt.tight_layout()\n",
    "        \n",
    "def show_distance_ratios(sort_by, topn):\n",
    "    topn = min(topn, pvalues_sig.shape[0])\n",
    "    if sort_by == 'pvalue':\n",
    "        top_indices = np.argsort(pvalues_sig)[:topn][::-1]\n",
    "    elif sort_by == 'ratio':\n",
    "        top_indices = np.argsort(np.absolute(all_ratios))[-topn:]\n",
    "        \n",
    "    fontsize = 10\n",
    "    fig = plt.figure(figsize=(8, max(2, topn // 3)))\n",
    "    \n",
    "    all_ratios_topn = all_ratios[top_indices]\n",
    "    xaxis_sig_topn = xaxis_sig[top_indices]\n",
    "    all_idxs = ratio_idxs[top_indices]\n",
    "\n",
    "    pltbar = plt.barh(np.arange(topn), all_ratios_topn, tick_label=xaxis_sig_topn, color=['C' + str(all_idxs[j]) for j in range(len(all_idxs))])\n",
    "    plt.xlim(min(-np.abs(np.min(all_ratios_topn)), -np.abs(np.max(all_ratios_topn))) - 1, max(np.abs(np.min(all_ratios_topn)), np.abs(np.max(all_ratios_topn))) + 1)\n",
    "    \n",
    "    handles = []\n",
    "    for att in range(num_attrs):\n",
    "        patch = mpatches.Patch(color='C' + str(att), label=attr_names[att])\n",
    "        handles.append(patch)\n",
    "\n",
    "    fontP = FontProperties()\n",
    "    fontP.set_size(12)\n",
    "    lgd = plt.legend(handles=handles, prop=fontP)\n",
    "    plt.xlabel('Proximity', fontsize=fontsize)\n",
    "    plt.ylabel('Categories', fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.tight_layout()        \n",
    "    plt.show()\n",
    "\n",
    "distanceratio_slider = widgets.IntSlider(min=5, max=50, step=1, value=10)\n",
    "distanceratio_sortby = widgets.Dropdown(options=['pvalue', 'ratio'], value='pvalue')\n",
    "\n",
    "def set_box_color(bp, color):\n",
    "    plt.setp(bp['boxes'], color=color)\n",
    "    plt.setp(bp['whiskers'], color=color)\n",
    "    plt.setp(bp['caps'], color=color)\n",
    "    plt.setp(bp['medians'], color=color)\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "# Saves qualitative images for each of the quartiles of distance\n",
    "def save_box_images(bpa, a_array, file_paths_a, indices_a, person_a, bpb, b_array, file_paths_b, indices_b, person_b, names, cat_index, to_save=False):\n",
    "    a = bpa['whiskers'][0].get_data()[1][1]\n",
    "    b = bpa['whiskers'][0].get_data()[1][0]\n",
    "    c = bpa['medians'][0].get_data()[1][0]\n",
    "    d = bpa['whiskers'][1].get_data()[1][0]\n",
    "    e = bpa['whiskers'][1].get_data()[1][1]\n",
    "    box_nums_a = [a, b, c, d, e]\n",
    "    a = bpb['whiskers'][0].get_data()[1][1]\n",
    "    b = bpb['whiskers'][0].get_data()[1][0]\n",
    "    c = bpb['medians'][0].get_data()[1][0]\n",
    "    d = bpb['whiskers'][1].get_data()[1][0]\n",
    "    e = bpb['whiskers'][1].get_data()[1][1]\n",
    "    box_nums_b = [a, b, c, d, e]\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    for k in range(2):\n",
    "        for j in range(5):\n",
    "            if k == 0:\n",
    "                index = find_nearest(a_array, box_nums_a[j])\n",
    "                indices = indices_a\n",
    "                file_paths = file_paths_a\n",
    "                pers_indices = person_a\n",
    "            else:\n",
    "                index = find_nearest(b_array, box_nums_b[j])\n",
    "                indices = indices_b\n",
    "                file_paths = file_paths_b\n",
    "                pers_indices = person_b\n",
    "            file_path = file_paths[index]\n",
    "            ann_index = indices[index]\n",
    "            image, anns = dataset.from_path(file_path)\n",
    "            image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "            \n",
    "            if k == 0:\n",
    "                ax = fig.add_subplot(2, 5, j+1)\n",
    "            else:\n",
    "                ax = fig.add_subplot(2, 5, j+6)\n",
    "            \n",
    "            ann = anns[0][ann_index]['bbox']\n",
    "            ann_0 = (ann[0]*image.shape[1], ann[2]*image.shape[0])\n",
    "            ann_w = (ann[1]-ann[0])*image.shape[1]\n",
    "            ann_h = (ann[3]-ann[2])*image.shape[0]\n",
    "            per = anns[1][1][pers_indices[index]]\n",
    "            per_0 = (per[0]*image.shape[1], per[2]*image.shape[0])\n",
    "            per_w = (per[1]-per[0])*image.shape[1]\n",
    "            per_h = (per[3]-per[2])*image.shape[0]\n",
    "            rect = patches.Rectangle(per_0,per_w, per_h, linewidth=2,edgecolor='r',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            rect = patches.Rectangle(ann_0,ann_w, ann_h, linewidth=2,edgecolor='b',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            title = 'min'\n",
    "            if j == 1:\n",
    "                title = 'lower quartile'\n",
    "            elif j == 2:\n",
    "                title = 'median'\n",
    "            elif j == 3:\n",
    "                title = 'upper quartile'\n",
    "            elif j == 4:\n",
    "                title = 'max'\n",
    "            ax.set_title(title, fontsize=15)\n",
    "            ax.axis(\"off\")\n",
    "            if all_ratios[cat_index] < 0:\n",
    "                top_hl = \"Closer\"\n",
    "                bottom_hl = \"Further\"\n",
    "            else:\n",
    "                top_hl = \"Further\"\n",
    "                bottom_hl = \"Closer\"\n",
    "\n",
    "            fig.suptitle(\"{} (Top, {}) vs NOT-{} (Bottom, {})\".format(attr_names[ratio_idxs[cat_index]].upper(), top_hl, attr_names[ratio_idxs[cat_index]].upper(), bottom_hl), fontsize=16)\n",
    "            im = ax.imshow(image, alpha=.66)\n",
    "\n",
    "    if to_save:\n",
    "        if np.mean(a_array) < np.mean(b_array):\n",
    "            closer_sent = attr_names[ratio_idxs[cat_index]] + \" are closer than any other attribute.\"\n",
    "        else:\n",
    "            closer_sent = attr_names[ratio_idxs[cat_index]] + \" are further than any other attribute.\"\n",
    "        to_write[5] = [\"(att_dis) Qualitative example of {0}, which has the biggest ratio in distance between object and person (which can be interpreted as a proxy for interaction) between the attributes. \".format(names) + closer_sent + \" There is a red box around the person, and blue box around the object.\"]\n",
    "        plt.savefig(\"results/{0}/{1}/3.png\".format(folder_name, save_loc))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def qualitative_boxexamples(cat_index):\n",
    "    try: \n",
    "        to_save = False\n",
    "        if cat_index is None and first_pass:\n",
    "            to_save = True\n",
    "            cat_index = distance_ratio_options[0][1]\n",
    "\n",
    "        a = attr_distr_dists[cat_index][0]\n",
    "        b = attr_distr_dists[cat_index][1]\n",
    "\n",
    "        bpa = plt.boxplot(a, sym='', widths=0.6)\n",
    "        bpb = plt.boxplot(b, sym='', widths=0.6)\n",
    "        plt.close()\n",
    "\n",
    "        save_box_images(bpa, a, attr_file_paths[cat_index][0], attr_indices[cat_index][0], attr_person[cat_index][0],\n",
    "            bpb, b, attr_file_paths[cat_index][1], attr_indices[cat_index][1], attr_person[cat_index][1],\n",
    "            xaxis_sig[cat_index], cat_index, to_save)\n",
    "    except (AttributeError, FileNotFoundError): \n",
    "        print('Some functionality not available for CoCoDatasetNoImages Class')\n",
    "        \n",
    "def disp_discrete_dist():\n",
    "    all_things = [distanceratio_sortby, distanceratio_slider]\n",
    "    ui = HBox(all_things)\n",
    "    out = widgets.interactive_output(show_distance_ratios, {'sort_by': distanceratio_sortby, 'topn': distanceratio_slider})\n",
    "    display(ui, out)\n",
    "        \n",
    "def disp_ratio_dist(value):\n",
    "    clear_output()\n",
    "    display(Markdown(\"\"\"Ordinal graph displays analysis of distance as a function of some ordinal variable. \n",
    "    The graph displays the **mean** and **median** distance of an object from a person by attribute. \n",
    "    Sample images from the specified percentile (of all distances between person and attribute) \n",
    "    in each attribute are displayed.\"\"\"))\n",
    "    toggle = widgets.ToggleButtons(options=[('Bar Plot', False), ('Ordinal Graph', True)], \n",
    "                                   value=value['new'],\n",
    "                                   tooltips=['Display bar plot with counts of each category by attribute',\n",
    "                                             'Display graph showing a statisically significant trend along an ordinal axis'])\n",
    "    ui = HBox([toggle])\n",
    "    display(ui)\n",
    "    if value['new']:\n",
    "        cat_choice_widget = widgets.Dropdown(options=cat_ordinal_options, layout=Layout(width='400px'))\n",
    "        ordinal_widget = widgets.FloatSlider(min=0, max=1.0, step=.1, value=.5, description=\"Percentile\")\n",
    "        ui_graph = HBox([cat_choice_widget])\n",
    "        ui_images = HBox([ordinal_widget])        \n",
    "        out_graph = widgets.interactive_output(show_ordinal_trends, {'cat': cat_choice_widget})\n",
    "        out_images = widgets.interactive_output(show_ordinal_images, {'cat': cat_choice_widget, 'perc': ordinal_widget})\n",
    "        display(ui_graph, out_graph)\n",
    "        display(ui_images, out_images)\n",
    "    else:\n",
    "        disp_discrete_dist()\n",
    "    \n",
    "    toggle.observe(disp_ratio_dist, 'value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"att_dis_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of average distance between attribute and object. Negative values represent that the attribute is **closer** to the object, while positive values represent that the attribute is **further** from the object (compared to all other attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ordinal:\n",
    "    disp_ratio_dist({'new': False})\n",
    "else:\n",
    "    disp_discrete_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_pass:\n",
    "    qualitative_boxexamples(None)\n",
    "interact(qualitative_boxexamples, cat_index=widgets.Dropdown(options=distance_ratio_options, layout=Layout(width='400px')));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# att_clu Metric: Linearly separable objects by attribute\n",
    "<a id=\"att_clu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"att_clu_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide att_clu code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not os.path.exists(\"results/{0}/att_clu/\".format(folder_name)):\n",
    "    os.mkdir(\"results/{0}/att_clu/\".format(folder_name))\n",
    "categories = dataset.categories\n",
    "names = dataset.labels_to_names\n",
    "stats_dict = pickle.load(open(\"results/{0}/att_clu.pkl\".format(folder_name), \"rb\"))\n",
    "instances = stats_dict['instance']\n",
    "scenes = stats_dict['scene']\n",
    "scene_filepaths = stats_dict['scene_filepaths']\n",
    "\n",
    "file_name = 'util_files/categories_places365.txt'\n",
    "if not os.access(file_name, os.W_OK):\n",
    "    synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt'\n",
    "    os.system('wget ' + synset_url)\n",
    "    os.rename('categories_places365.txt', 'util_files/categories_places365.txt')\n",
    "classes = list()\n",
    "with open(file_name) as class_file:\n",
    "    for line in class_file:\n",
    "        classes.append(line.strip().split(' ')[0][3:])\n",
    "scene_classes = tuple(classes)\n",
    "\n",
    "topn = 15\n",
    "\n",
    "plot_kwds = {'alpha' : .8, 's' : 30, 'linewidths':0}\n",
    "\n",
    "instance_p_values = []\n",
    "scene_p_values = []\n",
    "\n",
    "file.write(\"SVM accuracies\\n\")\n",
    "\n",
    "if not os.path.exists(\"checkpoints/{}/att_clu.pkl\".format(folder_name)):\n",
    "    value_to_phrase = {}\n",
    "    value_to_scenephrase = {}\n",
    "    for i in range(len(categories)):\n",
    "        # SVM's to classify between an object's features for each attribute\n",
    "        clf = svm.SVC(kernel='linear', probability=False, max_iter=5000)\n",
    "        clf_prob = svm.SVC(kernel='linear', probability=True)\n",
    "        cont = False\n",
    "        for j in range(num_attrs):\n",
    "            if len(instances[i][j]) <= 1 or len(scenes[i][j]) <= 1:\n",
    "                scene_p_values.append(float('inf'))\n",
    "                instance_p_values.append(float('inf'))\n",
    "                cont = True\n",
    "        if cont:\n",
    "            continue\n",
    "        \n",
    "        features_instances = np.concatenate([instances[i][j] for j in range(num_attrs)], axis=0)\n",
    "        boundary_instances = [len(instances[i][j]) for j in range(num_attrs)]\n",
    "        features_scenes = np.concatenate([scenes[i][j] for j in range(num_attrs)], axis=0)\n",
    "        boundary_scenes = [len(scenes[i][j]) for j in range(num_attrs)]\n",
    "\n",
    "        # Uncomment to visualize features of cropped object, saved as a png\n",
    "        # projection_instances = TSNE().fit_transform(features_instances)\n",
    "        # plt.scatter(*projection_instances.T, **plot_kwds, c=[COLORS[k] for k in range(num_attrs) for j in range(boundary_instances[k])])\n",
    "        # plt.savefig(\"results/{0}/{1}/instances_{2}.png\".format(folder_name, 4, i))\n",
    "        # plt.close()\n",
    "\n",
    "        current_p_values = []\n",
    "        for j in range(num_attrs):\n",
    "            for k in range(j + 1, num_attrs):\n",
    "                t, p = stats.ttest_ind(instances[i][j], instances[i][k])\n",
    "                current_p_values.append(np.nanmean(p))\n",
    "        instance_p_values.append(current_p_values)\n",
    "\n",
    "        # Uncomment to visualize features of entire scene, saved as a png\n",
    "        # projection_scenes = TSNE().fit_transform(features_scenes)\n",
    "        # plt.scatter(*projection_scenes.T, **plot_kwds, c=[COLORS[k] for k in range(num_attrs) for j in range(boundary_scenes[k])])\n",
    "        # plt.savefig(\"results/{0}/{1}/scenes_{2}.png\".format(folder_name, 4, i))\n",
    "        # plt.close()\n",
    "\n",
    "        current_p_values = []\n",
    "        for j in range(num_attrs):\n",
    "            for k in range(j + 1, num_attrs):\n",
    "                t, p = stats.ttest_ind(scenes[i][j], scenes[i][k])\n",
    "                current_p_values.append(np.nanmean(p))\n",
    "        scene_p_values.append(current_p_values)\n",
    "\n",
    "        num_features = int(np.sqrt(len(features_scenes)))\n",
    "\n",
    "        labels = np.zeros(len(features_scenes))\n",
    "        idx = 0\n",
    "        for j in range(num_attrs):\n",
    "            labels[idx:] = j\n",
    "            idx += len(scenes[i][j])\n",
    "        projected_features_scenes = StandardScaler().fit_transform(project(features_scenes, num_features))\n",
    "\n",
    "        clf.fit(projected_features_scenes, labels)\n",
    "        clf_prob.fit(projected_features_scenes, labels)\n",
    "        acc = clf.score(projected_features_scenes, labels)\n",
    "        preds = clf.predict(projected_features_scenes)\n",
    "        probs = clf.decision_function(projected_features_scenes)\n",
    "        scaled_probs = clf_prob.predict_proba(projected_features_scenes)\n",
    "        if (num_attrs == 2):\n",
    "            probs = np.stack([probs, -probs], axis=1)\n",
    "        \n",
    "        probs_copy = probs.copy()\n",
    "        probs_copy[probs_copy > 0] = 1\n",
    "        probs_copy[probs_copy < 0] = 0\n",
    "        target = np.zeros(probs_copy.shape)\n",
    "        target[np.arange(target.shape[0]), labels.astype(int)] = 1\n",
    "        accs = []\n",
    "        for j in range(num_attrs):\n",
    "            accs.append(np.sum(target[:, j] == probs_copy[:, j]))\n",
    "        most_acc_attr = np.argmax(accs)\n",
    "\n",
    "        a_probs = []\n",
    "        b_probs = []\n",
    "        split_filepaths = [[], []]\n",
    "        total_offset, curr_idx = 0, 0\n",
    "        for j in range(len(features_scenes)):\n",
    "            if j - total_offset >= boundary_scenes[curr_idx]:\n",
    "                total_offset += boundary_scenes[curr_idx]\n",
    "                curr_idx += 1\n",
    "            if labels[j] == most_acc_attr:\n",
    "                a_probs.append(probs[j][most_acc_attr])\n",
    "                split_filepaths[0].append(scene_filepaths[i][int(labels[j])][j - total_offset])\n",
    "            else:\n",
    "                b_probs.append(-probs[j][most_acc_attr])\n",
    "                split_filepaths[1].append(scene_filepaths[i][int(labels[j])][j - total_offset])\n",
    "                \n",
    "        a_indices = np.argsort(np.array(a_probs))\n",
    "        b_indices = np.argsort(np.array(b_probs))\n",
    "\n",
    "        pickle.dump([a_indices, b_indices, split_filepaths, a_probs, b_probs, most_acc_attr], open(\"results/{0}/att_clu/{1}_info.pkl\".format(folder_name, names[categories[i]]), \"wb\"))\n",
    "        \n",
    "        base_acc, rand_acc, p_value = permutation_test_score(clf, projected_features_scenes, labels, scoring=\"accuracy\", n_permutations=100)\n",
    "        ratio = base_acc/np.mean(rand_acc)\n",
    "        \n",
    "        if p_value > 0.05 and ratio <= 1.2: # can tune as desired\n",
    "            continue\n",
    "\n",
    "        amount = len(features_instances)\n",
    "        phrase = [ratio, names[categories[i]], acc, p_value, len(features_instances), num_features]\n",
    "        value_to_phrase[i] = phrase\n",
    "        \n",
    "        total_offset = 0\n",
    "        scenes_per_attr = [[[] for j in range(num_attrs)] for i in range(len(scene_classes))]\n",
    "        for c in range(num_attrs):\n",
    "            for j in range(boundary_scenes[c]):\n",
    "                this_scene = scene_filepaths[i][c][j][1]\n",
    "                scenes_per_attr[this_scene][c].append(np.absolute(scaled_probs[j + total_offset][c]))\n",
    "            total_offset += boundary_scenes[c]\n",
    "            \n",
    "        for j in range(len(scene_classes)):\n",
    "            dists = [scenes_per_attr[j][k] for k in range(num_attrs)]\n",
    "            all_a = [np.zeros(len(scenes[i][k])) for k in range(num_attrs)]\n",
    "            for d, a in zip(dists, all_a):\n",
    "                a[:len(d)] = 1\n",
    "            for k in range(num_attrs):\n",
    "                for l in range(k + 1, num_attrs):\n",
    "                    _, p = stats.ttest_ind(all_a[k], all_a[l])\n",
    "                    if not np.isnan(p):\n",
    "                        value_to_scenephrase[p] = [names[categories[i]], scene_classes[j], len(dists[k]), len(all_a[k]), len(dists[l]), len(all_a[l]), attr_names[k], attr_names[l]]\n",
    "        \n",
    "    pickle.dump([value_to_phrase, value_to_scenephrase], open(\"checkpoints/{}/att_clu.pkl\".format(folder_name), 'wb'))\n",
    "else:\n",
    "    value_to_phrase, value_to_scenephrase = pickle.load(open(\"checkpoints/{}/att_clu.pkl\".format(folder_name), 'rb'))\n",
    "\n",
    "def label_svm_qual(category, num):\n",
    "    to_save = False\n",
    "    ratio, name, acc, p_value, num_examples, num_features = value_to_phrase[category]\n",
    "    a_indices, b_indices, split_filepaths, a_probs, b_probs, most_acc_attr = pickle.load(open(\"results/{0}/att_clu/{1}_info.pkl\".format(folder_name, name), \"rb\"))\n",
    "    print_statement = \"{3}: Accuracy: {0}%, with p={1}, {2}x and {3} features\".format(round(acc*100., 3), round(p_value, 3), round(ratio, 3), num_features, name)\n",
    "    attr_statement = \"\\\"{0}\\\" had the highest classification accuracy\\n\".format(attr_names[most_acc_attr])\n",
    "    if num is None and first_pass:\n",
    "        to_save = True\n",
    "        num = 5\n",
    "        to_write[6] = [\"(att_clu) To discern if there is an appearance difference in how attributes are imaged with an object, we extract scene-level features from each image, and fit a linear SVM to distinguish between the them.\\nAn example of the most linearly separable object between attributes: {}\".format(name), print_statement]\n",
    "    else:\n",
    "        print(print_statement)\n",
    "        print(attr_statement)\n",
    "        \n",
    "    the_indices = [a_indices, b_indices]\n",
    "    the_probs = [a_probs, b_probs]\n",
    "    \n",
    "    def display_chunk(b=0, correct=True, to_save=False, name=None):\n",
    "        this_filepaths = split_filepaths[b]\n",
    "        this_indices = the_indices[b]\n",
    "        this_probs = the_probs[b]\n",
    "        collected_filepaths = []\n",
    "        \n",
    "        if correct:\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter = -1\n",
    "        while len(collected_filepaths) < num:\n",
    "            try:\n",
    "                index = this_indices[counter]\n",
    "            except:\n",
    "                break\n",
    "            file_path = this_filepaths[index][0]\n",
    "            if (this_probs[index] > 0 and correct) or (this_probs[index] < 0 and not correct):\n",
    "                collected_filepaths.append(file_path)\n",
    "            if correct:\n",
    "                counter += 1\n",
    "            else:\n",
    "                counter += -1\n",
    "        if to_save and first_pass:\n",
    "            this_loc = \"results/{0}/{1}/att_clu_{2}.png\".format(folder_name, save_loc, name)\n",
    "            if len(collected_filepaths) > 0:\n",
    "                fig = plt.figure(figsize=(16, 8))\n",
    "                for i in range(num):\n",
    "                    ax = fig.add_subplot(1, num, i+1)\n",
    "                    ax.axis(\"off\")\n",
    "                    if i >= len(collected_filepaths):\n",
    "                        image = np.ones((3, 3, 3))\n",
    "                    else:\n",
    "                        image, _ = dataset.from_path(collected_filepaths[i])\n",
    "                        image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "                    im = ax.imshow(image, extent=SAME_EXTENT)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(this_loc, bbox_inches = 'tight')\n",
    "                plt.close()\n",
    "            else:\n",
    "                os.system(\"cp util_files/no_images.png {}\".format(this_loc))\n",
    "        elif len(collected_filepaths) > 0:\n",
    "            display_filepaths(collected_filepaths, width = 800//len(collected_filepaths), height=800//len(collected_filepaths))\n",
    "        else:\n",
    "            print(\"No images in this category\")\n",
    "            \n",
    "    \n",
    "    if not to_save:\n",
    "        print(\"{}: Correct\".format(attr_names[most_acc_attr]))\n",
    "    else:\n",
    "        to_write[6].append(\"{}: Correct\".format(attr_names[most_acc_attr]))\n",
    "    display_chunk(0, True, to_save, 'a')\n",
    "    if not to_save:\n",
    "        print(\"{}: Incorrect\".format(attr_names[most_acc_attr]))\n",
    "    else:\n",
    "        to_write[6].append(\"{}: Incorrect\".format(attr_names[most_acc_attr]))\n",
    "    display_chunk(0, False, to_save, 'b')\n",
    "    if not to_save:\n",
    "        print(\"Not-{}: Correct\".format(attr_names[most_acc_attr]))\n",
    "    else:\n",
    "        to_write[6].append(\"Not-{}: Correct\".format(attr_names[most_acc_attr]))\n",
    "    display_chunk(1, True, to_save, 'c')\n",
    "    if not to_save:\n",
    "        print(\"Not-{}: Incorrect\".format(attr_names[most_acc_attr]))\n",
    "    else:\n",
    "        to_write[6].append(\"Not-{}: Incorrect\".format(attr_names[most_acc_attr]))\n",
    "    display_chunk(1, False, to_save, 'd')\n",
    "    \n",
    "cat_svm_options = []\n",
    "most_different_cat_value = 1.2\n",
    "most_different_cat = None\n",
    "\n",
    "for index, phrase in sorted(value_to_phrase.items(), key=lambda kv: kv[1][0], reverse=True):\n",
    "    ratio, name, acc, p_value, num_examples, num_features = value_to_phrase[index]\n",
    "    if acc > .75 and ratio > most_different_cat_value and num_features > 5:\n",
    "        most_different_cat_value = ratio\n",
    "        most_different_cat = index\n",
    "    if num_features > 4:\n",
    "        cat_svm_options.append(('{0}: {1}% and {2}x'.format(name, round(100.*acc, 2), round(ratio, 3)), index))\n",
    "\n",
    "def instance_diff_by_scene(topn):\n",
    "    print(\"\\nInstance differences by scene between attributes\\n\")\n",
    "    i = 0\n",
    "    for value, phr in sorted(value_to_scenephrase.items(), key=lambda kv: kv[0], reverse=False):\n",
    "        pair = \"({0}, {1})\".format(phr[6], phr[7])\n",
    "        print(\"{9:15} {0} p-value, {1}: in {2} for {3} of {4} {7}, and {5} of {6} {8}\".format(dec_to_show(value), phr[0], phr[1], phr[2], phr[3], phr[4], phr[5], phr[6], phr[7], pair[0:15]))\n",
    "        if i == topn:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"att_clu_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitative examples of the most linearly separable objects once featurized, by attribute. Attributes are predicted using a one-vs-rest scheme, where a model is fit for each attribute (against the other attributes) and the model with the highest score is taken as the predicted label. Permutation test p-values are given to check for random overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_qual_widget = widgets.IntSlider(min=1, max=20, step=1, value=5)\n",
    "cat_choice_widget = widgets.Dropdown(options=cat_svm_options, layout=Layout(width='400px'))\n",
    "all_things = [widgets.Label('Category, p_value, acc',layout=Layout(padding='0px 0px 0px 5px', width='200px')), cat_choice_widget, widgets.Label('Num',layout=Layout(padding='0px 5px 0px 40px', width='80px')), num_qual_widget]\n",
    "\n",
    "if first_pass and most_different_cat is not None:\n",
    "    label_svm_qual(most_different_cat, None)\n",
    "ui = HBox(all_things)\n",
    "out = widgets.interactive_output(label_svm_qual, {'category': cat_choice_widget, 'num': num_qual_widget})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences in scene between attribute for an object. Analyses on random sample of up to 500 for each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(instance_diff_by_scene, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# att_scn Metric: Scenes\n",
    "<a id=\"att_scn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"att_scn_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide att_scn code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_info = pickle.load(open('results/{}/att_scn.pkl'.format(folder_name), 'rb'))\n",
    "scenes_per = stats_info['scenes_per']\n",
    "info = pickle.load(open('util_files/places_scene_info.pkl', 'rb'))\n",
    "idx_to_scene = info['idx_to_scene']\n",
    "idx_to_scenegroup = info['idx_to_scenegroup']\n",
    "sceneidx_to_scenegroupidx = info['sceneidx_to_scenegroupidx']\n",
    "\n",
    "xaxis = [idx_to_scenegroup[i] for i in range(len(idx_to_scenegroup))]\n",
    "xaxis = ['\\n'.join(textwrap.wrap(chunk, width=30)) for chunk in xaxis]\n",
    "barWidth = .6\n",
    "fontsize=10\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "r1 = np.arange(len(scenes_per))\n",
    "\n",
    "scenes_attribute = [None]*num_attrs\n",
    "for a in range(num_attrs):\n",
    "    scenes_attribute[a] = np.array([scenes_per[i][a] for i in range(len(r1))]) / dataset.num_attribute_images[a]\n",
    "\n",
    "if ordinal:\n",
    "    cat_ordinal_options, p_trend = [], []\n",
    "    for i in range(len(xaxis)):\n",
    "        _, _, _, p, _ = stats.linregress([x for x in range(len(axis))], np.array(scenes_attribute)[axis][:, i])\n",
    "        p_trend.append(p)\n",
    "        cat_ordinal_options.append((f\"{xaxis[i]}, p={dec_to_show(p)}\", i))\n",
    "\n",
    "    cat_ordinal_options = [x for _, x in sorted(zip(p_trend, cat_ordinal_options))]\n",
    "\n",
    "def show_scenes():\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    scene_attr_ratios = [0 for i in range(len(scenes_attribute[0]))]\n",
    "    for scene in range(len(scenes_attribute[0])):\n",
    "        max_scene = 0\n",
    "        min_scene = float('inf')\n",
    "        for attr in range(num_attrs):\n",
    "            if scenes_attribute[attr][scene] > max_scene:\n",
    "                max_scene = scenes_attribute[attr][scene]\n",
    "            if scenes_attribute[attr][scene] < min_scene:\n",
    "                min_scene = scenes_attribute[attr][scene]\n",
    "        scene_attr_ratios[scene] = max_scene / min_scene\n",
    "                \n",
    "    order = np.argsort(scene_attr_ratios)\n",
    "   \n",
    "    r1 = np.arange(len(scenes_per))\n",
    "    r1 = [r1[i]+i*(num_attrs)*barWidth for i in range(len(r1))]\n",
    "    r = [r1]\n",
    "    for i in range(1, num_attrs+1):\n",
    "        r.append([x + barWidth for x in r[len(r)-1]]) \n",
    "    r = np.array(r)\n",
    "    \n",
    "    for i in range(num_attrs-1, -1, -1):\n",
    "        plt.barh(r[i], scenes_attribute[i][order], height=barWidth, color=COLORS[i], edgecolor='white', label=attr_names[i])\n",
    "    ticks = r[0]+(num_attrs/2)*barWidth\n",
    "    \n",
    "    plt.yticks(ticks, np.array(xaxis)[order], fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.ylabel('Scene', fontsize=fontsize, labelpad=20)\n",
    "    plt.xlabel('Fraction of Images with this Scene', fontsize=fontsize, labelpad=20, x=.3)\n",
    "    plt.legend(loc='best', fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "    plt.gcf().subplots_adjust(bottom=0.15)\n",
    "    plt.gcf().subplots_adjust(left=0.5)\n",
    "    if first_pass:\n",
    "        to_write[7] = [\"(att_scn) Distribution of scenes that each attribute appears in, sorted by ratio of the max and min attribute counts of the scene.\"]\n",
    "        plt.savefig(\"results/{0}/{1}/5.png\".format(folder_name, save_loc))\n",
    "    plt.show()\n",
    "    \n",
    "def disp_ordinal_scn_trends(value):\n",
    "    def ordinal_scn_plt(category):\n",
    "        fig=plt.figure()\n",
    "        ax=fig.add_subplot(111, label='mean')\n",
    "\n",
    "        ax.plot(np.array(attr_names)[axis], np.array(scenes_attribute)[axis][:, category], color=\"C0\")[0]\n",
    "        ax.set_xlabel(\"Attribute\")\n",
    "        ax.set_ylabel(\"Fraction\")\n",
    "\n",
    "        plt.title(f\"Fraction of Images with '{xaxis[category]}' by Attribute\");\n",
    "        plt.show()\n",
    "        \n",
    "    clear_output()\n",
    "    toggle = widgets.ToggleButtons(options=[('Bar Plot', False), ('Ordinal Graph', True)], \n",
    "                                   value=value['new'],\n",
    "                                   tooltips=['Display bar plot with counts of each category by attribute',\n",
    "                                             'Display graph showing a statisically significant trend along an ordinal axis'])\n",
    "    ui = HBox([toggle])\n",
    "    display(ui)\n",
    "    if value['new']:\n",
    "        ord_selection_widget = widgets.Dropdown(options=cat_ordinal_options)\n",
    "        ui_graph = HBox([ord_selection_widget])\n",
    "        out_graph = widgets.interactive_output(ordinal_scn_plt, {'category': ord_selection_widget})\n",
    "        display(ui_graph, out_graph)\n",
    "    else:\n",
    "        show_scenes()\n",
    "\n",
    "    toggle.observe(disp_ordinal_scn_trends, 'value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"att_scn_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenes that each attribute appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if dataset.group_mapping is not None:\n",
    "    if ordinal:\n",
    "        disp_ordinal_scn_trends({'new': False})\n",
    "    else:\n",
    "        show_scenes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up summary pdf\n",
    "<a id=\"summarypdf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pass = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pdf(numbers):\n",
    "    for i in numbers:\n",
    "        if i in to_write.keys():\n",
    "            if i not in [1, 6]:\n",
    "                for sentence in to_write[i]:\n",
    "                    pdf.write(5, sentence)\n",
    "                    pdf.ln()\n",
    "            if i == 0:\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[1][0])\n",
    "                pdf.ln()\n",
    "            elif i == 1:\n",
    "                continue\n",
    "            elif i == 2:\n",
    "                pdf.image('results/{0}/{1}/0.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 3:\n",
    "                pdf.image('results/{0}/{1}/1.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 4:\n",
    "                pdf.image('results/{0}/{1}/2.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "            elif i == 5:\n",
    "                pdf.image('results/{0}/{1}/3.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 6:\n",
    "                pdf.write(5, to_write[i][0])\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][1])\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][2])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/4_a.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][3])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/4_b.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][4])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/4_c.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][5])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/4_d.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "            elif i == 7:\n",
    "                pdf.image('results/{0}/{1}/5.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            pdf.ln(h=3)\n",
    "            pdf.dashed_line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "            pdf.ln(h=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font('Arial', 'B', 16)\n",
    "pdf.write(5, \"Attribute-Based Summary\")\n",
    "pdf.ln()\n",
    "pdf.ln()\n",
    "\n",
    "# Overview Statistics\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Overview Statistics\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "write_pdf([3, 7])\n",
    "\n",
    "# Interesting findings\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Sample Interesting Findings\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "write_pdf([0, 1, 2, 4, 5, 6])\n",
    "\n",
    "# Interesting findings\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Some of the other metrics in the notebook\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "pdf.write(5, \"- (att_cnt) Cooccurrence differences of objects between attributes\")\n",
    "pdf.ln()\n",
    "pdf.write(5, \"- (att_clu) Scene differences per object between attributes\")\n",
    "pdf.ln()\n",
    "\n",
    "\n",
    "pdf.output('results/{0}/{1}/summary.pdf'.format(folder_name, save_loc), \"F\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
