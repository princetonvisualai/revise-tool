{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. Fill in the dataset in section 1.1\n",
    "\n",
    "2. Run all cells\n",
    "\n",
    "3. Review the summary pdf generated AND/OR explore each metric below.\n",
    "    - All metrics are identified by a short keyword, and consist of a \"Setup\" and \"Analyses\" portion. The \"Setup\" portion contains code that does not need to be modified unless customization is needed, and the \"Analyses\" portion provides an interactive display of the results. \n",
    "\n",
    "## Table of Contents\n",
    "1. [Initial Setup](#setup) <br/>\n",
    "    1.1 [Dataset](#dataset)\n",
    "2. obj_cnt Metric: [Object counts, duplicate annotations, object cooccurrences](#obj_cnt)<br/>\n",
    "    2.1 [Setup](#obj_cnt_setup)<br/>\n",
    "    2.2 [Analyses](#obj_cnt_analyses)\n",
    "3. obj_siz Metric: [Size and distance from center of supercategories](#obj_siz))<br/>\n",
    "    3.1 [Setup](#obj_siz_setup)<br/>\n",
    "    3.2 [Analyses](#obj_siz_analyses)\n",
    "4. obj_ppl Metric: [Supercategories w/wo people](#obj_ppl)<br/>\n",
    "    4.1 [Setup](#obj_ppl_setup)<br/>\n",
    "    4.2 [Analyses](#obj_ppl_analyses)\n",
    "5. obj_scn Metric: [Scenes and object appearance diversity](#obj_scn)<br/>\n",
    "    5.1 [Setup](#obj_scn_setup)<br/>\n",
    "    5.2 [Analyses](#obj_scn_analyses)\n",
    "6. [Setting up summary pdf](#summarypdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup \n",
    "<a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "try: # only change dir if necessary\n",
    "    import datasets\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir(os.pardir)\n",
    "    import datasets\n",
    "import pickle\n",
    "import itertools\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import PIL.Image\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from math import sqrt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "from scipy.spatial.distance import squareform\n",
    "import pycountry\n",
    "from geonamescache import GeonamesCache\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "import textwrap\n",
    "import matplotlib.patches as mpatches\n",
    "import operator\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import imageio\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, HBox, Layout\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "from matplotlib.transforms import Bbox\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = sns.color_palette('Set2', 2)\n",
    "SAME_EXTENT = (-0.5, 6.5, -0.5, 6.5)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not os.path.exists(\"dataloader_files\"):\n",
    "    os.mkdir(\"dataloader_files\")\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook\n",
    "def hide_toggle(for_next=False, toggle_text='Toggle show/hide'):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide helper functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder(num, folder):\n",
    "    if not os.path.exists(\"results/{0}/{1}\".format(folder, num)):\n",
    "        os.mkdir(\"results/{0}/{1}\".format(folder, num))\n",
    "    file = open(\"results/{0}/{1}/results.txt\".format(folder, num), \"w\")\n",
    "    return file\n",
    "\n",
    "# Projecting a set of features into a lower-dimensional subspace with PCA\n",
    "def project(features, dim):\n",
    "    standardized = StandardScaler().fit_transform(features)\n",
    "    pca = PCA(n_components=dim)\n",
    "    principalComponents = pca.fit_transform(X=standardized)\n",
    "    return principalComponents\n",
    "\n",
    "# Calculating the binomial proportion confidence interval\n",
    "def wilson(p, n, z = 1.96):\n",
    "    denominator = 1 + z**2/n\n",
    "    centre_adjusted_probability = p + z*z / (2*n)\n",
    "    adjusted_standard_deviation = sqrt((p*(1 - p) + z*z / (4*n)) / n)\n",
    "    \n",
    "    lower_bound = (centre_adjusted_probability - z*adjusted_standard_deviation) / denominator\n",
    "    upper_bound = (centre_adjusted_probability + z*adjusted_standard_deviation) / denominator\n",
    "    return (lower_bound, upper_bound)\n",
    "\n",
    "def country_to_iso3(country):\n",
    "    missing = {'South+Korea': 'KOR',\n",
    "            'North+Korea': 'PRK',\n",
    "            'Laos': 'LAO',\n",
    "            'Caribbean+Netherlands': 'BES',\n",
    "            'St.+Lucia': 'LCA',\n",
    "            'East+Timor': 'TLS',\n",
    "            'Democratic+Republic+of+Congo': 'COD',\n",
    "            'Swaziland': 'SWZ',\n",
    "            'Cape+Verde': 'CPV',\n",
    "            'C%C3%B4te+d%C2%B4Ivoire': 'CIV',\n",
    "            'Ivory+Coast': 'CIV',\n",
    "            'Channel+Islands': 'GBR'\n",
    "            }\n",
    "    try:\n",
    "        iso3 = pycountry.countries.search_fuzzy(country.replace('+', ' '))[0].alpha_3\n",
    "    except LookupError:\n",
    "        try:\n",
    "            iso3 = missing[country]\n",
    "        except KeyError:\n",
    "            iso3 = None\n",
    "    return iso3\n",
    "\n",
    "def full_extent(ax, pad=0.0):\n",
    "    \"\"\"Get the full extent of an axes, including axes labels, tick labels, and\n",
    "    titles.\"\"\"\n",
    "    # For text objects, we need to draw the figure first, otherwise the extents\n",
    "    # are undefined.\n",
    "    ax.figure.canvas.draw()\n",
    "    items = ax.get_xticklabels() + ax.get_yticklabels() \n",
    "#    items += [ax, ax.title, ax.xaxis.label, ax.yaxis.label]\n",
    "    items += [ax, ax.title]\n",
    "    bbox = Bbox.union([item.get_window_extent() for item in items])\n",
    "\n",
    "    return bbox.expanded(1.0 + pad, 1.0 + pad)\n",
    "\n",
    "def display_filepaths(filepaths, width=100, height=100):\n",
    "    sidebyside = widgets.HBox([widgets.Image(value=open(filepath, 'rb').read(), format='png', width=width, height=height) for filepath in filepaths], layout=Layout(height='{}px'.format(height)))\n",
    "    display(sidebyside)\n",
    "\n",
    "# hide_toggle(toggle_text='Show/hide helper functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Fill in below with dataset and file path names\n",
    "<a id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([ \n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "dataset = datasets.CoCoDataset(transform_train)\n",
    "folder_name = 'coco_example'\n",
    "\n",
    "# dataset = datasets.OpenImagesDataset(transform_train)\n",
    "# folder_name = 'openimages_supp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = '1_pager_obj'\n",
    "os.system(\"rm -r results/{0}/{1}\".format(folder_name, save_loc))\n",
    "file = folder(save_loc, folder_name)\n",
    "first_pass = True\n",
    "to_write = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = None\n",
    "info = pickle.load(open(\"results/{}/obj_cnt.pkl\".format(folder_name), \"rb\"))\n",
    "sample_file = info['filepaths'][0][0]\n",
    "if not os.path.exists(sample_file):\n",
    "    assert data_folder is not None, \"initialize data_folder with folder path of your data\"\n",
    "    dataset.init_folder_path(data_folder)\n",
    "    print(\"overwriting from_path() function\")\n",
    "    dataset.from_path = dataset.from_path_prerun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# obj_cnt Metric: Object Counts, Duplicate Annotations, Object Co-Occurrences\n",
    "<a id=\"obj_cnt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"obj_cnt_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide obj_cnt Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset.categories\n",
    "names = dataset.labels_to_names\n",
    "if not os.path.exists(\"results/{0}/0/\".format(folder_name)):\n",
    "    os.mkdir(\"results/{0}/0/\".format(folder_name))\n",
    "info = pickle.load(open(\"results/{}/obj_cnt.pkl\".format(folder_name), \"rb\"))\n",
    "instances_size = info['instances_size']\n",
    "supercat_filepaths = info['filepaths']\n",
    "same = []\n",
    "not_same = {}\n",
    "with_people = info['with_people']\n",
    "not_with_people = info['not_with_people']\n",
    "counts = info['counts']\n",
    "overlap = info['overlap']\n",
    "threshold = .6\n",
    "for key in overlap.keys():\n",
    "    rat = overlap[key] / counts[key]\n",
    "    if overlap[key] / counts[key] > threshold:\n",
    "        same.append(key)\n",
    "    else:\n",
    "        not_same[key] = rat\n",
    "\n",
    "# Setting up the cooccurrence matrix\n",
    "counts_mat = np.zeros((len(categories), len(categories)))\n",
    "cooccurs = []\n",
    "for key in counts.keys():\n",
    "    a, b = key.split('-')\n",
    "    a, b = int(a), int(b)\n",
    "    counts_mat[b][a] = counts[key]\n",
    "    counts_mat[a][b] = counts[key]\n",
    "    if a != b:\n",
    "        cooccurs.append(counts[key])\n",
    "instance_counts = np.diagonal(counts_mat)\n",
    "\n",
    "mi_wilson = np.zeros((len(categories), len(categories)))\n",
    "mi = np.zeros((len(categories), len(categories)))\n",
    "for i in range(len(categories)):\n",
    "    for j in range(i+1, len(categories)):\n",
    "        denom = instance_counts[j] + instance_counts[i] - counts_mat[i][j]\n",
    "        mi_wilson[i][j] = wilson(counts_mat[i][j] / denom, denom)[0]\n",
    "        mi[i][j] = counts_mat[i][j]/denom\n",
    "\n",
    "normalized = np.divide(counts_mat, instance_counts)\n",
    "for i in range(len(categories)):\n",
    "    normalized[i][i] = -1\n",
    "    mi_wilson[i][i] = -1\n",
    "for people in dataset.people_labels:\n",
    "    index = categories.index(people)\n",
    "    mi_wilson[index] = -1\n",
    "    mi_wilson[:, index] = -1\n",
    "    normalized[index] = -1\n",
    "    normalized[:, index] -1\n",
    "flat_norm = normalized.flatten()\n",
    "\n",
    "def instance_counts_words(topn):\n",
    "    instance_indices = np.argsort(instance_counts)\n",
    "    print(\"Top appearances:\")\n",
    "    for i in range(topn):\n",
    "        index = instance_indices[-1-i]\n",
    "        print(\"{0}: {1}\".format(names[categories[index]], int(instance_counts[index])))\n",
    "\n",
    "    print(\"\\nBottom appearances:\")\n",
    "    for i in range(topn):\n",
    "        index = instance_indices[i]\n",
    "        print(\"{0}: {1}\".format(names[categories[index]], int(instance_counts[index])))\n",
    "\n",
    "def instance_counts_graph(log):\n",
    "    %matplotlib inline\n",
    "    if log:\n",
    "        hist, bins = np.histogram(instance_counts, bins='fd')\n",
    "        left = 0 if bins[0] == 0 else np.log10(bins[0])\n",
    "        logbins = np.logspace(left,np.log10(bins[-1]),len(bins) // 3)\n",
    "        n, bins, patches = plt.hist(instance_counts, bins=logbins)\n",
    "        plt.yticks([])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Number of Instance Occurrences')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Log Instance Occurrences')\n",
    "        z = np.abs(stats.zscore(np.log(instance_counts+1e-6)))\n",
    "        outliers = np.where(z > 3)[0]\n",
    "        if first_pass and len(outliers) > 0:\n",
    "            to_write[0] = [\"(obj_cnt) The outliers shown on the graph for instance count are:\"]\n",
    "            outliers_added = 0\n",
    "            for outlier in outliers:\n",
    "                outliers_added += 1\n",
    "                patches[max(np.digitize(instance_counts[outlier], bins, right=False) - 1, 0)].set_facecolor('C1')\n",
    "                if outliers_added < 5:\n",
    "                    to_write[0].append(\"{0}: {1}\".format(names[categories[outlier]], int(instance_counts[outlier])))\n",
    "                if outliers_added == 5:\n",
    "                    to_write[0].append(\"Look in the notebook for the rest of the outliers.\")\n",
    "            plt.savefig(\"results/{0}/{1}/1.png\".format(folder_name, save_loc))\n",
    "            for outlier in outliers:\n",
    "                patches[max(np.digitize(instance_counts[outlier], bins, right=False) - 1, 0)].set_facecolor('C0')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        plt.hist(instance_counts, bins='fd')\n",
    "        plt.xlabel('Number of Instance Occurrences')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Instance Occurrences')\n",
    "        plt.show()\n",
    "\n",
    "flat_norm = mi_wilson.flatten()\n",
    "flat_norm[flat_norm!=flat_norm] = -1.\n",
    "normalized_indices_top = np.argsort(flat_norm)\n",
    "\n",
    "flat_norm[flat_norm == -1] = float(\"inf\")\n",
    "normalized_indices_bot = np.argsort(flat_norm)\n",
    "\n",
    "def cooccurrence_counts_words(topn):\n",
    "    same_notadded = []\n",
    "    print(\"Top cooccurrences:\")\n",
    "    for i in range(topn):\n",
    "        index = normalized_indices_top[-1-i]\n",
    "        a, b = index % len(categories), index // len(categories)\n",
    "        key = '{0}-{1}'.format(b, a)\n",
    "        if key not in same:\n",
    "            round_flat_norm = 0.0 if round(flat_norm[index], 4) == 0 else round(flat_norm[index], 4)\n",
    "            print(\"{0} - {1}: {2}\".format(names[categories[a]], names[categories[b]], round_flat_norm))\n",
    "        else:\n",
    "            same_notadded.append(key)\n",
    "    \n",
    "    print(\"\\nBottom cooccurrences:\")\n",
    "    for i in range(topn):\n",
    "        index = normalized_indices_bot[i]\n",
    "        a, b = index % len(categories), index // len(categories)\n",
    "        round_flat_norm = 0.0 if round(flat_norm[index], 4) == 0 else round(flat_norm[index], 4)\n",
    "        print(\"{0} - {1}: {2}\".format(names[categories[a]], names[categories[b]], round_flat_norm))\n",
    "        \n",
    "    print('\\nNot included in above rankings because most likely the same object:')\n",
    "    if same_notadded == []:\n",
    "        print(\"N/A\")\n",
    "    for notadded in same_notadded:\n",
    "        a, b = notadded.split('-')\n",
    "        print(\"{0} - {1}\".format(names[categories[int(a)]], names[categories[int(b)]]))\n",
    "\n",
    "def cooccurence_counts_graph(log):\n",
    "    %matplotlib inline\n",
    "    if log:\n",
    "        hist, bins = np.histogram(cooccurs, bins='fd')\n",
    "        if len(bins) < 20:\n",
    "            hist, bins = np.histogram(cooccurs, bins=20)\n",
    "        left = 0 if bins[0] == 0 else np.log10(bins[0])\n",
    "        logbins = np.logspace(left,np.log10(bins[-1]),len(bins))\n",
    "        n, bins, patches = plt.hist(cooccurs, bins=logbins)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Number of Instance Cooccurrences')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Log Instance Occurrences')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        hist, bins = np.histogram(cooccurs, bins='fd')\n",
    "        if len(bins) < 20:\n",
    "            hist, bins = np.histogram(cooccurs, bins=20)\n",
    "        plt.hist(cooccurs, bins=bins)\n",
    "        plt.xlabel('Number of Instance Cooccurrences')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Cooccurrences')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "num_images = len(dataset.image_ids)\n",
    "group_mapping = dataset.group_mapping\n",
    "if group_mapping is not None:\n",
    "    num_per_group = np.zeros(len(with_people))\n",
    "    freqs_per_group = [[] for i in range(len(with_people))]\n",
    "    names_per_group = [[] for i in range(len(with_people))]\n",
    "    ps = []\n",
    "    phrases = []\n",
    "    for cat in dataset.categories:\n",
    "        num_per_group[group_mapping(cat)] += 1\n",
    "    with_people = info['with_people']\n",
    "    not_with_people = info['not_with_people']\n",
    "    number_images = np.add(with_people, not_with_people)\n",
    "    counts = info['counts']\n",
    "    for i in range(len(instance_counts)):\n",
    "        supercategory = group_mapping(dataset.categories[i])\n",
    "        freqs_per_group[supercategory].append(instance_counts[i])\n",
    "        names_per_group[supercategory].append(dataset.labels_to_names[dataset.categories[i]])\n",
    "        if num_per_group[supercategory] > 1:\n",
    "            this_counts = np.zeros(num_images)\n",
    "            this_counts[:int(instance_counts[i])] = 1\n",
    "            that_counts = np.zeros(num_images)\n",
    "            rest_counts = (number_images[supercategory] - instance_counts[i]) / (num_per_group[supercategory] - 1)\n",
    "            that_counts[:int(rest_counts)] = 1\n",
    "            p = stats.ttest_ind(this_counts, that_counts)[1]\n",
    "            p = stats.binom_test(int(instance_counts[i]), n=num_images, p=np.mean(that_counts))\n",
    "            if p < .05:\n",
    "                if np.mean(this_counts) > np.mean(that_counts):\n",
    "                    phrase = '{0} is over represented in the {1} category: {2}, {3}, {4}\\n'.format(dataset.labels_to_names[dataset.categories[i]], dataset.supercategories_to_names[supercategory], round(np.mean(this_counts), 4), round(np.mean(that_counts), 4), p)\n",
    "                else:\n",
    "                    phrase = '{0} is under represented in the {1} category: {2}, {3}, {4}\\n'.format(dataset.labels_to_names[dataset.categories[i]], dataset.supercategories_to_names[supercategory], round(np.mean(this_counts), 4), round(np.mean(that_counts), 4), p)\n",
    "\n",
    "                phrase = '{0} '.format(instance_counts[i]) + phrase\n",
    "                ps.append(p)\n",
    "                phrases.append(phrase)\n",
    "    indices = np.argsort(ps)\n",
    "    \n",
    "def within_category(category):\n",
    "    %matplotlib inline\n",
    "    \n",
    "    topn = 10\n",
    "\n",
    "    # looking at distribution within supercategory \n",
    "    fontsize = 20\n",
    "    fig = plt.figure(figsize=(14, 5))\n",
    "    grid = plt.GridSpec(1, 2, hspace=.2, wspace=.3)\n",
    "    ax1 = fig.add_subplot(grid[0, 0])\n",
    "    ax2 = fig.add_subplot(grid[0, 1])\n",
    "\n",
    "    total = with_people+not_with_people\n",
    "    names = [dataset.supercategories_to_names[i] for i in range(len(total))]\n",
    "\n",
    "    ax1.set_xlabel('Supercategory', fontsize=fontsize)\n",
    "    ax1.set_ylabel('Frequency', fontsize=fontsize)\n",
    "    ax1.set_title('Supercategories', size=fontsize)\n",
    "    order = np.argsort(total)[::-1]\n",
    "    pltbar = ax1.bar(np.arange(len(total)), np.array(total)[order], tick_label=np.array(names)[order])\n",
    "    for tick in ax1.get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "        tick.set_fontsize(fontsize)\n",
    "    for tick in ax1.get_yticklabels():\n",
    "        tick.set_fontsize(fontsize)\n",
    "    ax1.xaxis.labelpad = 10\n",
    "    ax1.yaxis.labelpad = 10\n",
    "    \n",
    "    if first_pass and not os.path.exists('results/{0}/{1}/3.png'.format(folder_name, save_loc)):\n",
    "        to_write[2] = [\"(obj_cnt) Distribution of object categories that appear in dataset.\"]\n",
    "        extent = full_extent(ax1, pad=0.3).transformed(fig.dpi_scale_trans.inverted())\n",
    "        fig.savefig('results/{0}/{1}/3.png'.format(folder_name, save_loc), bbox_inches=extent)\n",
    "\n",
    "    stds_per_group = [np.std(chunk) for chunk in freqs_per_group]\n",
    "#         peakiest_supercat = np.argmax(stds_per_group)\n",
    "    reverse = {v: k for k, v in dataset.supercategories_to_names.items()}\n",
    "    peakiest_supercat = reverse[category]\n",
    "    pltbar[list(order).index(peakiest_supercat)].set_color('C1')\n",
    "\n",
    "    ax2.set_xlabel('Instance Label', fontsize=fontsize)\n",
    "    ax2.set_ylabel('Frequency', fontsize=fontsize)\n",
    "    ax2.set_title('Within \"{}\"'.format(dataset.supercategories_to_names[peakiest_supercat]), size=fontsize)\n",
    "    freqs = freqs_per_group[peakiest_supercat]\n",
    "    order = np.argsort(freqs)[::-1]\n",
    "    ax2.bar(np.arange(len(freqs_per_group[peakiest_supercat])), np.array(freqs)[order], tick_label=np.array(names_per_group[peakiest_supercat])[order], color='C1')\n",
    "    for tick in ax2.get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "        tick.set_fontsize(fontsize)\n",
    "    for tick in ax2.get_yticklabels():\n",
    "        tick.set_fontsize(fontsize)\n",
    "    ax2.xaxis.labelpad = 10\n",
    "    ax2.yaxis.labelpad = 10\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    try: \n",
    "        filepaths = supercat_filepaths[peakiest_supercat]\n",
    "\n",
    "        fig = plt.figure(figsize=(17,5))\n",
    "        for i in range(21):\n",
    "            filepath = filepaths[i]\n",
    "            image, anns = dataset.from_path(filepath)\n",
    "            image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "            ax = fig.add_subplot(3, 7, 1+i)\n",
    "            im = ax.imshow(image, alpha=.6)\n",
    "            this_instances = set()\n",
    "            for ann in anns[0]:\n",
    "                if group_mapping(ann['label']) == peakiest_supercat:\n",
    "                    this_instances.add(dataset.labels_to_names[ann['label']])\n",
    "                    bbox = ann['bbox']\n",
    "\n",
    "                    ann_0 = (bbox[0]*image.shape[1], bbox[2]*image.shape[0])\n",
    "                    ann_w = (bbox[1]-bbox[0])*image.shape[1]\n",
    "                    ann_h = (bbox[3]-bbox[2])*image.shape[0]\n",
    "                    rect = patches.Rectangle(ann_0,ann_w, ann_h, linewidth=2,edgecolor='b',facecolor='none')\n",
    "                    ax.add_patch(rect)\n",
    "            ax.set_title(', '.join(list(this_instances)), size=10)\n",
    "            ax.axis(\"off\")\n",
    "            \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except AttributeError: \n",
    "        print('Some functionality not available for CocoDatasetNoImages Class')\n",
    "    \n",
    "\n",
    "def show_cooccurrence_hierarchy():\n",
    "    reverse = {v: k for k, v in dataset.labels_to_names.items()}\n",
    "    mi_wilson[np.isnan(mi_wilson)] = 0\n",
    "    xaxis = [dataset.labels_to_names[i] for i in categories]\n",
    "    biggest = np.amax(mi_wilson)*1.1\n",
    "    condensed_distance_matrix = []\n",
    "    for i in range(len(categories)):\n",
    "        for j in range(i+1, len(categories)):\n",
    "            condensed_distance_matrix.append(biggest - mi_wilson[i][j])\n",
    "\n",
    "    for p in [20]: # change this to have more or less labels shown\n",
    "        Z = linkage(condensed_distance_matrix, 'ward')\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        dn = dendrogram(Z, orientation='left', labels=xaxis, p=p, truncate_mode='level')\n",
    "        ax = plt.gca()\n",
    "        xlbls = ax.get_ymajorticklabels()\n",
    "        colorz = sns.color_palette('Set2', 12)\n",
    "        if dataset.group_mapping is not None:\n",
    "            for lbl in xlbls:\n",
    "                if lbl.get_text() not in reverse.keys():\n",
    "                    continue\n",
    "                ind = reverse[lbl.get_text()]\n",
    "                lbl.set_color(colorz[dataset.group_mapping(ind)])\n",
    "        plt.savefig(\"results/{0}/{1}/hierarchy_{2}.png\".format(folder_name, 'obj_cnt', p), dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Constants\n",
    "    img_width = 1600\n",
    "    img_height = 900\n",
    "    scale_factor = 0.5\n",
    "\n",
    "    # Add invisible scatter trace.\n",
    "    # This trace is added to help the autoresize logic work.\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, img_width * scale_factor],\n",
    "            y=[0, img_height * scale_factor],\n",
    "            mode=\"markers\",\n",
    "            marker_opacity=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Configure axes\n",
    "    fig.update_xaxes(\n",
    "        visible=False,\n",
    "        range=[0, img_width * scale_factor]\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        visible=False,\n",
    "        range=[0, img_height * scale_factor],\n",
    "        # the scaleanchor attribute ensures that the aspect ratio stays constant\n",
    "        scaleanchor=\"x\"\n",
    "    )\n",
    "\n",
    "    # Add image\n",
    "    fig.add_layout_image(\n",
    "        dict(\n",
    "            x=0,\n",
    "            sizex=img_width * scale_factor,\n",
    "            y=img_height * scale_factor,\n",
    "            sizey=img_height * scale_factor,\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            opacity=1.0,\n",
    "            layer=\"below\",\n",
    "            sizing=\"stretch\",\n",
    "            source=\"results/{0}/{1}/hierarchy_{2}.png\".format(folder_name, 'obj_cnt', p)))\n",
    "    \n",
    "\n",
    "    # Configure other layout\n",
    "    fig.update_layout(\n",
    "        width=img_width * scale_factor,\n",
    "        height=img_height * scale_factor,\n",
    "        margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0},\n",
    "    )\n",
    "\n",
    "    # Disable the autosize on double click because it adds unwanted margins around the image\n",
    "    # More detail: https://plot.ly/python/configuration-options/\n",
    "    fig.show(config={'doubleClick': 'reset'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"obj_cnt_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at individual object counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of images: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Median and Mean of instance counts: {0}, {1}\".format(int(np.median(instance_counts)), int(np.mean(instance_counts))))\n",
    "interact(instance_counts_words, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));\n",
    "interact(instance_counts_graph, log=widgets.ToggleButton(value=True, description=\"Toggle for log\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at cooccurrence counts in the form of numbers, graph, and a hierarchy. <br>\n",
    "Note: Scores are calculated using the Wilson score interval, which is a confidence interval for the probability that two items coocurr in the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Median and Mean of cooccurrence counts: {0}, {1}\".format(np.median(cooccurs), np.mean(cooccurs)))\n",
    "interact(cooccurrence_counts_words, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));\n",
    "interact(cooccurence_counts_graph, log=widgets.ToggleButton(value=True, description=\"Toggle for log\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing a hierarchy of terms based on their cooccurrences. Interact with the graph to zoom in and out. Can change settings of graph to have more/fewer labels in the show_cooccurrence_hierarchy function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cooccurrence_hierarchy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the supercategory level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if group_mapping is not None:\n",
    "    interact(within_category, category=widgets.Dropdown(options=sorted(list(dataset.supercategories_to_names.values())), value='accessory'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# obj_siz Metric: Size and Distance from Center of Supercategories\n",
    "<a id=\"obj_siz\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "<a id=\"obj_siz_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide obj_siz Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 15\n",
    "categories = dataset.categories\n",
    "idx_to_scenegroup = pickle.load(open('util_files/places_scene_info.pkl', 'rb'))['idx_to_scenegroup']\n",
    "info = pickle.load(open(\"results/{}/obj_cnt.pkl\".format(folder_name), \"rb\"))\n",
    "\n",
    "if dataset.group_mapping is not None:\n",
    "    sizes = info['sizes']\n",
    "    distances = info['distances']\n",
    "\n",
    "    all_sizes = np.concatenate(np.array(sizes), axis=0)\n",
    "    \n",
    "else:\n",
    "    all_sizes = []\n",
    "    for a_instance_size in instances_size:\n",
    "        for a_size in a_instance_size:\n",
    "            all_sizes.append(a_size[0])\n",
    "\n",
    "sorted_sizes = np.sort(all_sizes)\n",
    "fifth = len(sorted_sizes) // 5\n",
    "bins = [sorted_sizes[i*fifth] for i in range(5)]\n",
    "bins.append(1.00001)\n",
    "\n",
    "bar_mapping = {1: 'XS', 2: 'S', 3: 'M', 4: 'L', 5: 'XL'}\n",
    "reverse_bm = {v: k for k, v in bar_mapping.items()}\n",
    "\n",
    "instances_sizes = info['instances_size']\n",
    "instance_deviations = np.zeros(len(instances_sizes))\n",
    "for i in range(len(instances_sizes)):\n",
    "    this_sizes = [chunk[0] for chunk in instances_sizes[i]]\n",
    "    this_bins = np.digitize(this_sizes, bins)\n",
    "    _, counts = np.unique(this_bins, return_counts=True)\n",
    "    probs = counts / np.sum(counts)\n",
    "    entropy = -np.sum(np.multiply(probs, np.log2(probs+1e-6)), axis=0)\n",
    "    instance_deviations[i] = entropy\n",
    "indices = np.argsort(instance_deviations)\n",
    "counts = pickle.load(open('results/{}/obj_cnt.pkl'.format(folder_name), 'rb'))['counts']\n",
    "counts_mat = np.zeros((len(categories), len(categories)))\n",
    "cooccurs = []\n",
    "for key in counts.keys():\n",
    "    a, b = key.split('-')\n",
    "    a, b = int(a), int(b)\n",
    "    counts_mat[b][a] = counts[key]\n",
    "    counts_mat[a][b] = counts[key]\n",
    "    if a != b:\n",
    "        cooccurs.append(counts[key])\n",
    "scene_instance = pickle.load(open(\"results/{}/obj_scn.pkl\".format(folder_name), \"rb\"))['scene_instance']\n",
    "\n",
    "def mean_and_std(data, data_type):\n",
    "    %matplotlib inline\n",
    "    colorz = sns.color_palette('Set2', len(data))\n",
    "\n",
    "    f = data[0]\n",
    "    m = data[1]\n",
    "    means = []\n",
    "    stds = []\n",
    "    x = []\n",
    "    name = []\n",
    "    for i, cat in enumerate(data):\n",
    "        x.append(i)\n",
    "        means.append(np.mean(cat))\n",
    "        stds.append(np.std(cat))\n",
    "        name.append(dataset.supercategories_to_names[i])\n",
    "\n",
    "        histogram, bins = np.histogram(cat, bins='auto')\n",
    "        bin_centers = 0.5*(bins[1:] + bins[:-1])\n",
    "        area = np.trapz(histogram, x=bin_centers)\n",
    "        plt.plot(bin_centers, histogram/area, alpha=.75, label=dataset.supercategories_to_names[i], color=colorz[i])\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(data_type)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    sns.boxplot(data=data, showfliers=False) #outliers not shown \n",
    "    plt.xticks(plt.xticks()[0], name, rotation='vertical')\n",
    "    plt.xlabel('Category Groups')\n",
    "    plt.ylabel(data_type)\n",
    "    plt.tight_layout()\n",
    "    z = np.abs(stats.zscore(means))\n",
    "    outliers = np.where(z > 3)[0]\n",
    "    if data_type == 'Distances' and first_pass and len(outliers) > 0:\n",
    "        to_write[3] = [\"(obj_siz) In the graph, the following object(s) have outlier distances:\"]\n",
    "        for outlier in outliers:\n",
    "            to_write[3].append(name[outlier])\n",
    "        plt.savefig(\"results/{0}/{1}/4.png\".format(folder_name, save_loc))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def size_or_distance(metric):\n",
    "    if metric == 'size':\n",
    "        mean_and_std(sizes, 'Sizes')\n",
    "    elif metric == 'distance':\n",
    "        mean_and_std(distances, 'Distances')\n",
    "\n",
    "cat_to_ent = [('{0}: {1}'.format(dataset.labels_to_names[categories[index]], round(instance_deviations[index], 3)), index) for index in indices if len(instances_sizes[index]) > 10]\n",
    "\n",
    "def object_size(object_class, sizes):\n",
    "    try: \n",
    "        plt.clf()\n",
    "        index = object_class\n",
    "        cat = categories[index]\n",
    "        fontsize = 10\n",
    "        this_sizes = [chunk[0] for chunk in instances_sizes[index]]\n",
    "        this_bins = np.digitize(this_sizes, bins)\n",
    "        num, counts = np.unique(this_bins, return_counts=True)\n",
    "        fig = plt.figure(figsize=(5, 4))\n",
    "        ax = plt.subplot(111)\n",
    "        xticks = []\n",
    "        for j in bar_mapping.keys():\n",
    "            if j in num:\n",
    "                ax.bar(j, counts[list(num).index(j)], width=0.8, bottom=0.0, align='center', color='C0')\n",
    "            else:\n",
    "                ax.bar(j, 0, width=0.8, bottom=0.0, align='center', color='C0')\n",
    "            ax.plot(1,1,label = '{0}: {1}-{2}'.format(bar_mapping[j], round(bins[j-1], 3), round(bins[j], 3)),marker = '',ls ='')\n",
    "            xticks.append(bar_mapping[j])\n",
    "        plt.xticks(np.arange(len(xticks))+1, xticks, fontsize=fontsize)\n",
    "        plt.tick_params(labelsize=fontsize)\n",
    "        plt.xlabel('Size Bins', fontsize=fontsize)\n",
    "        plt.ylabel('Frequency', fontsize=fontsize)\n",
    "        plt.tight_layout()\n",
    "        plt.gcf().subplots_adjust(left=0.35)\n",
    "\n",
    "        if sizes is None:\n",
    "            if len(counts) == 1:\n",
    "                size_to_add = [num[0]]\n",
    "            else:\n",
    "                size_to_add = num[np.argpartition(counts, 2)[:2]]\n",
    "            plt.savefig(\"results/{0}/{1}/5.png\".format(folder_name, save_loc))\n",
    "            plt.close()\n",
    "        else:\n",
    "            size_to_add = [reverse_bm[size] for size in sizes]\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        filepaths = [instances_sizes[index][j][1] for j in range(len(instances_sizes[index])) if this_bins[j] in size_to_add]\n",
    "        if len(filepaths) == 0:\n",
    "            print(\"No images of objects in this size appear in the dataset. Please select an additional size.\")\n",
    "            return\n",
    "        other_instances = counts_mat[index]\n",
    "        other_scenes = scene_instance[:, index]\n",
    "\n",
    "        all_anns = [dataset.from_path(filepath)[1][0] for filepath in filepaths]\n",
    "        instances_per = np.array([list(set([ann['label'] for ann in anns if ann['label'] != cat])) for anns in all_anns])\n",
    "        these_instances = np.concatenate(instances_per, axis=0)\n",
    "        scenes_per = np.array([dataset.from_path(filepath)[1][4] for filepath in filepaths])\n",
    "        these_scenes = np.concatenate(scenes_per, axis=0)\n",
    "        num, counts = np.unique(these_instances, return_counts=True)\n",
    "        num = np.array([categories.index(nu) for nu in num])\n",
    "        these_instances = np.zeros(len(categories))\n",
    "        for i in range(len(num)):\n",
    "            these_instances[num[i]] = counts[i]\n",
    "\n",
    "        num, counts = np.unique(these_scenes, return_counts=True)\n",
    "        these_scenes = np.zeros(len(other_scenes))\n",
    "        for i in range(len(num)):\n",
    "            these_scenes[num[i]] = counts[i]\n",
    "\n",
    "        instance_probs = np.nan_to_num(np.divide(these_instances, other_instances))\n",
    "        instance_probs = np.nan_to_num(np.array([wilson(instance_probs[i], other_instances[i])[0] for i in range(len(instance_probs))]))\n",
    "        scene_probs = np.nan_to_num(np.divide(these_scenes, other_scenes))\n",
    "        scene_probs = np.nan_to_num(np.array([wilson(scene_probs[i], other_scenes[i])[0] for i in range(len(scene_probs))]))\n",
    "        instance_indices = np.argsort(instance_probs)\n",
    "        scene_indices = np.argsort(scene_probs)\n",
    "        i_counter = 1\n",
    "        s_counter = 1\n",
    "        imgs_to_show =3\n",
    "        i_indices = []\n",
    "        s_indices = []\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        i = 0\n",
    "        start = time.time()\n",
    "\n",
    "        # so that it displays combos even of object of interest in non-desired size\n",
    "        if sizes is not None:\n",
    "            print(\"Please wait, visualization can take ~25 seconds\")\n",
    "        filepaths = [instances_sizes[index][j][1] for j in range(len(instances_sizes[index]))]\n",
    "        things_per = np.array([dataset.from_path(filepath)[1] for filepath in filepaths])\n",
    "        scenes_per = np.array([thing[4] for thing in things_per])\n",
    "        all_anns = [thing[0] for thing in things_per]\n",
    "        instances_per = np.array([list(set([ann['label'] for ann in anns if ann['label'] != cat])) for anns in all_anns])\n",
    "        if sizes is not None:\n",
    "            print(\"Time took: {}\".format(time.time() - start))\n",
    "        fontsize = 10\n",
    "        added_filepaths = []\n",
    "        second_queries = []\n",
    "        second_probs = []\n",
    "        while i < imgs_to_show:\n",
    "            if i_counter > len(instance_indices) or s_counter > len(scene_indices):\n",
    "                break\n",
    "            if instance_probs[instance_indices[-i_counter]] < scene_probs[scene_indices[-s_counter]]:\n",
    "                s_index = scene_indices[-s_counter]\n",
    "                s_counter += 1\n",
    "                added = 0\n",
    "                for j, scenes in enumerate(scenes_per):\n",
    "                    if s_index in scenes:\n",
    "                        filepath = filepaths[j]\n",
    "                        if filepath in added_filepaths:\n",
    "                            continue\n",
    "                        added_filepaths.append(filepath)\n",
    "                        image, anns = dataset.from_path(filepath)\n",
    "                        image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "                        ax = fig.add_subplot(3, imgs_to_show, (added*imgs_to_show)+1+i)\n",
    "                        ax.clear()\n",
    "                        ax.set_title('\\n'.join(textwrap.wrap(idx_to_scenegroup[s_index], width=25)), fontsize=fontsize)\n",
    "                        ax.axis(\"off\")\n",
    "                        for ann in anns[0]:\n",
    "                            if ann['label'] == cat:\n",
    "                                bbox = ann['bbox']\n",
    "                                ann_0 = (bbox[0]*image.shape[1], bbox[2]*image.shape[0])\n",
    "                                ann_w = (bbox[1]-bbox[0])*image.shape[1]\n",
    "                                ann_h = (bbox[3]-bbox[2])*image.shape[0]\n",
    "                                rect = patches.Rectangle(ann_0,ann_w, ann_h, linewidth=2,edgecolor='#ff0000',facecolor='none')\n",
    "                                ax.add_patch(rect)\n",
    "                        im = ax.imshow(image, alpha=.66)\n",
    "                        added += 1\n",
    "                    if added == 3:\n",
    "                        second_queries.append('\\n'.join(textwrap.wrap(idx_to_scenegroup[s_index], width=20)))\n",
    "                        second_probs.append(scene_probs[s_index])\n",
    "                        break\n",
    "            else:\n",
    "                i_index = instance_indices[-i_counter]\n",
    "                i_counter += 1\n",
    "                added = 0\n",
    "                for j, instances in enumerate(instances_per):\n",
    "                    if categories[i_index] in instances:\n",
    "                        filepath = filepaths[j]\n",
    "                        if filepath in added_filepaths:\n",
    "                            continue\n",
    "                        added_filepaths.append(filepath)\n",
    "                        image, anns = dataset.from_path(filepath)\n",
    "                        image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "                        ax = fig.add_subplot(3, imgs_to_show, (added*imgs_to_show)+1+i)\n",
    "                        ax.clear()\n",
    "                        ax.set_title(dataset.labels_to_names[categories[i_index]], fontsize=fontsize)\n",
    "                        ax.axis(\"off\")\n",
    "                        for ann in anns[0]:\n",
    "                            if ann['label'] == cat:\n",
    "                                bbox = ann['bbox']\n",
    "                                ann_0 = (bbox[0]*image.shape[1], bbox[2]*image.shape[0])\n",
    "                                ann_w = (bbox[1]-bbox[0])*image.shape[1]\n",
    "                                ann_h = (bbox[3]-bbox[2])*image.shape[0]\n",
    "                                rect = patches.Rectangle(ann_0,ann_w, ann_h, linewidth=2,edgecolor='#ff0000',facecolor='none')\n",
    "                                ax.add_patch(rect)\n",
    "                        im = ax.imshow(image, alpha=.66)\n",
    "                        added += 1\n",
    "                    if added == 3:\n",
    "                        second_queries.append(dataset.labels_to_names[categories[i_index]])\n",
    "                        second_probs.append(instance_probs[i_index])\n",
    "                        break\n",
    "            if added == 3:\n",
    "                i += 1\n",
    "        if sizes is None:\n",
    "            plt.savefig(\"results/{0}/{1}/6.png\".format(folder_name, save_loc))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print()\n",
    "\n",
    "        # graph the new probability using second_queries and second_probs\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        plt.barh(np.arange(len(second_probs))[::-1], second_probs, tick_label=second_queries)\n",
    "        plt.ylabel('Query Term', fontsize=fontsize)\n",
    "        plt.xticks(fontsize=fontsize)\n",
    "        plt.yticks(fontsize=fontsize)\n",
    "        if sizes is None:\n",
    "            size_names = [bar_mapping[size_add] for size_add in size_to_add]\n",
    "            plt.xlabel('Conditional Probability\\n{0} is {1}'.format(dataset.labels_to_names[cat], ', '.join(list(size_names))), fontsize=fontsize)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"results/{0}/{1}/7.png\".format(folder_name, save_loc))\n",
    "            plt.close()\n",
    "            to_write[4] = [\"(obj_siz) {0} has the least uniform size distribution.\\nShown below is the size distribution for this object, what kinds of pairwise queries are recommended to augment the dataset for more uniform sizing, and qualitative examples of these pairs.\\nPairwise queries take the form of \\\"[Object 1] + [Object 2]\\\"\".format(dataset.labels_to_names[cat])]\n",
    "        else:\n",
    "            plt.xlabel('Conditional Probability\\n{0} is {1}'.format(dataset.labels_to_names[cat], ', '.join(list(sizes))), fontsize=fontsize)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    except AttributeError:\n",
    "        print('Some functionality not available for CocoDataNoImages Class')\n",
    "sizes_widget = widgets.SelectMultiple(\n",
    "    options=['XS', 'S', 'M', 'L', 'XL'],\n",
    "    value=['XS'],\n",
    "    description='',\n",
    "    disabled=False\n",
    ")\n",
    "object_class_widget = widgets.Dropdown(options=cat_to_ent,layout=Layout(width='200px'))\n",
    "all_things = [widgets.Label('[Object]: [amount of size distribution]',layout=Layout(padding='0px 0px 0px 5px', width='270px')), object_class_widget, widgets.Label('Sizes: select with cmd',layout=Layout(padding='0px 5px 0px 40px', width='260px')), sizes_widget]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"obj_siz_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at size (percent of image's pixels) and distance (from center) by object category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if dataset.group_mapping is not None:\n",
    "    interact(size_or_distance, metric=widgets.Dropdown(options=['distance', 'size'], value='distance'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actionable pairwise queries about how to equalize sizes for a particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_pass:\n",
    "    object_size(cat_to_ent[0][1], None)\n",
    "ui = HBox(all_things)\n",
    "out = widgets.interactive_output(object_size, {'object_class': object_class_widget, 'sizes': sizes_widget})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# obj_ppl Metric: Supercategories w/wo people\n",
    "<a id=\"obj_ppl\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"obj_ppl_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide obj_ppl Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pickle.load(open(\"results/{}/obj_cnt.pkl\".format(folder_name), \"rb\"))\n",
    "\n",
    "with_people = info['with_people']\n",
    "not_with_people = info['not_with_people']\n",
    "with_people_instances = info['with_people_instances']\n",
    "counts = info['counts']\n",
    "\n",
    "x, means, stds, name = [], [], [], []\n",
    "\n",
    "# Visualize how each supercategory is represented with people\n",
    "if dataset.group_mapping is not None:\n",
    "    for i in range(len(with_people)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        x.append(i)\n",
    "        total = with_people[i]+not_with_people[i]\n",
    "        p = with_people[i] / total\n",
    "        means.append(p)\n",
    "        stds.append((p*(1.-p))/total)\n",
    "        name.append(dataset.supercategories_to_names[i])\n",
    "\n",
    "def fraction_with_people():\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    fontsize = 20\n",
    "    plt.xlabel('Supercategory', fontsize=fontsize, labelpad=20)\n",
    "    plt.ylabel('Fraction with People', fontsize=fontsize, labelpad=20, y=0.29)\n",
    "    plt.bar(x, means, yerr=stds, tick_label=name)\n",
    "    plt.xticks(rotation='vertical', fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.title(\"The Fraction of Images in Each Category of Objects Represented with People\")\n",
    "    plt.tight_layout()\n",
    "    if first_pass:\n",
    "        to_write[5] = [\"(obj_ppl) Distribution of how often object categories are represented with people.\"]\n",
    "        plt.savefig(\"results/{0}/{1}/8.png\".format(folder_name, save_loc))\n",
    "    plt.show()\n",
    "\n",
    "group_mapping = dataset.group_mapping\n",
    "if group_mapping is not None:\n",
    "    sigsOver = {}\n",
    "    sigsUnder = {}\n",
    "    for i in range(len(dataset.categories)):\n",
    "        supercategory = group_mapping(dataset.categories[i])\n",
    "        instance_percent = with_people_instances[i] / counts['{0}-{1}'.format(i, i)]\n",
    "        supercat_percent = with_people[supercategory] / (with_people[supercategory]+not_with_people[supercategory])\n",
    "        p = stats.binom_test(with_people_instances[i], n=counts['{0}-{1}'.format(i, i)], p=supercat_percent)\n",
    "        if p < .05:\n",
    "            if instance_percent < supercat_percent:\n",
    "                phrase = \"- {0} is underrepresented with people within {1}: {2}, {3}\".format(dataset.labels_to_names[dataset.categories[i]].upper(), dataset.supercategories_to_names[supercategory].upper(), round(instance_percent, 2), round(supercat_percent, 2))\n",
    "                sigsUnder[phrase] = p\n",
    "            else:\n",
    "                phrase = \"- {0} is overrepresented with people within {1}: {2}, {3}\".format(dataset.labels_to_names[dataset.categories[i]].upper(), dataset.supercategories_to_names[supercategory].upper(), round(instance_percent, 2), round(supercat_percent, 2))\n",
    "                sigsOver[phrase] = p\n",
    "\n",
    "def represented_with_people(topn):\n",
    "    print(\"\\nThe first fraction is this object's representation with people, second fraction is the object category's. \\nListed in order of statistical significance.\")\n",
    "    i = 1\n",
    "    if first_pass:\n",
    "        to_write[6] = [\"(obj_ppl) The strongest deviations of an object from its category being represented with people. The first fraction is this object's representation with people, and the second is the object category's:\\n\"]\n",
    "        for phrase, p in sorted(sigsOver.items(), key=lambda x: x[1], reverse=False)[:4]:\n",
    "            to_write[6].append(phrase)\n",
    "        \n",
    "        to_write[6].append(\"\\n\")\n",
    "        \n",
    "        for phrase, p in sorted(sigsUnder.items(), key=lambda x: x[1], reverse=False)[:4]:\n",
    "            to_write[6].append(phrase)\n",
    "    print(\"\\nOVERrepresentation of instances with people within a supercategory\\n\")\n",
    "    for phrase, p in sorted(sigsOver.items(), key=lambda x: x[1], reverse=False):\n",
    "        print(phrase)\n",
    "        if i == topn:\n",
    "            break\n",
    "        i += 1\n",
    "    i = 1\n",
    "    print(\"\\nUNDERrepresentation of instances with people within a supercategory\\n\")\n",
    "    for phrase, p in sorted(sigsUnder.items(), key=lambda x: x[1], reverse=False):\n",
    "        print(phrase)\n",
    "        if i == topn:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"obj_ppl_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which categories of objects are imaged with people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if dataset.group_mapping is not None:\n",
    "    fraction_with_people()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which specific objects are over/under represented with people within their object category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.group_mapping is not None:\n",
    "    interact(represented_with_people, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# obj_scn Metric: Scenes and Object Appearance Diversity\n",
    "<a id=\"obj_scn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"obj_scn_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide obj_scn Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 10\n",
    "scene_info = pickle.load(open(\"results/{}/obj_scn.pkl\".format(folder_name), \"rb\"))\n",
    "scene_counts = scene_info['scenes']\n",
    "scene_supercategory = scene_info['scene_supercategory']\n",
    "scene_instance = scene_info['scene_instance']\n",
    "supercat_to_scenes_to_features = scene_info['supercat_to_scenes_to_features']\n",
    "\n",
    "supercategory_info = pickle.load(open(\"results/{}/obj_cnt.pkl\".format(folder_name), \"rb\"))\n",
    "supercategory_counts = np.add(supercategory_info['with_people'], supercategory_info['not_with_people'])\n",
    "\n",
    "info = pickle.load(open('util_files/places_scene_info.pkl', 'rb'))\n",
    "idx_to_scene = info['idx_to_scene']\n",
    "idx_to_scenegroup = info['idx_to_scenegroup']\n",
    "sceneidx_to_scenegroupidx = info['sceneidx_to_scenegroupidx']\n",
    "\n",
    "entropy_per_instance = np.zeros(len(dataset.categories))\n",
    "totals_per_instance = np.sum(scene_instance, axis=0)\n",
    "scene_probs = np.divide(scene_instance, totals_per_instance)\n",
    "entropy = -np.sum(np.multiply(scene_probs, np.log2(scene_probs+1e-6)), axis=0)\n",
    "indices = np.argsort(entropy)\n",
    "cat_to_ent = [('{0}: {1}'.format(dataset.labels_to_names[dataset.categories[index]], round(entropy[index], 3)), index) for index in indices if totals_per_instance[index] > 30]\n",
    "instance_filepaths = pickle.load(open(\"results/{}/obj_cnt.pkl\".format(folder_name), 'rb'))['instances_size']\n",
    "\n",
    "class SceneQual():\n",
    "    def __init__(self):\n",
    "        self.label = None\n",
    "        interact(self.obj_scene_div, label=widgets.Dropdown(options=cat_to_ent));\n",
    "\n",
    "    def obj_scene_div(self, label):\n",
    "        try: \n",
    "            self.label = label\n",
    "            fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "            filepaths = np.unique(np.array([chunk[1] for chunk in instance_filepaths[label]]))\n",
    "            random.shuffle(filepaths)\n",
    "            for i in range(30):\n",
    "                filepath = filepaths[i]\n",
    "                if i < 15:\n",
    "                    image, anns = dataset.from_path(filepath)\n",
    "                    image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "                    ax = fig.add_subplot(3, 5, 1+i)\n",
    "                    ax.axis(\"off\")\n",
    "                    im = ax.imshow(image, extent=SAME_EXTENT)\n",
    "            plt.show()\n",
    "        except AttributeError: \n",
    "            print('Some functionality not available for CocoDatasetNoImages Class')\n",
    "        \n",
    "    def click(self, b):  \n",
    "        if b != '':\n",
    "            clear_output()\n",
    "            interact(self.obj_scene_div, label=widgets.Dropdown(options=cat_to_ent, value=self.label));\n",
    "            \n",
    "        refresh_button = widgets.Button(description=\"Click to refresh examples\", layout=Layout(width='300px'))\n",
    "        refresh_button.on_click(self.click)\n",
    "        output = widgets.Output()\n",
    "        display(refresh_button, output)\n",
    "\n",
    "def scene_distribution():\n",
    "    indices = np.arange(len(idx_to_scenegroup))\n",
    "    order = np.argsort(scene_counts)\n",
    "    plt.barh(indices, np.array(scene_counts)[order], tick_label=['\\n'.join(textwrap.wrap(idx_to_scenegroup[i], width=30)) for i in np.array(indices)[order]])\n",
    "    plt.yticks(rotation='vertical')\n",
    "    plt.yticks(fontsize=10)\n",
    "    ax = plt.gca()\n",
    "    ax.tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "    ax.tick_params(axis=\"y\", left=True, right=False, labelleft=True, labelright=False, labelrotation=0)\n",
    "    plt.ylabel('Scenes')\n",
    "    plt.xlabel('Quantity')\n",
    "    plt.title(\"Distribution of scenes that appear in dataset\")\n",
    "    plt.tight_layout()\n",
    "    if first_pass:\n",
    "        to_write[7] = [\"(obj_scn) Distribution of scenes that appear in dataset.\"]\n",
    "        plt.savefig(\"results/{0}/{1}/9.png\".format(folder_name, save_loc), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def scene_supercat(topn):\n",
    "    mi_wilson = np.zeros_like(scene_supercategory)\n",
    "    for i in range(len(mi_wilson)):\n",
    "        for j in range(len(mi_wilson[0])):\n",
    "            denom = scene_counts[i] + supercategory_counts[j] - scene_supercategory[i][j]\n",
    "            mi_wilson[i][j] = wilson(scene_supercategory[i][j] / denom, denom)[0]\n",
    "\n",
    "    flat_norm = mi_wilson.flatten()\n",
    "    flat_norm[flat_norm!=flat_norm] = -1.\n",
    "    normalized_indices = np.argsort(flat_norm)\n",
    "    print(\"Top cooccurrences:\")\n",
    "    for i in range(topn):\n",
    "        index = normalized_indices[-1-i]\n",
    "        a, b = index % len(dataset.supercategories_to_names), index // len(dataset.supercategories_to_names)\n",
    "        print(\"{0} - {1}\".format(idx_to_scenegroup[b], dataset.supercategories_to_names[a], round(flat_norm[index], 4)))\n",
    "\n",
    "    print(\"\\nBottom cooccurrences:\")\n",
    "    for i in range(topn):\n",
    "        index = normalized_indices[i]\n",
    "        a, b = index % len(dataset.supercategories_to_names), index // len(dataset.supercategories_to_names)\n",
    "        print(\"{0} - {1}\".format(idx_to_scenegroup[b], dataset.supercategories_to_names[a], round(flat_norm[index], 4)))\n",
    "\n",
    "def diversify_supercat_by_scene(supercat):\n",
    "    all_scenes = []\n",
    "    big = []\n",
    "    small = []\n",
    "    person = []\n",
    "    obj_area = []\n",
    "    person_area = []\n",
    "    distance = []\n",
    "    filepaths = []\n",
    "    scenes_to_features = supercat_to_scenes_to_features[supercat]\n",
    "    for scene in scenes_to_features.keys():\n",
    "        all_scenes.append((scene, len(scenes_to_features[scene])))\n",
    "\n",
    "        small_ = [chunk[0][0] for chunk in scenes_to_features[scene]]\n",
    "        filepaths_ = [chunk[1] for chunk in scenes_to_features[scene]]\n",
    "\n",
    "        small.extend(small_)\n",
    "        filepaths.extend(filepaths_)\n",
    "    all_features = small\n",
    "    cluster_center = np.mean(all_features, axis=0)\n",
    "    dists = np.linalg.norm(cluster_center-all_features, axis=1)\n",
    "    sorted_indices = np.argsort(dists)\n",
    "    tracker = 0\n",
    "    scene_dists = []\n",
    "    print(\"\\n{} scene contributions\\n\".format(dataset.supercategories_to_names[supercat]))\n",
    "    boundaries = [tracker]\n",
    "    tsne_features = []\n",
    "    ind_labels = []\n",
    "    for i, scene in enumerate(all_scenes):\n",
    "        med_dist = np.median(dists[tracker:tracker+scene[1]])\n",
    "        scene_dists.append(med_dist)\n",
    "        tracker += scene[1]\n",
    "        boundaries.append(tracker)\n",
    "\n",
    "        tsne_features.extend(all_features[tracker:tracker+min(100, scene[1])])\n",
    "        ind_labels.extend([i]*min(100, scene[1]))\n",
    "\n",
    "        labels = []\n",
    "    sizes = []\n",
    "    intensities = []\n",
    "    for index in np.argsort(scene_dists):\n",
    "        label = idx_to_scenegroup[all_scenes[index][0]]\n",
    "        size = all_scenes[index][1]\n",
    "        intensity = scene_dists[index]\n",
    "        labels.append(label)\n",
    "        sizes.append(size)\n",
    "        intensities.append(intensity)\n",
    "\n",
    "    plt.close()\n",
    "    intensities = np.array(intensities - min(intensities))\n",
    "    intensities /= np.amax(intensities)\n",
    "    colors = plt.cm.Blues(intensities)\n",
    "    fig, ax = plt.subplots()\n",
    "    def pie_label(pct, values):\n",
    "        amount = int(pct*np.sum(values))\n",
    "        if pct >= 2.0:\n",
    "            return \"{0}%\".format(round(pct, 1))\n",
    "        else:\n",
    "            return ''\n",
    "    wedges, texts, autotexts = ax.pie(sizes, labels = ['' if sizes[i]/sum(sizes) < .02 else i for i in range(len(sizes))], autopct=lambda pct: pie_label(pct, sizes), shadow=False, startangle=90, colors=colors, radius=100, labeldistance=1.1, explode=[10.] * len(sizes))\n",
    "    for t in texts:\n",
    "        t.set_fontsize('xx-small')\n",
    "    for t in autotexts:\n",
    "        t.set_fontsize('xx-small')\n",
    "    for w in wedges:\n",
    "        w.set_edgecolor('black')\n",
    "        w.set_linewidth(.1)\n",
    "    ax.legend(wedges, ['{0}: {1}'.format(chunk[0], chunk[1]) for chunk in zip(np.arange(len(labels)), labels)], title='Scene Group Categories', loc='center left', bbox_to_anchor=(1., 0, 0.5, 1.), fontsize='xx-small')\n",
    "    plt.tight_layout()\n",
    "    ax.axis('equal')\n",
    "    # Uncomment below to see pi chart version\n",
    "#     plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    colorz = sns.color_palette('hls', 16)\n",
    "    xs = np.array(sizes) / sum(sizes)\n",
    "    xs /= np.amax(xs)\n",
    "    ys = intensities\n",
    "    fig = plt.figure(figsize=(9, 5))\n",
    "    fontsize = 27\n",
    "    sizez = [64]*16\n",
    "    plt.scatter(xs, ys, c=colorz, s=sizez)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.xlabel('Relative Commonness', fontsize=fontsize)\n",
    "    plt.ylabel('Relative Diversity', fontsize=fontsize)\n",
    "    \n",
    "    plt.title(\"\\n{} scene contributions\\n\".format(dataset.supercategories_to_names[supercat]), fontsize=fontsize)\n",
    "    \n",
    "    \n",
    "    handles = []\n",
    "    for lab in range(len(sizes)):\n",
    "        plt.annotate(lab, (xs[lab]+.015, ys[lab]+.01))\n",
    "        patch = mpatches.Patch(color=colorz[lab], label='{0}: {1}'.format(lab, labels[lab]))\n",
    "        handles.append(patch)\n",
    "    fontP = FontProperties()\n",
    "    fontP.set_size('medium')\n",
    "    plt.legend(handles=handles, prop=fontP, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    if first_pass:\n",
    "        to_write[8] = [\"(obj_scn) An example of how to diversify the appearance diversity of the \\\"{}\\\" category by augmenting the dataset with images in different scenes. Appearance diversity can thought of as something like intra-class variation, which is an important feature for object detection. However, there is a tradeoff between the amount of appearance diversity an object in a particular scene brings, and how common this object-scene combination is, which contributes to how easy it is to collect this kind of image.\".format(dataset.supercategories_to_names[supercat])]\n",
    "        plt.savefig(\"results/{0}/{1}/10.png\".format(folder_name, save_loc))\n",
    "    plt.show()\n",
    "\n",
    "#     # Visualize the supercategory features colored by their scene as a tsne\n",
    "#     plot_kwds = {'alpha' : .8, 's' : 30, 'linewidths':0}\n",
    "#     projection_instances = TSNE().fit_transform(tsne_features)\n",
    "#     plt.scatter(*projection_instances.T, **plot_kwds, c=[colorz[ind_labels[i]] for i in range(len(tsne_features))])\n",
    "#     lgd = plt.legend(handles=handles, bbox_to_anchor=(1.04,1), loc=\"upper left\", prop=fontP)\n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"obj_scn_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scene distribution of entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the object (which has its scene diversity value next to it) to visualize qualitative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_qual = SceneQual()\n",
    "scene_qual.click('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cooccurrences between scenes and object categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.group_mapping is not None:\n",
    "    interact(scene_supercat, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actionable insights on how to diversify an object category's appearance. Visualization of tradeoff between how easy it is to find a scene vs how much appearance diversity it brings to the object category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.group_mapping is not None:\n",
    "    pairs = [(dataset.supercategories_to_names[index], index) for index in supercat_to_scenes_to_features.keys()]\n",
    "    pairs = sorted(pairs, key=lambda x: x[0])\n",
    "    interact(diversify_supercat_by_scene, supercat=widgets.Dropdown(options=pairs));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up summary pdf\n",
    "<a id=\"summarypdf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pass = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pdf(numbers):\n",
    "    for i in numbers:\n",
    "        if i in to_write.keys():\n",
    "            for sentence in to_write[i]:\n",
    "                pdf.write(5, sentence)\n",
    "                pdf.ln()\n",
    "            if i == 0:\n",
    "                pdf.image('results/{0}/{1}/1.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 1:\n",
    "                pdf.image('results/{0}/{1}/2.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 2:\n",
    "                pdf.image('results/{0}/{1}/3.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 3:\n",
    "                pdf.image('results/{0}/{1}/4.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 4:\n",
    "                y_spot = pdf.get_y()\n",
    "                pdf.image('results/{0}/{1}/5.png'.format(folder_name, save_loc), w=85)\n",
    "                after_y_spot = pdf.get_y()\n",
    "                if after_y_spot < y_spot:\n",
    "                    y_spot = 10\n",
    "                pdf.image('results/{0}/{1}/7.png'.format(folder_name, save_loc), w=85, x=95, y=y_spot)\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/6.png'.format(folder_name, save_loc),h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 5:\n",
    "                pdf.image('results/{0}/{1}/8.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 7:\n",
    "                pdf.image('results/{0}/{1}/9.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 8:\n",
    "                pdf.image('results/{0}/{1}/10.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            pdf.ln(h=3)\n",
    "            pdf.dashed_line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "            pdf.ln(h=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font('Arial', 'B', 16)\n",
    "pdf.write(5, \"Object-Based Summary\")\n",
    "pdf.ln()\n",
    "pdf.ln()\n",
    "\n",
    "# Overview Statistics\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Overview Statistics\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "write_pdf([2, 5, 7])\n",
    "\n",
    "# Interesting findings\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Sample Interesting Findings\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "write_pdf([0, 1, 3, 4, 6, 8])\n",
    "\n",
    "# Interesting findings\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Some of the other metrics in the notebook\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "pdf.write(5, \"- (obj_cnt) Cooccurrences of objects as a hierarchical graph\")\n",
    "pdf.ln()\n",
    "pdf.write(5, \"- (obj_cnt) Finer grained look at distribution within each object category \")\n",
    "pdf.ln()\n",
    "pdf.write(5, \"- (obj_siz) Size of each object category\")\n",
    "pdf.ln()\n",
    "pdf.write(5, \"- (obj_scn) Qualitative look at what each object's scenes are like\")\n",
    "pdf.ln()\n",
    "pdf.write(5, \"- (obj_scn) Highest/lowest cooccurrences between object categories and scenes\")\n",
    "pdf.ln()\n",
    "\n",
    "pdf.output('results/{0}/{1}/summary.pdf'.format(folder_name, save_loc), \"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
