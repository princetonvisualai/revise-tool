{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geography Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. Fill in the dataset in section 1.1\n",
    "\n",
    "2. Run all cells\n",
    "\n",
    "3. Review the summary pdf generated AND/OR explore each metric below.\n",
    "    - All metrics are identified by a short keyword, and consist of a \"Setup\" and \"Analyses\" portion. The \"Setup\" portion contains code that does not need to be modified unless customization is needed, and the \"Analyses\" portion provides an interactive display of the results.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Initial Setup](#setup) <br/>\n",
    "    1.1 [Dataset](#dataset)\n",
    "2. geo_ctr Metric: [Country Counts](#geo_ctr)<br/>\n",
    "    2.1 [Setup](#geo_ctr_setup)<br/>\n",
    "    2.2 [Analyses](#geo_ctr_analyses)\n",
    "3. geo_tag Metric: [Image Tags](#geo_tag)<br/>\n",
    "    3.1 [Setup](#geo_tag_setup)<br/>\n",
    "    3.2 [Analyses](#geo_tag_analyses)\n",
    "4. geo_lng Metric: [Languages for tourist vs local](#geo_lng) <br/>\n",
    "    4.1 [Setup](#geo_lng_setup)<br/>\n",
    "    4.2 [Analyses](#geo_lng_analyses)\n",
    "5. [Setting up summary pdf](#summarypdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup \n",
    "<a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "try: # only change dir if necessary\n",
    "    import datasets\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir(os.pardir)\n",
    "    import datasets\n",
    "import pickle\n",
    "import itertools\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import PIL.Image\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from math import sqrt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "from scipy.spatial.distance import squareform\n",
    "import pycountry\n",
    "from geonamescache import GeonamesCache\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "try: \n",
    "    from mpl_toolkits.basemap import Basemap\n",
    "except (FileNotFoundError, ModuleNotFoundError) as e: \n",
    "    print(e, '\\n', 'Please refer to \\'Potential Environment Issues\\' on the README page to resolve. ')\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "try: \n",
    "    from countryinfo import CountryInfo\n",
    "except (FileNotFoundError, ModuleNotFoundError) as e: \n",
    "    print(e, '\\n', 'Please refer to \\'Potential Environment Issues\\' on the README page to resolve. ')\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "import re\n",
    "import copy\n",
    "import textwrap\n",
    "import matplotlib.patches as mpatches\n",
    "import operator\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import imageio\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, HBox, Layout\n",
    "import ipywidgets as widgets\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display, HTML, Image\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import pandas as pd\n",
    "import folium \n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = sns.color_palette('Set2', 2)\n",
    "SAME_EXTENT = (-0.5, 6.5, -0.5, 6.5)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not os.path.exists(\"dataloader_files\"):\n",
    "    os.mkdir(\"dataloader_files\")\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook\n",
    "def hide_toggle(for_next=False, toggle_text='Toggle show/hide'):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide helper functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder(num, folder):\n",
    "    if not os.path.exists(\"results/{0}/{1}\".format(folder, num)):\n",
    "        os.mkdir(\"results/{0}/{1}\".format(folder, num))\n",
    "    file = open(\"results/{0}/{1}/results.txt\".format(folder, num), \"w\")\n",
    "    return file\n",
    "\n",
    "# Projecting a set of features into a lower-dimensional subspace with PCA\n",
    "def project(features, dim):\n",
    "    standardized = StandardScaler().fit_transform(features)\n",
    "    pca = PCA(n_components=dim)\n",
    "    principalComponents = pca.fit_transform(X=standardized)\n",
    "    return principalComponents\n",
    "\n",
    "# Calculating the binomial proportion confidence interval\n",
    "def wilson(p, n, z = 1.96):\n",
    "    denominator = 1 + z**2/n\n",
    "    centre_adjusted_probability = p + z*z / (2*n)\n",
    "    adjusted_standard_deviation = sqrt((p*(1 - p) + z*z / (4*n)) / n)\n",
    "    \n",
    "    lower_bound = (centre_adjusted_probability - z*adjusted_standard_deviation) / denominator\n",
    "    upper_bound = (centre_adjusted_probability + z*adjusted_standard_deviation) / denominator\n",
    "    return (lower_bound, upper_bound)\n",
    "\n",
    "def country_to_iso3(country):\n",
    "    missing = {'South+Korea': 'KOR',\n",
    "            'North+Korea': 'PRK',\n",
    "            'Laos': 'LAO',\n",
    "            'Caribbean+Netherlands': 'BES',\n",
    "            'St.+Lucia': 'LCA',\n",
    "            'East+Timor': 'TLS',\n",
    "            'Democratic+Republic+of+Congo': 'COD',\n",
    "            'Swaziland': 'SWZ',\n",
    "            'Cape+Verde': 'CPV',\n",
    "            'C%C3%B4te+d%C2%B4Ivoire': 'CIV',\n",
    "            'Ivory+Coast': 'CIV',\n",
    "            'Channel+Islands': 'GBR'\n",
    "            }\n",
    "    try:\n",
    "        iso3 = pycountry.countries.search_fuzzy(country.replace('+', ' '))[0].alpha_3\n",
    "    except LookupError:\n",
    "        try:\n",
    "            iso3 = missing[country]\n",
    "        except KeyError:\n",
    "            iso3 = None\n",
    "    return iso3\n",
    "\n",
    "def display_filepaths(filepaths, width=100, height=100):\n",
    "    sidebyside = widgets.HBox([widgets.Image(value=open(filepath, 'rb').read(), format='png', width=width, height=height) for filepath in filepaths], layout=Layout(height='{}px'.format(height)))\n",
    "    display(sidebyside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Fill in below with dataset and file path names\n",
    "<a id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([ \n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "# dataset = datasets.YfccPlacesDataset(transform_train)\n",
    "# folder_name = 'yfcc_supp'\n",
    "\n",
    "# dataset = datasets.CityScapesDataset(transform_train)\n",
    "# folder_name = 'cityscapes_test'\n",
    "\n",
    "dataset = datasets.BDD100KDataset(transform_train)\n",
    "folder_name = 'bdd100k_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = '1_pager_geo'\n",
    "os.system(\"rm -r results/{0}/{1}\".format(folder_name, save_loc))\n",
    "file = folder(save_loc, folder_name)\n",
    "first_pass = True\n",
    "to_write = {}\n",
    "if not os.path.exists(\"checkpoints/{}\".format(folder_name)):\n",
    "    os.mkdir(\"checkpoints/{}\".format(folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geo_ctr Metric: Region Counts\n",
    "<a id=\"geo_ctr\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = '1_pager_geo'\n",
    "os.system(\"rm -r results/{0}/{1}\".format(folder_name, save_loc))\n",
    "file = folder(save_loc, folder_name)\n",
    "first_pass = True\n",
    "to_write = {}\n",
    "if not os.path.exists(\"checkpoints/{}\".format(folder_name)):\n",
    "    os.mkdir(\"checkpoints/{}\".format(folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"geo_ctr_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide geo_ctr code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"COUNTRY_LABEL\"):\n",
    "    counts = pickle.load(open(\"results/{}/geo_ctr.pkl\".format(folder_name), \"rb\"))\n",
    "    iso3_to_subregion = pickle.load(open('util_files/iso3_to_subregion_mappings.pkl', 'rb'))\n",
    "    gc = GeonamesCache()\n",
    "    iso3_codes = list(gc.get_dataset_by_key(gc.get_countries(), 'iso3').keys())\n",
    "\n",
    "    # https://ramiro.org/notebook/basemap-choropleth/\n",
    "    cm = plt.get_cmap('Blues')\n",
    "    bins = np.logspace(min(list(counts.values())), np.log2(max(list(counts.values()))+1), base=2.0)\n",
    "    num_colors = len(bins)\n",
    "    scheme = [cm(i / num_colors) for i in range(num_colors)]\n",
    "\n",
    "    subregion_counts = {}\n",
    "    iso3_to_bin = {}\n",
    "    total = sum(counts.values())\n",
    "    country_count_phrases = []\n",
    "    iso3_to_scaledpop = {}\n",
    "\n",
    "    for country in ['England', 'Scotland', 'Wales', 'Northern+Ireland']:\n",
    "        if country in counts.keys():\n",
    "            counts['United+Kingdom'] += counts[country]\n",
    "        counts.pop(country, None)\n",
    "\n",
    "    for country, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        country_count_phrases.append(\"{0}: {1}   {2}%\".format(country, count, round(100.*count/total)))\n",
    "        iso3 = country_to_iso3(country)\n",
    "        if iso3 is not None:\n",
    "            iso3_to_bin[iso3] = np.digitize(count, bins)\n",
    "            try:\n",
    "                iso3_to_scaledpop[iso3] = count / CountryInfo(country.replace('+', ' ')).population()\n",
    "            except KeyError:\n",
    "                pass\n",
    "    #             print(\"{} not found in CountryInfo\".format(country))\n",
    "        try:\n",
    "            subregion = iso3_to_subregion[iso3]\n",
    "            if subregion in subregion_counts.keys():\n",
    "                subregion_counts[subregion] += count\n",
    "            else:\n",
    "                subregion_counts[subregion] = count\n",
    "        except KeyError:\n",
    "            print(\"This country's subregion not found: {}\".format(country))\n",
    "    for key in iso3_to_scaledpop.keys():\n",
    "        iso3_to_scaledpop[key] /= min(iso3_to_scaledpop.values())\n",
    "\n",
    "elif (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"REGION_LABEL\"):\n",
    "    ctr_dict = pickle.load(open(\"results/{}/geo_ctr.pkl\".format(folder_name), \"rb\"))\n",
    "    counts = ctr_dict[\"region_to_id\"]\n",
    "    total = sum(counts.values())\n",
    "    region_count_phrases = []\n",
    "    for region, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        region_count_phrases.append(\"{0}: {1}   {2}%\".format(region, count, round(100.*count/total)))\n",
    "        \n",
    "elif dataset.geography_info_type == \"GPS_LABEL\":\n",
    "    counts_gps = pickle.load(open(\"results/{}/geo_ctr.pkl\".format(folder_name), \"rb\"))\n",
    "    region_to_id = counts_gps[\"region_to_id\"]\n",
    "    id_to_gps = counts_gps[\"id_to_gps\"]\n",
    "    subregion_to_id = counts_gps.get(\"subregion_to_id\", None)\n",
    "    geo_boundaries = dataset.geo_boundaries\n",
    "    subregion_boundaries = dataset.subregion_boundaries\n",
    "    choro_data = pd.read_csv(dataset.choropleth_filepath)\n",
    "    \n",
    "    info_stats = pickle.load(open(\"results/{}/geo_tag.pkl\".format(folder_name), \"rb\")) \n",
    "    region_tags = info_stats['region_tags']\n",
    "\n",
    "    counts = {}\n",
    "    for region in region_to_id:\n",
    "        counts[region] = len(region_to_id[region])\n",
    "    total = sum(counts.values())\n",
    "    country_count_phrases = []\n",
    "\n",
    "    for country, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        country_count_phrases.append(\"{0}: {1}   {2}%\".format(country, count, round(100.*count/total)))\n",
    "    \n",
    "    subregion_counts = None\n",
    "    if subregion_to_id is not None:\n",
    "        subregion_counts = {}\n",
    "        for subregion in subregion_to_id:\n",
    "            subregion_counts[subregion] = len(subregion_to_id[subregion])\n",
    "\n",
    "    colors = [\n",
    "        'red',\n",
    "        'blue',\n",
    "        'gray',\n",
    "        'darkred',\n",
    "        'blue',\n",
    "        'green',\n",
    "        'purple',\n",
    "        'pink',\n",
    "        'green',\n",
    "        'purple',\n",
    "        'pink',\n",
    "    ]\n",
    "    \n",
    "    def abline(intercept, slope):\n",
    "        \"\"\"Plot a line from slope and intercept\"\"\"\n",
    "        axes = plt.gca()\n",
    "        x_vals = np.array(axes.get_xlim())\n",
    "        y_vals = intercept + slope * x_vals\n",
    "        plt.plot(x_vals, y_vals, '--')\n",
    "\n",
    "def region_counts_num(topn):\n",
    "\n",
    "    print(\"Total images: {}\\n\".format(total))\n",
    "    print(\"Region Counts\\n\")\n",
    "\n",
    "    print(\"Top:\\n\")\n",
    "    topn = min(topn, len(country_count_phrases))\n",
    "    for i in range(topn):\n",
    "        print(country_count_phrases[i])\n",
    "\n",
    "    print(\"\\nBottom:\\n\")\n",
    "    for i in range(topn):\n",
    "        print(country_count_phrases[-1-i])\n",
    "   \n",
    "def subregion_counts_num():\n",
    "    if (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"COUNTRY_LABEL\"):\n",
    "        print(\"Subregion Counts\\n\")\n",
    "        total_subregion = sum(subregion_counts.values())\n",
    "        for subregion, count in sorted(subregion_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(\"{0}: {1}    {2}%\".format(subregion, count, round(100.*count/total_subregion)))\n",
    "    elif dataset.geography_info_type == \"GPS_LABEL\" and subregion_to_id is not None:\n",
    "        print(\"Subregion Counts\\n\")\n",
    "        total_subregion = sum(subregion_counts.values())\n",
    "        for subregion, count in sorted(subregion_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(\"{0}: {1}    {2}%\".format(subregion, count, round(100.*count/total_subregion)))\n",
    "    else:\n",
    "        print(\"Subregion analysis not available\")\n",
    "            \n",
    "def region_map():\n",
    "    if (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"COUNTRY_LABEL\"):\n",
    "        fig = plt.figure(figsize=(16, 7))\n",
    "\n",
    "        fontsize = 20\n",
    "        ax = fig.add_subplot(111, facecolor='w', frame_on=False)\n",
    "        fig.suptitle('Dataset representation by number of images', fontsize=fontsize, y=.95)\n",
    "\n",
    "        m = Basemap(lon_0=0, projection='robin')\n",
    "        m.drawmapboundary(color='w')\n",
    "\n",
    "        shapefile = 'util_files/ne_10m_admin_0_countries_lakes'\n",
    "\n",
    "        m.readshapefile(shapefile, 'units', color='#444444', linewidth=.2)\n",
    "        for info, shape in zip(m.units_info, m.units):\n",
    "            iso3 = info['ADM0_A3']\n",
    "            if iso3 not in iso3_to_bin.keys():\n",
    "                color = '#dddddd'\n",
    "            else:\n",
    "                try:\n",
    "                    color = scheme[iso3_to_bin[iso3]]\n",
    "                except IndexError:\n",
    "                    print(iso3)\n",
    "                    print(\"this index: {0} when length is {1}\".format(iso3_to_bin[iso3], len(scheme)))\n",
    "\n",
    "\n",
    "            patches = [Polygon(np.array(shape), True)]\n",
    "            pc = PatchCollection(patches)\n",
    "            pc.set_facecolor(color)\n",
    "            ax.add_collection(pc)\n",
    "\n",
    "        # Cover up Antarctica so legend can be placed over it.\n",
    "        ax.axhspan(0, 1000 * 1800, facecolor='w', edgecolor='w', zorder=2)\n",
    "\n",
    "        # Draw color legend.\n",
    "        ax_legend = fig.add_axes([0.35, 0.14, 0.3, 0.03], zorder=3)\n",
    "        cmap = mpl.colors.ListedColormap(scheme)\n",
    "        cb = mpl.colorbar.ColorbarBase(ax_legend, cmap=cmap, ticks=bins, boundaries=bins, orientation='horizontal')\n",
    "        #cb = mpl.colorbar.ColorbarBase(ax_legend, cmap=cmap, ticks=bins, boundaries=bins, orientation='vertical')\n",
    "        spots = len(bins) // 4\n",
    "        spots = [0, spots, spots*2, spots*3, len(bins)- 1]\n",
    "        cb.ax.set_xticklabels([str(round(int(i), -3)) if j in spots else '' for j, i in enumerate(bins)])\n",
    "        cb.ax.tick_params(labelsize=fontsize)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Total countries: {}\".format(len(iso3_to_bin)))\n",
    "        \n",
    "    elif dataset.geography_info_type == \"GPS_LABEL\":\n",
    "        label_map = {}\n",
    "        if os.path.exists(\"results/{}/geo_tag.pkl\".format(folder_name)):\n",
    "            # load in geo_tag results for info popup of labels when click on folium map\n",
    "            info_stats = pickle.load(open(\"results/{}/geo_tag.pkl\".format(folder_name), \"rb\"))\n",
    "            label_map = info_stats.get('fileid_to_label_string', {})\n",
    "            \n",
    "        m = folium.Map()\n",
    "        folium.GeoJson(geo_boundaries, name=\"geojson\").add_to(m)\n",
    "        marker_cluster = MarkerCluster().add_to(m)\n",
    "        count = 0\n",
    "        for key in region_to_id:\n",
    "            filename_arr = region_to_id[key]\n",
    "            for i, fileid in enumerate(filename_arr):\n",
    "                lab_str = label_map.get(fileid, '')\n",
    "                test = folium.Html('<div style=\"font-size: 12pt;\">{}</div>'.format(lab_str), script=True)\n",
    "                popup = folium.Popup(test)\n",
    "                folium.Marker(\n",
    "                    location=[id_to_gps[fileid]['lat'], id_to_gps[fileid]['lng']], \n",
    "                    popup = popup,\n",
    "                    icon=folium.Icon(color=colors[count%len(colors)])\n",
    "                ).add_to(marker_cluster)\n",
    "            count += 1\n",
    "        return m\n",
    "    \n",
    "    elif (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"REGION_LABEL\"):\n",
    "        m = folium.Map()\n",
    "\n",
    "        region_names = []\n",
    "        region_counts = []\n",
    "        for region in counts:\n",
    "            region_names.append(region)\n",
    "            region_counts.append(counts[region])\n",
    "\n",
    "        counts_new = {\"region\": region_names, \"region_counts\": region_counts}\n",
    "        counts_df = pd.DataFrame.from_dict(counts_new)\n",
    "        \n",
    "        # Change \"Data\" to column name of csv you want\n",
    "        folium.Choropleth(\n",
    "            geo_data=geo_boundaries,\n",
    "            name=\"choropleth\",\n",
    "            data=counts_df,\n",
    "            columns=[\"region\", \"region_counts\"],\n",
    "            key_on=\"feature.properties.{0}\".format(dataset.geo_boundaries_key_name),\n",
    "            fill_color=\"BuPu\",\n",
    "            fill_opacity=0.7,\n",
    "            line_opacity=0.2,\n",
    "            legend_name=\"Image Counts over subregion\",\n",
    "        ).add_to(m)\n",
    "\n",
    "        folium.LayerControl().add_to(m)\n",
    "        return m\n",
    "    \n",
    "def choropleth(region_title = \"postalCode\", data_title = \"median_income\"):\n",
    "    if dataset.geography_info_type == \"GPS_LABEL\":\n",
    "        label_map = {}\n",
    "        if os.path.exists(\"results/{}/geo_tag.pkl\".format(folder_name)):\n",
    "            # load in geo_tag results for info popup of labels when click on folium map\n",
    "            info_stats = pickle.load(open(\"results/{}/geo_tag.pkl\".format(folder_name), \"rb\"))\n",
    "            label_map = info_stats.get('fileid_to_label_string', {})\n",
    "        \n",
    "        choro_data[region_title] = choro_data[region_title].astype(str)\n",
    "        m = folium.Map(tiles=\"cartodbpositron\")\n",
    "        marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "        # Change \"Data\" to column name of csv you want\n",
    "        folium.Choropleth(\n",
    "            geo_data=geo_boundaries,\n",
    "            name=\"choropleth\",\n",
    "            data=choro_data,\n",
    "            columns=[region_title, data_title],\n",
    "            key_on=\"feature.properties.{0}\".format(dataset.geo_boundaries_key_name),\n",
    "            fill_color=\"BuPu\",\n",
    "            fill_opacity=0.7,\n",
    "            line_opacity=0.2,\n",
    "            legend_name=data_title,\n",
    "        ).add_to(m)\n",
    "\n",
    "        folium.LayerControl().add_to(m)\n",
    "\n",
    "        count = 0\n",
    "        \n",
    "        for key in region_to_id:\n",
    "            filename_arr = region_to_id[key]\n",
    "            for i, fileid in enumerate(filename_arr):\n",
    "\n",
    "                folium.Marker(\n",
    "                    location=[id_to_gps[fileid]['lat'], id_to_gps[fileid]['lng']], \n",
    "                    popup=label_map.get(fileid, ''),\n",
    "                    icon=folium.Icon(color=colors[count%len(colors)])\n",
    "                ).add_to(marker_cluster)\n",
    "            count += 1\n",
    "        return m\n",
    "    else:\n",
    "        print(\"No choropleth available\")\n",
    "        \n",
    "def country_map_population():\n",
    "    if (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"COUNTRY_LABEL\"):\n",
    "        fig = plt.figure(figsize=(16, 7))\n",
    "\n",
    "        fontsize = 20\n",
    "        ax = fig.add_subplot(111, facecolor='w', frame_on=False)\n",
    "\n",
    "        m = Basemap(lon_0=0, projection='robin')\n",
    "        m.drawmapboundary(color='w')\n",
    "\n",
    "        shapefile = 'util_files/ne_10m_admin_0_countries_lakes'\n",
    "\n",
    "        cm = plt.get_cmap('Blues')\n",
    "        bins = np.logspace(min(list(iso3_to_scaledpop.values())), np.log2(max(list(iso3_to_scaledpop.values()))+1.), base=2.0)\n",
    "        num_colors = len(bins)\n",
    "        scheme = [cm(i / num_colors) for i in range(num_colors)]\n",
    "\n",
    "        m.readshapefile(shapefile, 'units', color='#444444', linewidth=.2)\n",
    "        for info, shape in zip(m.units_info, m.units):\n",
    "            iso3 = info['ADM0_A3']\n",
    "            if iso3 not in iso3_to_scaledpop.keys():\n",
    "                color = '#dddddd'\n",
    "            else:\n",
    "                try:\n",
    "                    color = scheme[np.digitize(iso3_to_scaledpop[iso3], bins)]\n",
    "                except IndexError:\n",
    "                    print(iso3)\n",
    "                    print(\"this index: {0} when length is {1}\".format(iso3_to_bin[iso3], len(scheme)))\n",
    "\n",
    "\n",
    "            patches = [Polygon(np.array(shape), True)]\n",
    "            pc = PatchCollection(patches)\n",
    "            pc.set_facecolor(color)\n",
    "            ax.add_collection(pc)\n",
    "\n",
    "        ax.axhspan(0, 1000 * 1800, facecolor='w', edgecolor='w', zorder=2)\n",
    "\n",
    "        to_write[0] = ['(geo_ctr) Geographic representation of dataset scaled by country population, colored on a logarithmic scale.']\n",
    "        plt.savefig(\"results/{0}/{1}/0.png\".format(folder_name, save_loc), bbox_inches='tight', pad_inches=.2)\n",
    "        fig.suptitle('Dataset representation scaled by country population, logarithmic scale', fontsize=fontsize, y=.95)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No population information available, use choropleth instead\")\n",
    "        \n",
    "        \n",
    "        \n",
    "def regress_counts_quantized(region_colname = 'postalCode', normalize_colname = 'population', data_colname = 'median_income', log_income = False):\n",
    "######################################\n",
    "    def quantize(data, pt, bins):\n",
    "        boundaries = np.linspace(np.amin(data), np.amax(data), bins+1)\n",
    "        for i in range(1, len(boundaries)):\n",
    "            if pt <= boundaries[i]:\n",
    "                return i\n",
    "        return bins\n",
    "    \n",
    "    # Normalized by population\n",
    "    # region_to_pop maps region_name (type string) to population_count (type int)\n",
    "#     choro_data = pd.read_csv('/Users/home/Desktop/research/data/zip_full_df.csv') todo: remove when sure this is useless\n",
    "    choro_data[region_colname] = choro_data[region_colname].astype(str)\n",
    "    region_to_normalized_var = choro_data[[region_colname, normalize_colname]].set_index(region_colname).T.to_dict('records')[0]\n",
    "    region_to_income = choro_data[[region_colname, data_colname]].set_index(region_colname).T.to_dict('records')[0]\n",
    "\n",
    "    area_normalized_counts = {}\n",
    "    for region in region_to_id:\n",
    "        if region in region_to_normalized_var:\n",
    "            area_normalized_counts[region] = len(region_to_id[region])/(region_to_normalized_var[region])\n",
    "\n",
    "    income = []\n",
    "    image_count = []\n",
    "\n",
    "    for region in area_normalized_counts:\n",
    "        if region in region_to_income:\n",
    "            income.append(region_to_income[region]/1000)\n",
    "            image_count.append(area_normalized_counts[region])\n",
    "\n",
    "    num_bins = 10\n",
    "    # create df for quantization work, using pd.qcut\n",
    "    quantized_image_counts = pd.DataFrame({'income': income, 'count': image_count})\n",
    "    labels = np.arange(1, num_bins+1)*100/num_bins\n",
    "    quantized_image_counts['quantiles_10'] = pd.qcut(quantized_image_counts['income'], q=num_bins, precision=0, labels = labels)\n",
    "\n",
    "    # average image counts over quantized income bins:\n",
    "    image_count = [] # overwrrite image_count (already stored in pandas df)\n",
    "    for i in labels:\n",
    "        image_count.append(np.mean(quantized_image_counts[quantized_image_counts['quantiles_10'] == i]['count']))\n",
    "\n",
    "    model = LinearRegression().fit(np.asarray(labels).reshape((-1, 1)), image_count)\n",
    "    r_sq = model.score(np.asarray(labels).reshape((-1, 1)), image_count)\n",
    "    print(\"R squared: {0}\".format(r_sq))\n",
    "\n",
    "    labels = [str(int(i)) for i in labels]\n",
    "    plt.bar(x = labels, height=image_count)\n",
    "    # abline(model.intercept_, model.coef_)\n",
    "    plt.xlabel(\"Income decile of ZIP code\", fontsize = 15)\n",
    "    plt.ylabel(\"Mean images {0} normalized for\\n ZIP codes in this decile\".format(normalize_colname), fontsize = 15)\n",
    "    tksize = 19\n",
    "    plt.xticks(fontsize= tksize) \n",
    "    plt.yticks(fontsize= tksize)\n",
    "#     plt.savefig('/Users/home/Desktop/img_regressions/img_density_per_cap', dpi=300, bbox_inches='tight')\n",
    "# todo: add save fig in the bottom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"geo_ctr_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(region_counts_num, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts by subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subregion_counts_num()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of representation by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "region_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of representation by country, scaled by population. Logarithmic scale. Some countries may be grayed out because the population could not be found for that country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_map_population()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choropleth visualization with custom data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "choropleth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantized relationship between (income) and image counts normalized by (population/area size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can change normalize colname to population or area to toggle the normalization factor\n",
    "(referring to bdd100k dataset)\n",
    "'''\n",
    "regress_counts_quantized(normalize_colname = 'area')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geo_tag Metric: Image Tags\n",
    "<a id=\"geo_tag\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "<a id=\"geo_tag_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide geo_tag code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"COUNTRY_LABEL\"):\n",
    "    if not os.path.exists(\"results/{0}/geo_tag\".format(folder_name)):\n",
    "        os.mkdir(\"results/{0}/geo_tag\".format(folder_name))\n",
    "    info_stats = pickle.load(open(\"results/{}/geo_tag.pkl\".format(folder_name), \"rb\")) #20GB\n",
    "    country_tags = info_stats['country_tags']\n",
    "    tag_to_subregion_features = info_stats['tag_to_subregion_features']\n",
    "    iso3_to_subregion = pickle.load(open('util_files/iso3_to_subregion_mappings.pkl', 'rb'))\n",
    "    categories = dataset.categories\n",
    "    total_counts = np.zeros(len(categories))\n",
    "    subregion_tags = {}\n",
    "    for country, counts in country_tags.items():\n",
    "        total_counts = np.add(total_counts, counts)\n",
    "        subregion = iso3_to_subregion[country_to_iso3(country)]\n",
    "        if subregion not in subregion_tags.keys():\n",
    "            subregion_tags[subregion] = np.zeros(len(categories))\n",
    "        subregion_tags[subregion] = np.add(subregion_tags[subregion], counts)\n",
    "    total_counts = total_counts.astype(int)\n",
    "    sum_total_counts = int(np.sum(total_counts))\n",
    "    if not os.path.exists('checkpoints/{}/geo_tag_a.pkl'.format(folder_name)):\n",
    "        pvalues_over = {} # pvalue : '[country]: [tag] (country num and total num info for now)'\n",
    "        pvalues_under = {} \n",
    "        for country, counts in country_tags.items():\n",
    "            tags_for_country = int(np.sum(counts))\n",
    "            if tags_for_country < 50: # threshold for country to have at least 50 tags so there are enough samples for analysis\n",
    "                continue\n",
    "            for i, count in enumerate(counts):\n",
    "                this_counts = np.zeros(tags_for_country)\n",
    "                this_counts[:int(count)] = 1\n",
    "                that_counts = np.zeros(sum_total_counts - tags_for_country)\n",
    "                that_counts[:total_counts[i] - int(count)] = 1\n",
    "                p = stats.ttest_ind(this_counts, that_counts)[1]\n",
    "                tag_info = '{0}-{1} ({2}/{3} vs {4}/{5})'.format(country, categories[i], int(count), tags_for_country, int(total_counts[i] - count), sum_total_counts - tags_for_country)\n",
    "                if np.mean(this_counts) > np.mean(that_counts):\n",
    "                    pvalues_over[p] = tag_info\n",
    "                else:\n",
    "                    pvalues_under[p] = tag_info\n",
    "    else:\n",
    "        pvalues_under, pvalues_over = pickle.load(open('checkpoints/{}/geo_tag_a.pkl'.format(folder_name), 'rb'))\n",
    "    subregion_pvalues_over = {}\n",
    "    subregion_pvalues_under = {}\n",
    "    for subregion, counts in subregion_tags.items():\n",
    "        tags_for_subregion = int(np.sum(counts))\n",
    "        for i, count in enumerate(counts):\n",
    "            this_counts = np.zeros(tags_for_subregion)\n",
    "            this_counts[:int(count)] = 1\n",
    "            that_counts = np.zeros(sum_total_counts - tags_for_subregion)\n",
    "            that_counts[:total_counts[i] - int(count)] = 1\n",
    "            p = stats.ttest_ind(this_counts, that_counts)[1]\n",
    "            tag_info = '{0}-{1} ({2}/{3} vs {4}/{5})'.format(subregion, categories[i], int(count), tags_for_subregion, int(total_counts[i] - count), sum_total_counts - tags_for_subregion)\n",
    "            if np.mean(this_counts) > np.mean(that_counts):\n",
    "                subregion_pvalues_over[p] = tag_info\n",
    "            else:\n",
    "                subregion_pvalues_under[p] = tag_info\n",
    "                \n",
    "elif (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"REGION_LABEL\"):\n",
    "    if not os.path.exists(\"results/{0}/geo_tag\".format(folder_name)):\n",
    "        os.mkdir(\"results/{0}/geo_tag\".format(folder_name))\n",
    "    info_stats = pickle.load(open(\"results/{}/geo_tag.pkl\".format(folder_name), \"rb\")) #20GB\n",
    "    region_tags = info_stats['region_tags']\n",
    "    tag_to_region_features = info_stats['tag_to_region_features']\n",
    "\n",
    "    categories = dataset.categories\n",
    "    total_counts = np.zeros(len(categories))\n",
    "\n",
    "    for region, counts in region_tags.items():\n",
    "        total_counts = np.add(total_counts, counts)\n",
    "\n",
    "    total_counts = total_counts.astype(int)\n",
    "    sum_total_counts = int(np.sum(total_counts))\n",
    "\n",
    "    if not os.path.exists('checkpoints/{}/geo_tag_a.pkl'.format(folder_name)):\n",
    "        pvalues_over = {} # pvalue : '[region]: [tag] (region num and total num info for now)'\n",
    "        pvalues_under = {} \n",
    "        for region, counts in region_tags.items():\n",
    "            tags_for_region = int(np.sum(counts))\n",
    "            if tags_for_region < 50: # threshold for region to have at least 50 tags so there are enough samples for analysis\n",
    "                continue\n",
    "            for i, count in enumerate(counts):\n",
    "                this_counts = np.zeros(tags_for_region)\n",
    "                this_counts[:int(count)] = 1\n",
    "                that_counts = np.zeros(sum_total_counts - tags_for_region)\n",
    "                that_counts[:total_counts[i] - int(count)] = 1\n",
    "                p = stats.ttest_ind(this_counts, that_counts)[1]\n",
    "                tag_info = '{0}-{1} ({2}/{3} vs {4}/{5})'.format(region, categories[i], int(count), tags_for_region, int(total_counts[i] - count), sum_total_counts - tags_for_region)\n",
    "                if np.mean(this_counts) > np.mean(that_counts):\n",
    "                    pvalues_over[p] = tag_info\n",
    "                else:\n",
    "                    pvalues_under[p] = tag_info\n",
    "    else:\n",
    "        pvalues_under, pvalues_over = pickle.load(open('checkpoints/{}/geo_tag_a.pkl'.format(folder_name), 'rb'))\n",
    "\n",
    "elif dataset.geography_info_type == \"GPS_LABEL\":\n",
    "    if not os.path.exists(\"results/{0}/geo_tag\".format(folder_name)):\n",
    "        os.mkdir(\"results/{0}/geo_tag\".format(folder_name))\n",
    "    info_stats = pickle.load(open(\"results/{}/geo_tag.pkl\".format(folder_name), \"rb\"))\n",
    "    region_tags = info_stats['region_tags']\n",
    "    subregion_tags = info_stats.get('subregion_tags', None)\n",
    "    tag_to_region_features = info_stats['tag_to_region_features']\n",
    "    choro_data = pd.read_csv(dataset.choropleth_filepath)\n",
    "\n",
    "    categories = dataset.categories\n",
    "    total_counts = np.zeros(len(categories))\n",
    "\n",
    "    for region, counts in region_tags.items():\n",
    "        total_counts = np.add(total_counts, counts)\n",
    "\n",
    "    total_counts = total_counts.astype(int)\n",
    "    sum_total_counts = int(np.sum(total_counts))\n",
    "\n",
    "    if not os.path.exists('checkpoints/{}/geo_tag_a.pkl'.format(folder_name)):\n",
    "        pvalues_over = {} # pvalue : '[region]: [tag] (region num and total num info for now)'\n",
    "        pvalues_under = {} \n",
    "        for region, counts in region_tags.items():\n",
    "            tags_for_region = int(np.sum(counts))\n",
    "            if tags_for_region < 50: # threshold for region to have at least 50 tags so there are enough samples for analysis\n",
    "                continue\n",
    "            for i, count in enumerate(counts):\n",
    "                this_counts = np.zeros(tags_for_region)\n",
    "                this_counts[:int(count)] = 1\n",
    "                that_counts = np.zeros(sum_total_counts - tags_for_region)\n",
    "                that_counts[:total_counts[i] - int(count)] = 1\n",
    "                p = stats.ttest_ind(this_counts, that_counts)[1]\n",
    "                tag_info = '{0}-{1} ({2}/{3} vs {4}/{5})'.format(region, categories[i], int(count), tags_for_region, int(total_counts[i] - count), sum_total_counts - tags_for_region)\n",
    "                if np.mean(this_counts) > np.mean(that_counts):\n",
    "                    pvalues_over[p] = tag_info\n",
    "                else:\n",
    "                    pvalues_under[p] = tag_info\n",
    "    else:\n",
    "        pvalues_under, pvalues_over = pickle.load(open('checkpoints/{}/geo_tag_a.pkl'.format(folder_name), 'rb'))\n",
    "\n",
    "    if subregion_tags is not None:\n",
    "        subregion_total_counts = np.zeros(len(categories))\n",
    "        for region, counts in subregion_tags.items():\n",
    "            subregion_total_counts = np.add(subregion_total_counts, counts)\n",
    "\n",
    "        subregion_total_counts = subregion_total_counts.astype(int)\n",
    "        sum_subregion_total_counts = int(np.sum(subregion_total_counts))\n",
    "\n",
    "        subregion_pvalues_over = {} \n",
    "        subregion_pvalues_under = {} \n",
    "        for region, counts in subregion_tags.items():\n",
    "            tags_for_region = int(np.sum(counts))\n",
    "            if tags_for_region < 50: # threshold for subregion to have at least 50 tags so there are enough samples for analysis\n",
    "                continue\n",
    "            for i, count in enumerate(counts):\n",
    "                this_counts = np.zeros(tags_for_region)\n",
    "                this_counts[:int(count)] = 1\n",
    "                that_counts = np.zeros(sum_subregion_total_counts - tags_for_region)\n",
    "                that_counts[:subregion_total_counts[i] - int(count)] = 1\n",
    "                p = stats.ttest_ind(this_counts, that_counts)[1]\n",
    "                tag_info = '{0}-{1} ({2}/{3} vs {4}/{5})'.format(region, categories[i], int(count), tags_for_region, int(subregion_total_counts[i] - count), sum_subregion_total_counts - tags_for_region)\n",
    "                if np.mean(this_counts) > np.mean(that_counts):\n",
    "                    subregion_pvalues_over[p] = tag_info\n",
    "                else:\n",
    "                    subregion_pvalues_under[p] = tag_info\n",
    "                    \n",
    "def abline(intercept, slope):\n",
    "        \"\"\"Plot a line from slope and intercept\"\"\"\n",
    "        axes = plt.gca()\n",
    "        x_vals = np.array(axes.get_xlim())\n",
    "        y_vals = intercept + slope * x_vals\n",
    "        plt.plot(x_vals, y_vals, '--')\n",
    "        \n",
    "        \n",
    "def label_regression(cat_ind = 7, region_colname = \"postalCode\", data_colname = \"median_income\", is_log_scale = False):\n",
    "    choro_data[region_colname] = choro_data[region_colname].astype(str)\n",
    "    region_to_income = choro_data[[region_colname, data_colname]].set_index(region_colname).T.to_dict('records')[0]\n",
    "    print(cat_ind)\n",
    "    image_count = {}\n",
    "    for region in region_tags:\n",
    "        if region != 'out_of_boundary' and region in region_to_income and len(region_to_id[region]) > 250:\n",
    "            \n",
    "            # rhs is # of images from region with at least 1 label of interest / # of images in region\n",
    "            image_count[region] = region_tags[region][cat_ind] / len(region_to_id[region])\n",
    "\n",
    "    income = []\n",
    "    label_count = []\n",
    "    for region in image_count:\n",
    "        income.append(region_to_income[region])\n",
    "        label_count.append(image_count[region])\n",
    "    xlabstring = data_colname\n",
    "    if is_log_scale:\n",
    "        income = np.log(income)\n",
    "        xlabstring = \"Log Scale {0}\".format(data_colname)\n",
    "    model = LinearRegression().fit(np.asarray(income).reshape((-1, 1)), label_count)\n",
    "    r_sq = model.score(np.asarray(income).reshape((-1, 1)), label_count)\n",
    "\n",
    "    print(dataset.categories[cat_ind])\n",
    "    print(\"R-Squared: {0}\".format(r_sq))\n",
    "    # print(\"Slope: {0}\".format(model.coef_))\n",
    "\n",
    "    plt.scatter(income, label_count)\n",
    "    abline(model.intercept_, model.coef_)\n",
    "    plt.xlabel(xlabstring)\n",
    "    _=plt.ylabel(\"Percentage of images with '{}'\".format(dataset.categories[cat_ind]))\n",
    "\n",
    "\n",
    "def tag_rep_by_region(topn):\n",
    "    if (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"COUNTRY_LABEL\"):\n",
    "        if first_pass:\n",
    "            to_write[1] = [\"(geo_tag) Overrepresentations of tags by country (tag in country vs tag in rest of the countries):\"]\n",
    "            for p, content in sorted(pvalues_over.items(), key=lambda x: x[0])[:4]:\n",
    "                to_write[1].append(('{0}: {1}'.format(round(p, 4), content)))\n",
    "            to_write[1].append(\"\")\n",
    "            to_write[1].append(\"Underrepresentations of tags by country (tag in country vs tag in rest of the countries):\")\n",
    "            for p, content in sorted(pvalues_under.items(), key=lambda x: x[0])[:4]:\n",
    "                to_write[1].append(('{0}: {1}'.format(round(p, 4), content)))\n",
    "            \n",
    "        print(\"By Country\\n\")\n",
    "        print('Over represented\\n')\n",
    "        for p, content in sorted(pvalues_over.items(), key=lambda x: x[0])[:topn]:\n",
    "            print('{0}: {1}'.format(round(p, 4), content))\n",
    "        print('\\nUnder represented\\n')\n",
    "        for p, content in sorted(pvalues_under.items(), key=lambda x: x[0])[:topn]:\n",
    "            print('{0}: {1}'.format(round(p, 4), content))\n",
    "    else:\n",
    "        if first_pass:\n",
    "            to_write[1] = [\"(geo_tag) Overrepresentations of tags by region (tag in region vs tag in rest of the countries):\"]\n",
    "            for p, content in sorted(pvalues_over.items(), key=lambda x: x[0])[:4]:\n",
    "                to_write[1].append(('{0}: {1}'.format(round(p, 4), content)))\n",
    "            to_write[1].append(\"\")\n",
    "            to_write[1].append(\"Underrepresentations of tags by region (tag in region vs tag in rest of the countries):\")\n",
    "            for p, content in sorted(pvalues_under.items(), key=lambda x: x[0])[:4]:\n",
    "                to_write[1].append(('{0}: {1}'.format(round(p, 4), content)))\n",
    "        \n",
    "        print(\"By region\\n\")\n",
    "        print('Over represented\\n')\n",
    "        for p, content in sorted(pvalues_over.items(), key=lambda x: x[0])[:topn]:\n",
    "            print('{0}: {1}'.format(round(p, 4), content))\n",
    "        print('\\nUnder represented\\n')\n",
    "        for p, content in sorted(pvalues_under.items(), key=lambda x: x[0])[:topn]:\n",
    "            print('{0}: {1}'.format(round(p, 4), content))\n",
    "\n",
    "\n",
    "def tag_rep_by_subregion(topn):\n",
    "    if (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"COUNTRY_LABEL\"):\n",
    "        print(\"By Subregion\\n\")\n",
    "        print('Over represented\\n')\n",
    "        for p, content in sorted(subregion_pvalues_over.items(), key=lambda x: x[0])[:topn]:\n",
    "           print('{0}: {1}'.format(round(p, 4), content))\n",
    "        print('\\nUnder represented\\n')\n",
    "        for p, content in sorted(subregion_pvalues_under.items(), key=lambda x: x[0])[:topn]:\n",
    "           print('{0}: {1}'.format(round(p, 4), content))\n",
    "            \n",
    "    elif dataset.geography_info_type == \"GPS_LABEL\" and subregion_tags is not None:\n",
    "        print(\"By Subregion\\n\")\n",
    "        print('Over represented\\n')\n",
    "        for p, content in sorted(subregion_pvalues_over.items(), key=lambda x: x[0])[:topn]:\n",
    "           print('{0}: {1}'.format(round(p, 4), content))\n",
    "        print('\\nUnder represented\\n')\n",
    "        for p, content in sorted(subregion_pvalues_under.items(), key=lambda x: x[0])[:topn]:\n",
    "           print('{0}: {1}'.format(round(p, 4), content))\n",
    "    else:\n",
    "        print(\"No subregion data for gps-formatted datasets\")\n",
    "\n",
    "if (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"COUNTRY_LABEL\"):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    if not os.path.exists('checkpoints/{}/geo_tag_b.pkl'.format(folder_name)):\n",
    "        phrase_to_value = {}\n",
    "        # Look at appearance differences in how a tag is represented across subregions\n",
    "        for tag in tag_to_subregion_features.keys():\n",
    "            subregion_features = tag_to_subregion_features[tag]\n",
    "            all_subregions = list(subregion_features.keys())\n",
    "            all_features = []\n",
    "            all_filepaths = []\n",
    "            start = 0\n",
    "            for subregion in all_subregions:\n",
    "                this_features = [features[0] for features in subregion_features[subregion]]\n",
    "                this_filepaths = [features[1] for features in subregion_features[subregion]]\n",
    "                if len(this_features) > 0:\n",
    "                    all_features.append(np.array(this_features)[:, 0, :])\n",
    "                    all_filepaths.append(this_filepaths)\n",
    "            if len(all_features) == 0:\n",
    "                continue\n",
    "            all_features = np.concatenate(all_features, axis=0)\n",
    "            all_filepaths = np.concatenate(all_filepaths, axis=0)\n",
    "            labels = np.zeros(len(all_features))\n",
    "            for j, subregion in enumerate(all_subregions):\n",
    "                labels[start:len(subregion_features[subregion])+start] = j\n",
    "                start += len(subregion_features[subregion])\n",
    "            num_features = int(np.sqrt(len(all_features)))\n",
    "            all_features = StandardScaler().fit_transform(project(all_features, num_features))\n",
    "\n",
    "            clf = svm.SVC(kernel='linear', probability=True, decision_function_shape='ovr', class_weight='balanced', max_iter=5000)\n",
    "\n",
    "            if len(np.unique(labels)) <= 1:\n",
    "                continue\n",
    "            clf.fit(all_features, labels)\n",
    "            acc = clf.score(all_features, labels)\n",
    "            probs = clf.decision_function(all_features)\n",
    "\n",
    "            labels = labels.astype(np.integer)\n",
    "            plot_kwds = {'alpha' : .8, 's' : 30, 'linewidths':0}\n",
    "            colorz = sns.color_palette('hls', int(np.amax(labels)) + 1)\n",
    "            projection_instances = TSNE().fit_transform(all_features)\n",
    "            plt.scatter(*projection_instances.T, **plot_kwds, c=[colorz[labels[i]] for i in range(len(all_features))])\n",
    "            handles = []\n",
    "            for lab in np.unique(labels):\n",
    "                patch = mpatches.Patch(color=colorz[lab], label=all_subregions[lab])\n",
    "                handles.append(patch)\n",
    "            fontP = FontProperties()\n",
    "            fontP.set_size('small')\n",
    "            lgd = plt.legend(handles=handles, bbox_to_anchor=(1.04,1), loc=\"upper left\", prop=fontP)\n",
    "            plt.savefig('results/{0}/{1}/{2}_tsne.png'.format(folder_name, \"geo_tag\", dataset.categories[tag]), bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            class_preds = clf.predict(all_features)\n",
    "            class_probs = clf.predict_proba(all_features)\n",
    "            j_to_acc = {}\n",
    "            for j, subregion in enumerate(all_subregions):\n",
    "                if j in labels:\n",
    "                    # to get acc in subregion vs out\n",
    "                    this_labels = np.copy(labels)\n",
    "                    this_labels[np.where(labels!=j)[0]] = -1\n",
    "                    this_preds = np.copy(class_preds)\n",
    "                    this_preds[np.where(class_preds!=j)[0]] = -1\n",
    "                    this_acc = np.mean(this_preds == this_labels)\n",
    "                    j_to_acc[j] = this_acc\n",
    "\n",
    "            fig = plt.figure(figsize=(16, 12))\n",
    "            plt.subplots_adjust(hspace=.48)\n",
    "            fontsize = 24\n",
    "            diff_subregion = max(j_to_acc.items(), key=operator.itemgetter(1))[0]\n",
    "            subregion_index = list(clf.classes_).index(diff_subregion)\n",
    "            class_probs = class_probs[:, subregion_index]\n",
    "            in_sub = np.where(labels == diff_subregion)[0]\n",
    "            out_sub = np.where(labels != diff_subregion)[0]\n",
    "            in_probs = class_probs[in_sub]\n",
    "            out_probs = class_probs[out_sub]\n",
    "            in_indices = np.argsort(in_probs)\n",
    "            out_indices = np.argsort(out_probs)\n",
    "            \n",
    "            original_labels = np.copy(labels)\n",
    "\n",
    "            def subregion_scoring(estimator, X_test, y_test):\n",
    "                y_pred = estimator.predict(X_test)\n",
    "                y_test[np.where(y_test!=diff_subregion)[0]] = -1\n",
    "                y_pred[np.where(y_pred!=diff_subregion)[0]] = -1\n",
    "                acc_random = np.mean(y_test == y_pred)\n",
    "                return acc_random\n",
    "\n",
    "            base_acc, rand_acc, p_value = permutation_test_score(clf, all_features, labels, scoring=subregion_scoring, n_permutations=100)\n",
    "            value = base_acc/np.mean(rand_acc)\n",
    "            if p_value > .05 and value < 1.2: # can tune as desired\n",
    "                continue\n",
    "\n",
    "            phrase = dataset.labels_to_names[dataset.categories[tag]]\n",
    "            phrase_to_value[phrase] = [value, all_subregions[diff_subregion], acc, p_value, num_features, j_to_acc]\n",
    "            \n",
    "            pickle.dump([original_labels, class_probs, class_preds, diff_subregion, all_filepaths], open('results/{0}/{1}/{2}_info.pkl'.format(folder_name, \"geo_tag\", dataset.labels_to_names[dataset.categories[tag]]), 'wb'))\n",
    "        pickle.dump(phrase_to_value, open('checkpoints/{}/geo_tag_b.pkl'.format(folder_name), 'wb'))\n",
    "    else:\n",
    "        phrase_to_value = pickle.load(open('checkpoints/{}/geo_tag_b.pkl'.format(folder_name), 'rb'))\n",
    "\n",
    "    svm_options = []\n",
    "    best_tag = None\n",
    "    best_tag_value = 1.2\n",
    "    for phrase, value in sorted(phrase_to_value.items(), key=lambda x: x[1][0], reverse=True):\n",
    "        value, region, acc, p_value, num_features, j_to_acc = phrase_to_value[phrase]\n",
    "        if acc > .75 and value > best_tag_value:\n",
    "            best_tag_value = value\n",
    "            best_tag = phrase\n",
    "        svm_options.append(('{0} in {1}: {2}% and {3}x'.format(phrase, region, round(100.*acc, 3), round(value, 3)), phrase))\n",
    "\n",
    "def show_svm_tag(tag, num):\n",
    "    if not (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"COUNTRY_LABEL\"):\n",
    "        print(\"SVM metric not setup for gps datasets\")\n",
    "    else:\n",
    "        if tag is None:\n",
    "            return\n",
    "        this_info = pickle.load(open('results/{0}/{1}/{2}_info.pkl'.format(folder_name, \"geo_tag\", tag), 'rb'))\n",
    "        labels, class_probs, class_preds, diff_subregion, all_filepaths = this_info\n",
    "        value, region, acc, p_value, num_features, j_to_acc = phrase_to_value[tag]\n",
    "        if num is not None:\n",
    "            print(\"{0} in {1} has acc: {2}, with p={3}, {4}x and {5} features\".format(tag, region, round(acc, 3), round(p_value, 3), round(value, 3), num_features))\n",
    "            print()\n",
    "        \n",
    "        in_sub = np.where(labels == diff_subregion)[0]\n",
    "        out_sub = np.where(labels != diff_subregion)[0]\n",
    "        in_probs = class_probs[in_sub]\n",
    "        out_probs = class_probs[out_sub]\n",
    "        in_indices = np.argsort(in_probs)\n",
    "        out_indices = np.argsort(out_probs)\n",
    "        \n",
    "        to_save = False\n",
    "        if num is None:\n",
    "            to_write[2] = ['(geo_tag) To discern if there is an appearance difference in how certain subregions represent a tag, we extract scene-level features from each image, and fit a linear SVM to distinguish between the tag in the subregion and out of the subregion.\\nAn example of the most linearly separable tag: {0} has an accuracy of {1} when classifying in {2} vs outside {2}.\\n'.format(tag, round(acc, 3), region)]\n",
    "            to_save = True\n",
    "            num = 5\n",
    "                    \n",
    "        def display_chunk(in_subregion_label=True, in_subregion_pred=True, to_save=False, name=None):\n",
    "            subregion_filepaths = []\n",
    "            if in_subregion_label == in_subregion_pred:\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter = -1\n",
    "            while len(subregion_filepaths) < num:\n",
    "                try:\n",
    "                    if in_subregion_label:\n",
    "                        index_a = in_sub[in_indices[counter]]\n",
    "                    else:\n",
    "                        index_a = out_sub[out_indices[counter]]\n",
    "                except:\n",
    "                    break\n",
    "                file_path_a = all_filepaths[index_a]\n",
    "                if (in_subregion_pred and class_preds[index_a] == diff_subregion) or ((not in_subregion_pred) and class_preds[index_a] != diff_subregion):\n",
    "                    subregion_filepaths.append(file_path_a)\n",
    "                if in_subregion_label == in_subregion_pred:\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    counter += -1\n",
    "            if to_save and first_pass:\n",
    "                this_loc = \"results/{0}/{1}/1_{2}.png\".format(folder_name, save_loc, name)\n",
    "                if len(subregion_filepaths) > 0:\n",
    "                    fig = plt.figure(figsize=(16, 8))\n",
    "                    for i in range(num):\n",
    "                        ax = fig.add_subplot(1, num, i+1)\n",
    "                        ax.axis(\"off\")\n",
    "                        if i >= len(subregion_filepaths):\n",
    "                            image = np.ones((3, 3, 3))\n",
    "                        else:\n",
    "                            image, _ = dataset.from_path(subregion_filepaths[i])\n",
    "                            image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "                        im = ax.imshow(image, extent=SAME_EXTENT)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(this_loc, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    os.system(\"cp util_files/no_images.png {0}\".format(this_loc))\n",
    "            elif len(subregion_filepaths) > 0:\n",
    "                display_filepaths(subregion_filepaths, width = 800//len(subregion_filepaths), height=800//len(subregion_filepaths))\n",
    "            else:\n",
    "                print(\"No images in this category\")\n",
    "        if not to_save:\n",
    "            print(\"In: Correct\")\n",
    "        else:\n",
    "            to_write[2].append(\"In: Correct\")\n",
    "        display_chunk(True, True, to_save, 'a')\n",
    "        if not to_save:\n",
    "            print(\"In: Incorrect\")\n",
    "        else:\n",
    "            to_write[2].append(\"In: Incorrect\")\n",
    "        display_chunk(True, False, to_save, 'b')\n",
    "        if not to_save:\n",
    "            print(\"Out: Incorrect\")\n",
    "        else:\n",
    "            to_write[2].append(\"Out: Incorrect\")\n",
    "        display_chunk(False, True, to_save, 'c')\n",
    "        if not to_save:\n",
    "            print(\"Out: Correct\")\n",
    "        else:\n",
    "            to_write[2].append(\"Out: Correct\")\n",
    "        display_chunk(False, False, to_save, 'd')\n",
    "        \n",
    "        \n",
    "def custom_label_regression_quantized(cat_name = \"bicycle\", region_colname = \"postalCode\", data_colname = \"median_income\", is_log_scale = False):\n",
    "    # calculate mean % of images with cat_name label over different income deciles\n",
    "    cat_ind = 0\n",
    "    try:\n",
    "        cat_ind = dataset.categories.index(cat_name)\n",
    "    except:\n",
    "        print(\"Please enter a valid category name from: {0}\".format(dataset.categories))\n",
    "        return\n",
    "        \n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    choro_data[region_colname] = choro_data[region_colname].astype(str)\n",
    "    region_to_income = choro_data[[region_colname, data_colname]].set_index(region_colname).T.to_dict('records')[0]\n",
    "    image_count = {}\n",
    "    for region in region_tags:\n",
    "        if region != 'out_of_boundary' and region in region_to_income and len(region_to_id[region]) > 250:\n",
    "\n",
    "            # rhs is # of images from region with at least 1 label of interest / # of images in region\n",
    "            image_count[region] = region_tags[region][cat_ind] / len(region_to_id[region])\n",
    "\n",
    "    income = []\n",
    "    label_count = []\n",
    "    for region in image_count:\n",
    "        income.append(region_to_income[region] / 1000)\n",
    "        label_count.append(image_count[region]*100)\n",
    "    ########################################\n",
    "    #### quantization work\n",
    "    num_bins = 10\n",
    "    # create df for quantization work, using pd.qcut\n",
    "    quantized_image_counts = pd.DataFrame({'income': income, 'count': label_count})\n",
    "    labels = np.arange(1, 10+1)*10\n",
    "    quantized_image_counts['quantiles_10'] = pd.qcut(quantized_image_counts['income'], q=num_bins, precision=0, labels = labels)\n",
    "    # average image counts over quantized income bins:\n",
    "    image_count = [] # overwrrite image_count (already stored in pandas df)\n",
    "    for i in labels:\n",
    "        image_count.append(np.mean(quantized_image_counts[quantized_image_counts['quantiles_10'] == i]['count']))\n",
    "\n",
    "    model = LinearRegression().fit(np.asarray(labels).reshape((-1, 1)), image_count)\n",
    "\n",
    "    labels = [str(int(i)) for i in labels]\n",
    "    plt.bar(labels, image_count)\n",
    "    # abline(model.intercept_, model.coef_)\n",
    "    plt.xlabel(\"Income decile of ZIP code\", fontsize = 30)\n",
    "    plt.ylabel(\"Mean % of Images with label\\n for ZIP codes in this decile\", fontsize =30)\n",
    "    plt.title(\"{}\".format(dataset.categories[cat_ind].capitalize()), fontsize = 30)\n",
    "    tksize = 19\n",
    "    plt.xticks(fontsize= tksize) \n",
    "    plt.yticks(fontsize= tksize)\n",
    "    \n",
    "    \n",
    "#     plt.savefig('/Users/home/Desktop/img_regressions/{0}_quantized'.format(dataset.categories[cat_ind]), dpi=300, bbox_inches='tight')\n",
    "# todo: add the save fig somewhere\n",
    "\n",
    "def weather_attr_pie(type_attr = \"region_weather\"):\n",
    "    weather_dat = info_stats['comb_attr'][type_attr] # todo: make this unbroken\n",
    "    overall_weather = {}\n",
    "    for region in weather_dat:\n",
    "        for weather_type in weather_dat[region]:\n",
    "            overall_weather[weather_type] = overall_weather.get(weather_type, 0) + weather_dat[region][weather_type]\n",
    "    overall_weather = {k: v for k, v in sorted(overall_weather.items(), key=lambda item: item[1], reverse = True)}\n",
    "\n",
    "\n",
    "\n",
    "    # Data to plot\n",
    "    labels = []\n",
    "    sizes = []\n",
    "\n",
    "\n",
    "    for x, y in overall_weather.items():\n",
    "        labels.append(x.capitalize())\n",
    "        sizes.append(y)\n",
    "    comb = []\n",
    "    for i in range(len(labels)):\n",
    "        comb.append(\"{0}: {1}\".format(labels[i].capitalize(), \"{:.0f}%\".format(round(100*sizes[i]/np.sum(sizes), 10))))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5), subplot_kw=dict(aspect=\"equal\"))\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.rcParams['font.size'] = 25 #22\n",
    "\n",
    "    def func(pct):\n",
    "        if pct < 1:\n",
    "            return \"{:.1f}%\".format(pct)\n",
    "        else:\n",
    "            return \"{:.0f}%\".format(pct)\n",
    "\n",
    "    wedges, texts= ax.pie(sizes, startangle=30)\n",
    "\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.1\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    kw = dict(arrowprops=dict(arrowstyle=\"-\"),\n",
    "              bbox=bbox_props, zorder=0, va=\"center\")\n",
    "\n",
    "    ax.legend(wedges, comb,\n",
    "              bbox_to_anchor=(0.98,0.21), loc=\"lower right\", \n",
    "                              bbox_transform=plt.gcf().transFigure,\n",
    "              fontsize = '18')\n",
    "    # plt.savefig('/Users/home/Desktop/img_regressions/{0}_NYC_legend'.format(type_attr),dpi=300, bbox_inches='tight')\n",
    "    # todo: add back the save fig\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses \n",
    "<a id=\"geo_tag_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression analysis for data of interest against instances of label of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over- and under- representations of tags by region. The first fraction shows how many of this region's tags are made up of this one, and the second fraction shows how many of all of the region's tags are made up of this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(tag_rep_by_region, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over- and under- representations of tags by subregion, fractions represent same thing as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(tag_rep_by_subregion, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How linearly separable images with a particular tag are in one subregion compared to the rest.\n",
    "The percentage in the dropdown menu indicates the accuracy of the classifier at distinguishing this subregion from the others, and the ratio represents test accuracy over that of random labels. The p-value is provided using the permutation test and gives a sense of how well the classifier exploits dependancies between features and labels (as opposed to overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset.geography_info_type == \"STRING_FORMATTED_LABEL\" and dataset.geography_label_string_type == \"COUNTRY_LABEL\"):\n",
    "    num_widget = widgets.IntSlider(min=1, max=20, step=1, value=5)\n",
    "    tag_widget = widgets.Dropdown(options=svm_options, layout=Layout(width='400px'))\n",
    "    all_things = [widgets.Label('Tag, acc, acc/acc_random',layout=Layout(padding='0px 0px 0px 5px', width='170px')), tag_widget, widgets.Label('Num',layout=Layout(padding='0px 5px 0px 40px', width='80px')), num_widget]\n",
    "\n",
    "    if first_pass:\n",
    "        show_svm_tag(best_tag, None)\n",
    "    ui = HBox(all_things)\n",
    "    out = widgets.interactive_output(show_svm_tag, {'tag': tag_widget, 'num': num_widget})\n",
    "    display(ui, out)\n",
    "else:\n",
    "    print(\"SVM metric not setup for gps datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate mean % of images with cat_name label over different income deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_label_regression_quantized(cat_name = \"bicycle\") # toggle cat_name for different category labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of attr distribution, using weather here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_attr_pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geo_lng Metric: Languages for tourist vs local\n",
    "<a id=\"geo_lng\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"metric10_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide geo_lng code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso3_to_subregion = pickle.load(open('util_files/iso3_to_subregion_mappings.pkl', 'rb'))\n",
    "mappings = pickle.load(open('util_files/country_lang_mappings.pkl', 'rb'))\n",
    "iso3_to_lang = mappings['iso3_to_lang']\n",
    "lang_to_iso3 = mappings['lang_to_iso3']\n",
    "\n",
    "lang_info = pickle.load(open('results/{}/geo_lng.pkl'.format(folder_name), 'rb'))\n",
    "counts = lang_info['lang_counts']\n",
    "country_with_langs = lang_info['country_with_langs']\n",
    "country_with_imgs = lang_info['country_with_imgs']\n",
    "\n",
    "gc = GeonamesCache()\n",
    "iso3_codes = list(gc.get_dataset_by_key(gc.get_countries(), 'iso3').keys())\n",
    "\n",
    "cm = plt.get_cmap('Blues')\n",
    "iso3_to_counts = {}\n",
    "iso3_to_bin = {}\n",
    "total = sum(counts.values())\n",
    "langcount_phrases = []\n",
    "for lang, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    lang_name = pycountry.languages.get(alpha_2=lang)\n",
    "    if lang_name is not None:\n",
    "        lang_name = lang_name.name\n",
    "    else:\n",
    "        lang_name = lang\n",
    "    langcount_phrases.append(\"{0}: {1}   {2}%\".format(lang_name, count, round(count*100./total, 4)))\n",
    "    try:\n",
    "        for iso3 in lang_to_iso3[lang]:\n",
    "            if iso3 not in iso3_to_counts.keys():\n",
    "                iso3_to_counts[iso3] = count\n",
    "            else:\n",
    "                iso3_to_counts[iso3] += count\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "bins = np.logspace(min(list(iso3_to_counts.values())), np.log2(max(list(iso3_to_counts.values()))+1), base=2.0)\n",
    "num_colors = len(bins)\n",
    "scheme = [cm(i / num_colors) for i in range(num_colors)]\n",
    "\n",
    "for key in iso3_to_counts.keys():\n",
    "    iso3_to_bin[key] = np.digitize(iso3_to_counts[key], bins) - 1\n",
    "\n",
    "def language_representation_map():\n",
    "    fig = plt.figure(figsize=(12, 7))\n",
    "    fontsize = 14\n",
    "\n",
    "    ax = fig.add_subplot(111, facecolor='w', frame_on=False)\n",
    "    fig.suptitle('Dataset representation by tag language for images', fontsize=fontsize, y=.95)\n",
    "\n",
    "    m = Basemap(lon_0=0, projection='robin')\n",
    "    m.drawmapboundary(color='w')\n",
    "\n",
    "    shapefile = 'util_files/ne_10m_admin_0_countries_lakes'\n",
    "\n",
    "    m.readshapefile(shapefile, 'units', color='#444444', linewidth=.2)\n",
    "    for info, shape in zip(m.units_info, m.units):\n",
    "        iso3 = info['ADM0_A3']\n",
    "        if iso3 not in iso3_to_bin.keys():\n",
    "            color = '#dddddd'\n",
    "        else:\n",
    "            try:\n",
    "                color = scheme[iso3_to_bin[iso3]]\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        patches = [Polygon(np.array(shape), True)]\n",
    "        pc = PatchCollection(patches)\n",
    "        pc.set_facecolor(color)\n",
    "        ax.add_collection(pc)\n",
    "\n",
    "    # Cover up Antarctica so legend can be placed over it.\n",
    "    ax.axhspan(0, 1000 * 1800, facecolor='w', edgecolor='w', zorder=2)\n",
    "\n",
    "    # Draw color legend.\n",
    "    ax_legend = fig.add_axes([0.35, 0.14, 0.3, 0.03], zorder=3)\n",
    "    cmap = mpl.colors.ListedColormap(scheme)\n",
    "    cb = mpl.colorbar.ColorbarBase(ax_legend, cmap=cmap, ticks=bins, boundaries=bins, orientation='horizontal')\n",
    "    spots = len(bins) // 4\n",
    "    spots = [0, spots, spots*2, spots*3, len(bins)- 1]\n",
    "    cb.ax.set_xticklabels([str(int(i)) if j in spots else '' for j, i in enumerate(bins)])\n",
    "    cb.ax.tick_params(labelsize=fontsize)\n",
    "    plt.show()\n",
    "\n",
    "def language_counts(topn):\n",
    "    if first_pass:\n",
    "        to_write[3] = ['(geo_lng) Most popular languages:']\n",
    "        for i in range(3):\n",
    "            to_write[3].append(langcount_phrases[i])\n",
    "    print(\"Most popular languages\\n\")\n",
    "    for i in range(topn):\n",
    "        print(langcount_phrases[i])\n",
    "    \n",
    "    print(\"\\nLeast popular languages\\n\")\n",
    "    for i in range(topn):\n",
    "        print(langcount_phrases[-1-i])\n",
    "        \n",
    "to_write_lower = {}\n",
    "to_write_upper = {}\n",
    "iso3_to_percent = {}\n",
    "subregion_to_percents = {}\n",
    "subregion_to_filepaths = {} # 0 is tourist, 1 is local\n",
    "subregion_to_embeddings = {} # 0 is tourist, 1 is local\n",
    "for country in country_with_langs.keys():\n",
    "    iso3 = country_to_iso3(country)\n",
    "    langs_in = 0\n",
    "    langs_out = {}\n",
    "    for lang in country_with_langs[country]:\n",
    "        try:\n",
    "            if lang in iso3_to_lang[iso3]:\n",
    "                langs_in += 1\n",
    "            else:\n",
    "                if lang in langs_out.keys():\n",
    "                    langs_out[lang] += 1\n",
    "                else:\n",
    "                    langs_out[lang] = 1\n",
    "        except KeyError:\n",
    "             print(\"This iso3 can't be found in iso3_to_lang: {}\".format(iso3))\n",
    "    this_total = len(country_with_langs[country])\n",
    "    others = ''\n",
    "    for lang in langs_out.keys():\n",
    "        if len(lang) == 2:\n",
    "            lang_name = pycountry.languages.get(alpha_2=lang)\n",
    "        elif len(lang) == 3:\n",
    "            lang_name = pycountry.languages.get(alpha_3=lang)\n",
    "        else:\n",
    "            print(\"{} is not 2 or 3 letters?\".format(lang))\n",
    "        if lang_name is not None:\n",
    "            lang_name = lang_name.name\n",
    "        else:\n",
    "            lang_name = lang\n",
    "        others += lang_name + \": \" + str(round(langs_out[lang]/this_total, 4)) + \", \"\n",
    "    if iso3 is not None:\n",
    "        subregion = iso3_to_subregion[iso3]\n",
    "        if subregion in subregion_to_percents.keys():\n",
    "            subregion_to_percents[subregion][0] += langs_in\n",
    "            subregion_to_percents[subregion][1] += this_total\n",
    "            subregion_to_filepaths[subregion][0].extend([chunk[1] for chunk in country_with_imgs[country][0]])\n",
    "            subregion_to_filepaths[subregion][1].extend([chunk[1] for chunk in country_with_imgs[country][1]])\n",
    "            subregion_to_embeddings[subregion][0].extend([chunk[0] for chunk in country_with_imgs[country][0]])\n",
    "            subregion_to_embeddings[subregion][1].extend([chunk[0] for chunk in country_with_imgs[country][1]])\n",
    "        else:\n",
    "            subregion_to_percents[subregion] = [langs_in, this_total]\n",
    "            subregion_to_filepaths[subregion] = [[chunk[1] for chunk in country_with_imgs[country][0]], [chunk[1] for chunk in country_with_imgs[country][1]]]\n",
    "            subregion_to_embeddings[subregion] = [[chunk[0] for chunk in country_with_imgs[country][0]], [chunk[0] for chunk in country_with_imgs[country][1]]]\n",
    "    tourist_percent = 1.0 - (langs_in / this_total)\n",
    "    lp_under, lp_over = wilson(tourist_percent, this_total)\n",
    "    phrase = '{0} has {1}% non-local tags, and the extra tags are:\\n\\n{2}'.format(country, round(100.*tourist_percent, 4), others)\n",
    "    to_write_lower[country] = [phrase, tourist_percent]\n",
    "    iso3_to_percent[iso3] = lp_under\n",
    "\n",
    "def lang_dist_by_country(country):\n",
    "    print(to_write_lower[country][0][:-2])\n",
    "\n",
    "subregion_to_accuracy = {}\n",
    "subregion_to_percents_phrase = {}\n",
    "for key in subregion_to_percents.keys():\n",
    "    if not os.path.exists('results/{0}/{1}/{2}_info.pkl'.format(folder_name, 10, key.replace(' ', '_'))):\n",
    "        low_bound, high_bound = wilson(1 - subregion_to_percents[key][0] / subregion_to_percents[key][1], subregion_to_percents[key][1])\n",
    "\n",
    "        clf = svm.SVC(kernel='linear', probability=False, decision_function_shape='ovr', class_weight='balanced')\n",
    "        clf_random = svm.SVC(kernel='linear', probability=False, decision_function_shape='ovr', class_weight='balanced')\n",
    "        tourist_features = subregion_to_embeddings[key][0]\n",
    "        local_features = subregion_to_embeddings[key][1]\n",
    "        if len(tourist_features) == 0 or len(local_features) == 0:\n",
    "            continue\n",
    "        tourist_features, local_features = np.array(tourist_features)[:, 0, :], np.array(local_features)[:, 0, :]\n",
    "        all_features = np.concatenate([tourist_features, local_features], axis=0)\n",
    "        num_features = int(np.sqrt(len(all_features)))\n",
    "        all_features = project(all_features, num_features)\n",
    "        labels = np.zeros(len(all_features))\n",
    "        labels[len(tourist_features):] = 1\n",
    "        clf.fit(all_features, labels)\n",
    "        acc = clf.score(all_features, labels)\n",
    "        probs = clf.decision_function(all_features)\n",
    "\n",
    "        np.random.shuffle(all_features)\n",
    "        clf_random.fit(all_features, labels)\n",
    "        acc_random = clf_random.score(all_features, labels)\n",
    "        value = acc / acc_random\n",
    "\n",
    "        subregion_to_percents_phrase[key] = [subregion_to_percents[key][0] / subregion_to_percents[key][1], '[{0} - {1}] for {2}'.format(round(low_bound, 4), round(high_bound, 4), subregion_to_percents[key][1])]\n",
    "        subregion_to_accuracy[key] = [acc, value, len(tourist_features), len(all_features), num_features]\n",
    "        tourist_probs = []\n",
    "        local_probs = []\n",
    "        for j in range(len(all_features)):\n",
    "            if j < len(tourist_features):\n",
    "                tourist_probs.append(-probs[j])\n",
    "            else:\n",
    "                local_probs.append(probs[j])        \n",
    "        pickle.dump([labels, tourist_probs, local_probs, subregion_to_filepaths[key]], open('results/{0}/{1}/{2}_info.pkl'.format(folder_name, 10, key.replace(' ', '_')), 'wb'))\n",
    "subregion_local_svm_loc = 'results/{0}/{1}/subregion_svm.pkl'.format(folder_name, 10)\n",
    "if not os.path.exists(subregion_local_svm_loc):\n",
    "    pickle.dump([subregion_to_accuracy, subregion_to_percents_phrase], open(subregion_local_svm_loc, 'wb'))\n",
    "\n",
    "def subregion_language_analysis(key, num):\n",
    "    to_save = False\n",
    "    acc, value, num_tourists, num_total, num_features = pickle.load(open(subregion_local_svm_loc, 'rb'))[0][key]\n",
    "    print_statement = \"Accuracy: {0}%, {1}x with {2} features. {3} out of {4} are tourist\".format(round(acc*100., 3), round(value, 3), num_features, num_tourists, num_total)\n",
    "    if num is None:\n",
    "        to_save = True\n",
    "        num = 5\n",
    "        to_write[4] = [\"(geo_lng) Subregion that is most linearly separable between locals and tourists.\"]\n",
    "        to_write[4].append(print_statement)\n",
    "    else:\n",
    "        print(print_statement)\n",
    "\n",
    "    labels, tourist_probs, local_probs, the_filepaths = pickle.load(open('results/{0}/{1}/{2}_info.pkl'.format(folder_name, 10, key.replace(' ', '_')), 'rb'))\n",
    "    \n",
    "    tourist_indices = np.argsort(np.array(tourist_probs))\n",
    "    local_indices = np.argsort(np.array(local_probs))\n",
    "    \n",
    "    the_indices = [tourist_indices, local_indices]\n",
    "    the_probs = [tourist_probs, local_probs]\n",
    "    \n",
    "    def display_chunk(local=0, correct=True, to_save=False, name=None):\n",
    "        this_filepaths = the_filepaths[local]\n",
    "        this_indices = the_indices[local]\n",
    "        this_probs = the_probs[local]\n",
    "        collected_filepaths = []\n",
    "        \n",
    "        if correct:\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter = -1\n",
    "        while len(collected_filepaths) < num:\n",
    "            try:\n",
    "                index_a = this_indices[counter]\n",
    "            except:\n",
    "                break\n",
    "            file_path_a = this_filepaths[index_a]\n",
    "            if (this_probs[index_a] > 0 and correct) or (this_probs[index_a] < 0 and not correct):\n",
    "                collected_filepaths.append(file_path_a)\n",
    "            if correct:\n",
    "                counter += 1\n",
    "            else:\n",
    "                counter -= -1\n",
    "        if to_save and first_pass:\n",
    "            this_loc = \"results/{0}/{1}/2_{2}.png\".format(folder_name, save_loc, name)\n",
    "            if len(collected_filepaths) > 0:\n",
    "                fig = plt.figure(figsize=(16, 8))\n",
    "                for i in range(num):\n",
    "                    ax = fig.add_subplot(1, num, i+1)\n",
    "                    ax.axis(\"off\")\n",
    "                    if i >= len(collected_filepaths):\n",
    "                        image = np.ones((3, 3, 3))\n",
    "                    else:\n",
    "                        image, _ = dataset.from_path(collected_filepaths[i])\n",
    "                        image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "                    im = ax.imshow(image, extent=SAME_EXTENT)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(this_loc, bbox_inches = 'tight')\n",
    "                plt.close()\n",
    "            else:\n",
    "                os.system(\"cp util_files/no_images.png {}\".format(this_loc))\n",
    "        elif len(collected_filepaths) > 0:\n",
    "            display_filepaths(collected_filepaths, width = 800//len(collected_filepaths), height=800//len(collected_filepaths))\n",
    "        else:\n",
    "            print(\"No images in this category\")\n",
    "    if not to_save:\n",
    "        print(\"Tourist: Correct\")\n",
    "    else:\n",
    "        to_write[4].append(\"Tourist: Correct\")\n",
    "    display_chunk(0, True, to_save, 'a')\n",
    "    if not to_save:\n",
    "        print(\"Tourist: Incorrect\")\n",
    "    else:\n",
    "        to_write[4].append(\"Tourist: Incorrect\")\n",
    "    display_chunk(0, False, to_save, 'b')\n",
    "    if not to_save:\n",
    "        print(\"Local: Incorrect\")\n",
    "    else:\n",
    "        to_write[4].append(\"Local: Incorrect\")\n",
    "    display_chunk(1, False, to_save, 'c')\n",
    "    if not to_save:\n",
    "        print(\"Local: Correct\")\n",
    "    else:\n",
    "        to_write[4].append(\"Local: Correct\")\n",
    "    display_chunk(1, True, to_save, 'd')\n",
    "\n",
    "subregion_to_accuracy, subregion_to_percents_phrase = pickle.load(open(subregion_local_svm_loc, 'rb'))\n",
    "subregion_svm_options = []\n",
    "most_different_subregion_value = 1.2\n",
    "most_different_subregion = None\n",
    "for subregion, value in sorted(subregion_to_accuracy.items(), key=lambda x: x[1][1], reverse=True):\n",
    "    acc, value, num_tourists, num_total, num_features = subregion_to_accuracy[subregion]\n",
    "    if acc > .75 and value > most_different_subregion_value:\n",
    "        most_different_subregion_value = value\n",
    "        most_different_subregion = subregion\n",
    "    subregion_svm_options.append(('{0}: {1}% and {2}x'.format(subregion, round(100.*acc, 3), round(value, 3)), subregion))\n",
    "    \n",
    "def non_local_lang_map():\n",
    "    iso3_to_bin = {}\n",
    "    num_colors = 20\n",
    "    cm = plt.get_cmap('Blues')\n",
    "    bins = np.linspace(0., 1., num_colors)\n",
    "\n",
    "    scheme = [cm(i / num_colors) for i in range(num_colors)]\n",
    "\n",
    "    for key in iso3_to_percent.keys():\n",
    "        iso3_to_bin[key] = np.digitize(iso3_to_percent[key], bins) - 1 # add a -1 here if np.linspace\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    fontsize = 20\n",
    "\n",
    "    ax = fig.add_subplot(111, facecolor='w', frame_on=False)\n",
    "    fig.suptitle('Percentage of tags in non-local language', fontsize=fontsize, y=.95)\n",
    "\n",
    "    m = Basemap(lon_0=0, projection='robin')\n",
    "    m.drawmapboundary(color='w')\n",
    "\n",
    "    shapefile = 'util_files/ne_10m_admin_0_countries_lakes'\n",
    "\n",
    "    m.readshapefile(shapefile, 'units', color='#444444', linewidth=.2)\n",
    "    for info, shape in zip(m.units_info, m.units):\n",
    "        iso3 = info['ADM0_A3']\n",
    "        if iso3 not in iso3_to_bin.keys():\n",
    "            color = '#dddddd'\n",
    "        else:\n",
    "            try:\n",
    "                color = scheme[iso3_to_bin[iso3]]\n",
    "            except IndexError:\n",
    "                print(iso3)\n",
    "                print(\"this index: {0} when length is {1}\".format(iso3_to_bin[iso3], len(scheme)))\n",
    "\n",
    "\n",
    "        patches = [Polygon(np.array(shape), True)]\n",
    "        pc = PatchCollection(patches)\n",
    "        pc.set_facecolor(color)\n",
    "        ax.add_collection(pc)\n",
    "\n",
    "    # Cover up Antarctica so legend can be placed over it.\n",
    "    ax.axhspan(0, 1000 * 1800, facecolor='w', edgecolor='w', zorder=2)\n",
    "\n",
    "    # Draw color legend.\n",
    "    ax_legend = fig.add_axes([0.35, 0.14, 0.3, 0.03], zorder=3)\n",
    "    cmap = mpl.colors.ListedColormap(scheme)\n",
    "    cb = mpl.colorbar.ColorbarBase(ax_legend, cmap=cmap, ticks=bins, boundaries=bins, orientation='horizontal')\n",
    "    spots = len(bins) // 3\n",
    "    spots = [0, spots, spots*2, len(bins)- 1]\n",
    "    cb.ax.set_xticklabels([str(round(i, 2)) if j in spots else '' for j, i in enumerate(bins)])\n",
    "    cb.ax.tick_params(labelsize=fontsize)\n",
    "    if first_pass:\n",
    "        plt.savefig(\"results/{0}/{1}/3.png\".format(folder_name, save_loc))\n",
    "        to_write[5] = [\"(geo_lng) Map representing the fraction of tags in a country that are not labeled in a local language.\"]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"geo_lng_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map that for each language, contributes to the counts of all countries that have that language as a national language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_representation_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Languages represented in the dataset, as detected by FastText. 3 letter acronyms mean that we could not automatically find the language corresponding to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(language_counts, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets you select a country, along with the fraction of images in that country that are tagged in a non-local language, to see what languages the tags are made up of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [('{0}: {1}'.format(country, round(value[1], 3)), country) for country, value in sorted(to_write_lower.items(), key=lambda x: x[1][1], reverse=True)]\n",
    "interact(lang_dist_by_country, country=widgets.Dropdown(options=pairs));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows the subregion and how accurately a linear model can separate images taken by locals vs tourists. Ratio is accuracy over that of randomly shuffled labels, as mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_sub_widget = widgets.IntSlider(min=1, max=20, step=1, value=5)\n",
    "key_widget = widgets.Dropdown(options=subregion_svm_options, layout=Layout(width='400px'))\n",
    "all_things = [widgets.Label('Tag, acc/acc_random, acc',layout=Layout(padding='0px 0px 0px 5px', width='170px')), key_widget, widgets.Label('Num',layout=Layout(padding='0px 5px 0px 40px', width='80px')), num_sub_widget]\n",
    "\n",
    "if first_pass and most_different_subregion is not None:\n",
    "    subregion_language_analysis(most_different_subregion, None)\n",
    "ui = HBox(all_things)\n",
    "out = widgets.interactive_output(subregion_language_analysis, {'key': key_widget, 'num': num_sub_widget})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows for each country, the percentage of tags in a non-local language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_local_lang_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence bounds on fracion of each subregion's languages that are non-local, and number of images from that subregion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bounds on fraction of each subregion's languages that are non-local\")\n",
    "for subregion, percent in sorted(subregion_to_percents_phrase.items(), key=lambda x: x[1][0], reverse=True):\n",
    "    print(\"{0}: {1}\".format(subregion, percent[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up summary pdf\n",
    "<a id=\"summarypdf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pass = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pdf(numbers):\n",
    "    for i in numbers:\n",
    "        if i in to_write.keys():\n",
    "            if i not in [2, 4]:\n",
    "                for sentence in to_write[i]:\n",
    "                    pdf.write(5, sentence)\n",
    "                    pdf.ln()\n",
    "            if i == 0:\n",
    "                pdf.image('results/{0}/{1}/0.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 2:\n",
    "                pdf.write(5, to_write[i][0])\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][1])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/1_a.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][2])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/1_b.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][3])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/1_c.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][4])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/1_d.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "            elif i == 4:\n",
    "                pdf.write(5, to_write[i][0])\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][1])\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][2])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/2_a.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][3])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/2_b.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][4])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/2_c.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][5])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/2_d.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "            elif i == 5:\n",
    "                pdf.image('results/{0}/{1}/3.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            pdf.ln(h=3)\n",
    "            pdf.dashed_line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "            pdf.ln(h=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font('Arial', 'B', 16)\n",
    "pdf.write(5, \"Geography-Based Summary\")\n",
    "pdf.ln()\n",
    "pdf.ln()\n",
    "\n",
    "# Overview Statistics\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Overview Statistics\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "write_pdf([0, 3, 5])\n",
    "\n",
    "# Interesting findings\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Sample Interesting Findings\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "write_pdf([1, 2, 4])\n",
    "\n",
    "# Interesting findings\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Some of the other metrics in the notebook\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "pdf.write(5, \"- (geo_ctr) Image breakdown by country and subregion\")\n",
    "pdf.ln()\n",
    "pdf.write(5, \"- (geo_ctr) Dataset representation map\")\n",
    "pdf.ln()\n",
    "pdf.write(5, \"- (geo_tag) Over/under representations of tags by subregion\")\n",
    "pdf.ln()\n",
    "pdf.write(5, \"- (geo_lng) Visual representation of what languages are represented\")\n",
    "pdf.ln()\n",
    "pdf.write(5, \"- (geo_lng) What languages each country's tags are in\")\n",
    "pdf.ln()\n",
    "\n",
    "pdf.output('results/{0}/{1}/summary.pdf'.format(folder_name, save_loc), \"F\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
