{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In this work weâ€™ve limited our analyses to a grouping based on perceived binary gender because these labels already exist in the datasets we look at, even though it is not at all inclusive of all gender categories. We use the terms male and female to refer to binarized socially-perceived gender expression, and not gender identity nor sex assigned at birth, neither of which can be inferred from an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. Fill in the dataset in section 1.1\n",
    "\n",
    "2. Run all cells\n",
    "\n",
    "3. Review the summary pdf generated AND/OR explore each metric below.\n",
    "    - All metrics are identified by a short keyword, and consist of a \"Setup\" and \"Analyses\" portion. The \"Setup\" portion contains code that does not need to be modified unless customization is needed, and the \"Analyses\" portion provides an interactive display of the results.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Initial Setup](#setup) <br/>\n",
    "    1.1 [Dataset](#dataset)\n",
    "2. att_siz Metric: [Distance from center, size, gender label inference](#att_siz)<br/>\n",
    "    2.1 [Setup](#att_siz_setup)<br/>\n",
    "    2.2 [Analyses](#att_siz_analyses)\n",
    "3. att_cnt Metric: [Object occurrences and cooccurrences](#att_cnt)<br/>\n",
    "    3.1 [Setup](#att_cnt_setup)<br/>\n",
    "    3.2 [Analyses](#att_cnt_analyses)\n",
    "4. att_dis Metric: [Distance from object as proxy for interaction](#att_dis)<br/>\n",
    "    4.1 [Setup](#att_dis_setup)<br/>\n",
    "    4.2 [Analyses](#att_dis_analyses)\n",
    "5. att_clu Metric: [Linearly separable objects by gender](#att_clu)<br/>\n",
    "    5.1 [Setup](#att_clu_setup)<br/>\n",
    "    5.2 [Analyses](#att_clu_analyses)\n",
    "6. att_scn Metric [Scenes](#att_scn)<br/>\n",
    "    6.1 [Setup](#att_scn_setup)<br/>\n",
    "    6.2 [Analyses](#att_scn_analyses)\n",
    "7. [Setting up summary pdf](#summarypdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "<a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "try: \n",
    "    import datasets\n",
    "except ModuleNotFoundError: # switch to parent directory on first run \n",
    "    os.chdir(os.pardir)\n",
    "    import datasets\n",
    "import pickle\n",
    "import itertools\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import PIL.Image\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from math import sqrt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "from scipy.spatial.distance import squareform\n",
    "import pycountry\n",
    "from geonamescache import GeonamesCache\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "import textwrap\n",
    "import matplotlib.patches as mpatches\n",
    "import operator\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import imageio\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, HBox, Layout\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "from matplotlib.transforms import Bbox\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = sns.color_palette('Set2', 2)\n",
    "SAME_EXTENT = (-0.5, 6.5, -0.5, 6.5)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not os.path.exists(\"dataloader_files\"):\n",
    "    os.mkdir(\"dataloader_files\")\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook\n",
    "def hide_toggle(for_next=False, toggle_text='Toggle show/hide'):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide helper functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder(num, folder):\n",
    "    if not os.path.exists(\"results/{0}/{1}\".format(folder, num)):\n",
    "        os.mkdir(\"results/{0}/{1}\".format(folder, num))\n",
    "    file = open(\"results/{0}/{1}/results.txt\".format(folder, num), \"w\")\n",
    "    return file\n",
    "\n",
    "# Projecting a set of features into a lower-dimensional subspace with PCA\n",
    "def project(features, dim):\n",
    "    standardized = StandardScaler().fit_transform(features)\n",
    "    pca = PCA(n_components=dim)\n",
    "    principalComponents = pca.fit_transform(X=standardized)\n",
    "    return principalComponents\n",
    "\n",
    "# Calculating the binomial proportion confidence interval\n",
    "def wilson(p, n, z = 1.96):\n",
    "    denominator = 1 + z**2/n\n",
    "    centre_adjusted_probability = p + z*z / (2*n)\n",
    "    adjusted_standard_deviation = sqrt((p*(1 - p) + z*z / (4*n)) / n)\n",
    "    \n",
    "    lower_bound = (centre_adjusted_probability - z*adjusted_standard_deviation) / denominator\n",
    "    upper_bound = (centre_adjusted_probability + z*adjusted_standard_deviation) / denominator\n",
    "    return (lower_bound, upper_bound)\n",
    "\n",
    "def country_to_iso3(country):\n",
    "    missing = {'South+Korea': 'KOR',\n",
    "            'North+Korea': 'PRK',\n",
    "            'Laos': 'LAO',\n",
    "            'Caribbean+Netherlands': 'BES',\n",
    "            'St.+Lucia': 'LCA',\n",
    "            'East+Timor': 'TLS',\n",
    "            'Democratic+Republic+of+Congo': 'COD',\n",
    "            'Swaziland': 'SWZ',\n",
    "            'Cape+Verde': 'CPV',\n",
    "            'C%C3%B4te+d%C2%B4Ivoire': 'CIV',\n",
    "            'Ivory+Coast': 'CIV',\n",
    "            'Channel+Islands': 'GBR'\n",
    "            }\n",
    "    try:\n",
    "        iso3 = pycountry.countries.search_fuzzy(country.replace('+', ' '))[0].alpha_3\n",
    "    except LookupError:\n",
    "        try:\n",
    "            iso3 = missing[country]\n",
    "        except KeyError:\n",
    "            iso3 = None\n",
    "    return iso3\n",
    "\n",
    "def full_extent(ax, pad=0.0):\n",
    "    \"\"\"Get the full extent of an axes, including axes labels, tick labels, and\n",
    "    titles.\"\"\"\n",
    "    # For text objects, we need to draw the figure first, otherwise the extents\n",
    "    # are undefined.\n",
    "    ax.figure.canvas.draw()\n",
    "    items = ax.get_xticklabels() + ax.get_yticklabels() \n",
    "    items += [ax, ax.title]\n",
    "    bbox = Bbox.union([item.get_window_extent() for item in items])\n",
    "\n",
    "    return bbox.expanded(1.0 + pad, 1.0 + pad)\n",
    "\n",
    "def display_filepaths(filepaths, width=100, height=100):\n",
    "    try: \n",
    "        sidebyside = widgets.HBox([widgets.Image(value=open(filepath, 'rb').read(), format='png', width=width, height=height) for filepath in filepaths], layout=Layout(height='{}px'.format(height)))\n",
    "        display(sidebyside)\n",
    "    except FileNotFoundError: \n",
    "        print('Some functionality not available for CocoDatasetNoImages Class')\n",
    "\n",
    "def dec_to_show(p):\n",
    "    if p < .001:\n",
    "        return '{:0.3e}'.format(p)\n",
    "    else:\n",
    "        return round(p, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Fill in below with dataset and file path names\n",
    "<a id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([ \n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "dataset = datasets.CoCoDataset(transform_train)\n",
    "folder_name = 'coco_example'\n",
    "\n",
    "# dataset = datasets.OpenImagesDataset(transform_train)\n",
    "# folder_name = 'openimages_supp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = '1_pager_gen'\n",
    "os.system(\"rm -r results/{0}/{1}\".format(folder_name, save_loc))\n",
    "file = folder(save_loc, folder_name)\n",
    "first_pass = True\n",
    "to_write = {}\n",
    "if not os.path.exists(\"checkpoints/{}\".format(folder_name)):\n",
    "    os.mkdir(\"checkpoints/{}\".format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = None\n",
    "\n",
    "distances = pickle.load(open(\"results/{}/att_dis.pkl\".format(folder_name), \"rb\"))\n",
    "sample_file = distances[0][0][0][3]\n",
    "if not os.path.exists(sample_file):\n",
    "    assert data_folder is not None, \"initialize data_folder with folder path of your data\"\n",
    "    dataset.init_folder_path(data_folder)\n",
    "    print(\"overwriting from_path() function\")\n",
    "    dataset.from_path = dataset.from_path_prerun\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# att_siz Metric: Distance from center, size, gender label inference\n",
    "<a id=\"att_siz\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"att_siz_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide att_siz code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pickle.load(open(\"results/{}/att_siz.pkl\".format(folder_name), \"rb\"))\n",
    "sizes = info['sizes']\n",
    "dists = info['distances']\n",
    "tiny_sizes = info['tiny_sizes']\n",
    "no_faces = info['noface_sizes']\n",
    "f_scenes = np.array(list(itertools.chain.from_iterable([chunk[2] for chunk in no_faces[0]])) + list(itertools.chain.from_iterable([chunk[1] for chunk in tiny_sizes[0]])))\n",
    "m_scenes = np.array(list(itertools.chain.from_iterable([chunk[2] for chunk in no_faces[1]])) + list(itertools.chain.from_iterable([chunk[1] for chunk in tiny_sizes[1]])))\n",
    "tiny_sizes[0] = [chunk[0] for chunk in tiny_sizes[0]]\n",
    "tiny_sizes[1] = [chunk[0] for chunk in tiny_sizes[1]]\n",
    "no_faces[0] = [chunk[0] for chunk in no_faces[0]]\n",
    "no_faces[1] = [chunk[0] for chunk in no_faces[1]]\n",
    "\n",
    "info = pickle.load(open('util_files/places_scene_info.pkl', 'rb'))\n",
    "idx_to_scene = info['idx_to_scene']\n",
    "idx_to_scenegroup = info['idx_to_scenegroup']\n",
    "sceneidx_to_scenegroupidx = info['sceneidx_to_scenegroupidx']\n",
    "\n",
    "xaxis = [idx_to_scenegroup[i] for i in range(len(idx_to_scenegroup))]\n",
    "xaxis = ['\\n'.join(textwrap.wrap(chunk, width=30)) for chunk in xaxis]\n",
    "barWidth = .4\n",
    "fontsize = 15\n",
    "\n",
    "r1 = np.arange(len(idx_to_scenegroup))\n",
    "scenes_f = np.bincount(f_scenes)\n",
    "scenes_m = np.bincount(m_scenes)\n",
    "\n",
    "total_images = np.sum(scenes_f+scenes_m)\n",
    "scenes_f, scenes_m = scenes_f / total_images, scenes_m / total_images\n",
    "all_sizes = [tiny_sizes[0]+no_faces[0]+sizes[0], tiny_sizes[1]+no_faces[1]+sizes[1]]\n",
    "\n",
    "def numbers_where_gender_inferred():\n",
    "\n",
    "    tiny_females, tiny_males = len(tiny_sizes[0]), len(tiny_sizes[1])\n",
    "    noface_females, noface_males = len(no_faces[0]), len(no_faces[1])\n",
    "    original_females, original_males = tiny_females+noface_females+len(sizes[0]), tiny_males+noface_males+len(sizes[1])\n",
    "    print(\"Total images: {0}, and {1} were male, {2} were female. {3}x\".format(original_females+original_males, original_males, original_females, round(original_males/original_females, 4)))\n",
    "    try:\n",
    "        print(\"Discarded {0} images for being too small, and {1} were male, {2} were female. {3}x\".format(tiny_females+tiny_males, tiny_males, tiny_females, round(tiny_males/tiny_females, 4)))\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Discarded {0} images for being too small, and {1} were male, {2} were female.\".format(tiny_females+tiny_males, tiny_males, tiny_females))\n",
    "    try:\n",
    "        print(\"Discarded {0} images for having no face detected, and {1} were male, {2} were female. {3}x\".format(noface_females+noface_males,noface_males, noface_females, round(noface_males/noface_females, 4)))\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Discarded {0} images for having no face detected, and {1} were male, {2} were female.\".format(noface_females+noface_males,noface_males, noface_females))\n",
    "    print(\"There were {0} male images and {1} female images included. {2}x\".format(len(sizes[1]), len(sizes[0]), round(len(sizes[1])/len(sizes[0]), 4)))\n",
    "\n",
    "    # P(male|no face)\n",
    "    nogender_males = tiny_males + noface_males\n",
    "    nogender_females = tiny_females + noface_females\n",
    "    prob = nogender_males / (nogender_males + nogender_females)\n",
    "    prob_statement = \"Probability image is labeled male when it should not be, i.e. given there's no face detected or person is too small: {}\".format(round(prob, 4))\n",
    "    if (prob < .45 or prob > .55) and first_pass:\n",
    "        to_write[0] = [\"(att_siz) \" + prob_statement]\n",
    "    print()\n",
    "    print(prob_statement)\n",
    "\n",
    "\n",
    "def scenes_where_no_face():\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    \n",
    "    scene_mf_ratios = np.divide(scenes_m, scenes_f)\n",
    "    order = np.argsort(scene_mf_ratios)\n",
    "    \n",
    "    biggest_diff_scenes = []\n",
    "    if first_pass and scene_mf_ratios[order[-1]] > 1.:\n",
    "        biggest_diff_scenes.append(\"{0} is the scene where the label of male is most likely to be picked over that of female\".format(xaxis[order[-1]]))\n",
    "    if first_pass and scene_mf_ratios[order[0]] < 1.:\n",
    "        biggest_diff_scenes.append(\"{0} is the scene where the label of female is most likely to be picked over that of male\".format(xaxis[order[0]]))\n",
    "    if len(biggest_diff_scenes) > 0:\n",
    "        to_write[1] = biggest_diff_scenes\n",
    "    \n",
    "    plt.barh(r2, scenes_m[order], height=barWidth, color=COLORS[1], edgecolor='white', label='male')\n",
    "    plt.barh(r1, scenes_f[order], height=barWidth, color=COLORS[0], edgecolor='white', label='female')\n",
    "    plt.yticks([r + (barWidth/2.) for r in range(len(r1))], np.array(xaxis)[order], fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.ylabel('Scene', fontsize=fontsize)\n",
    "    plt.xlabel('Proportion of Images Discarded with this Scene', fontsize=fontsize)\n",
    "    plt.legend(loc='best', prop={'size': fontsize})\n",
    "    plt.title(\"Scenes where gender was labeled when it should not have been\", fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "    plt.gcf().subplots_adjust(bottom=0.18)\n",
    "    plt.gcf().subplots_adjust(left=0.4)\n",
    "    plt.show()\n",
    "    \n",
    "comparisons_widget = widgets.Dropdown(options=['Sizes', 'Distances', 'All sizes', 'Sizes where no face was detected'], value='Sizes')\n",
    "prob_v_freq_toggle = widgets.ToggleButton(value=True, description=\"Probability View\", layout=Layout(width='200px'))\n",
    "\n",
    "def compare_sizedist(metric, view):\n",
    "    def mean_and_std(data, data_type):\n",
    "        f = data[0]\n",
    "        m = data[1]\n",
    "\n",
    "        histf, binsf = np.histogram(f, bins='auto')\n",
    "        histm, binsm = np.histogram(m, bins='auto')\n",
    "\n",
    "        if binsm[-1] > binsf[-1]:\n",
    "            bins = binsm\n",
    "        else:\n",
    "            bins = binsf\n",
    "        try:\n",
    "            biggest = max(max(f), max(m))\n",
    "        except ValueError:\n",
    "            biggest = 1.\n",
    "        prob_bins = np.arange(0, biggest, biggest / 15)\n",
    "        histf, _ = np.histogram(f, bins=prob_bins)\n",
    "        histm, _ = np.histogram(m, bins=prob_bins)\n",
    "        histf = histf/len(f)\n",
    "        histm = histm/len(m)\n",
    "        conditioned = np.divide(histf, histf+histm)\n",
    "        \n",
    "        f_mean = np.mean(f)\n",
    "        m_mean = np.mean(m)\n",
    "        f_std = np.std(f)\n",
    "        m_std = np.std(m)\n",
    "        t, p = stats.ttest_ind(f, m)\n",
    "        \n",
    "        to_save = False\n",
    "        if view is None and p < .05 and first_pass:\n",
    "            data_descrip = ''\n",
    "            if data_type == 'dists':\n",
    "                data_descrip = 'Distance from center'\n",
    "            if data_type == 'sizes':\n",
    "                data_descrip = 'Fraction of image taken up by a person'\n",
    "            to_write[2] = [\"(att_siz) {0} is different between the genders with a p-value of {1}, distribution shown below\".format(data_descrip, dec_to_show(p))]\n",
    "            to_save = True\n",
    "        \n",
    "        if view is None or view:\n",
    "\n",
    "            prob_v_freq_toggle.description = 'Probability view'\n",
    "            histogram_f, bins_f = np.histogram(f, bins='auto')\n",
    "            bin_centers_f = 0.5*(bins_f[1:] + bins_f[:-1])\n",
    "            area_f = np.trapz(histogram_f, x=bin_centers_f)\n",
    "            plt.plot(bin_centers_f, histogram_f/area_f, alpha=.75, label='female', color=COLORS[0])\n",
    "\n",
    "            histogram_m, bins_m = np.histogram(m, bins='auto')\n",
    "            bin_centers_m = 0.5*(bins_m[1:] + bins_m[:-1])\n",
    "            area_m = np.trapz(histogram_m, x=bin_centers_m)\n",
    "            plt.plot(bin_centers_m, histogram_m/area_m, alpha=.75, label='male', color=COLORS[1])\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.xlabel('Distances' if data_type == 'dists' else 'Sizes')\n",
    "            plt.ylabel('Frequency')\n",
    "            if to_save and first_pass:\n",
    "                plt.savefig(\"results/{0}/{1}/0.png\".format(folder_name, save_loc))\n",
    "                plt.close()\n",
    "            elif view is None:\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "        else:\n",
    "            prob_v_freq_toggle.description = 'Frequency view'\n",
    "            plt.scatter(np.arange(len(conditioned)), conditioned)\n",
    "            plt.plot(np.arange(len(conditioned)), [.5] * len(conditioned), color='red')\n",
    "            plt.xticks(np.arange(len(conditioned)), ['{0} to {1}'.format(round(prob_bins[i], 2), round(prob_bins[i+1], 2)) for i in range(len(conditioned))], rotation='vertical', fontsize='6')\n",
    "            plt.xlabel('Distances' if data_type == 'dists' else 'Sizes')\n",
    "            plt.ylabel('Probability of Female')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        if view is not None:\n",
    "            print(\"Female: {0} +- {1}\\n\".format(round(f_mean, 4), round(f_std, 4)))\n",
    "            print(\"Male: {0} +- {1}\\n\".format(round(m_mean, 4), round(m_std, 4)))\n",
    "            print(\"P value that they are different: {}\\n\".format('{:0.3e}'.format(p)))\n",
    "        \n",
    "\n",
    "    if metric == 'Sizes':\n",
    "        mean_and_std(sizes, 'sizes')\n",
    "    elif metric == 'All sizes':\n",
    "        mean_and_std(all_sizes, 'all_sizes')\n",
    "    elif metric == 'Sizes where no face was detected':\n",
    "        mean_and_std(no_faces, 'no_faces')  \n",
    "    elif metric == 'Distances':\n",
    "        mean_and_std(dists, 'dists')\n",
    "    elif metric == 'first_pass' and first_pass:\n",
    "        mean_and_std(sizes, 'sizes')\n",
    "        mean_and_std(dists, 'dists')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"att_siz_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics on how many gender labels were inferred when they shouldn't have been because the person was either too small, or no face was detected. The scenes where this happens are shown to investigate if perhaps annotators are relying on contextual clues to make this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_where_gender_inferred()\n",
    "scenes_where_no_face()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution by gender of sizes and distances, both after removing images where gender was unlikely to be able to be labeled, all sizes before any images were removed, and the sizes of people where no face was detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_pass:\n",
    "    compare_sizedist('first_pass', None)\n",
    "all_things = [comparisons_widget, prob_v_freq_toggle]\n",
    "ui = HBox(all_things)\n",
    "out = widgets.interactive_output(compare_sizedist, {'metric': comparisons_widget, 'view': prob_v_freq_toggle})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# att_cnt Metric: Object occurrences and cooccurrences\n",
    "<a id=\"att_cnt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"att_cnt_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide att_cnt code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset.categories\n",
    "names = dataset.labels_to_names\n",
    "counts = pickle.load(open(\"results/{}/att_cnt.pkl\".format(folder_name), \"rb\"))\n",
    "\n",
    "topn = 10\n",
    "counts_mat_f = np.zeros((len(categories), len(categories)))\n",
    "counts_mat_m = np.zeros((len(categories), len(categories)))\n",
    "\n",
    "# Create the cooccurrence matrix from the counts\n",
    "counts_f = counts[0]\n",
    "counts_m = counts[1]\n",
    "for key in counts_f.keys():\n",
    "    a, b = key.split('-')\n",
    "    a, b = int(a), int(b)\n",
    "    counts_mat_f[b][a] = counts_f[key]\n",
    "    counts_mat_f[a][b] = counts_f[key]\n",
    "    counts_mat_m[b][a] = counts_m[key]\n",
    "    counts_mat_m[a][b] = counts_m[key]\n",
    "instance_counts_f = np.diagonal(counts_mat_f)\n",
    "instance_counts_m = np.diagonal(counts_mat_m)\n",
    "normalized_f = np.divide(counts_mat_f, instance_counts_f)\n",
    "normalized_m = np.divide(counts_mat_m, instance_counts_m)\n",
    "\n",
    "def make_dist(counts, total):\n",
    "    samples = np.zeros(total)\n",
    "    samples[:int(counts)] = 1\n",
    "    return samples\n",
    "\n",
    "# Looking at if the number of times an object appears with different genders is statistically significant\n",
    "p_values = [stats.ttest_ind(make_dist(instance_counts_f[i], dataset.num_gender_images[0]), make_dist(instance_counts_m[i], dataset.num_gender_images[1]))[1] for i in range(len(instance_counts_f))]\n",
    "norm_instance_counts_f = instance_counts_f / dataset.num_gender_images[0]\n",
    "norm_instance_counts_m = instance_counts_m / dataset.num_gender_images[1]\n",
    "\n",
    "xaxis = [names[i] for i in categories]\n",
    "barWidth = .4\n",
    "\n",
    "# Graphs the counts of each supercategory by gender\n",
    "def supercategory_by_gender():\n",
    "    if dataset.group_mapping is not None:\n",
    "        fontsize = 15\n",
    "        supercategory_counts_f = np.zeros(len(dataset.supercategories_to_names))\n",
    "        supercategory_counts_m = np.zeros(len(dataset.supercategories_to_names))\n",
    "        for i in range(len(categories)):\n",
    "            supercat = dataset.group_mapping(categories[i])\n",
    "            supercategory_counts_f[supercat] += norm_instance_counts_f[i]\n",
    "            supercategory_counts_m[supercat] += norm_instance_counts_m[i]\n",
    "        supercategory_counts_f = supercategory_counts_f[1:]\n",
    "        supercategory_counts_m = supercategory_counts_m[1:]\n",
    "\n",
    "        supercategory_counts_f, supercategory_counts_m = np.array(supercategory_counts_f), np.array(supercategory_counts_m)\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        r1 = np.arange(len(dataset.supercategories_to_names)-1)\n",
    "        r2 = [x + barWidth for x in r1]\n",
    "        order = np.argsort(np.divide(supercategory_counts_m, supercategory_counts_f))\n",
    "        plt.barh(r2, supercategory_counts_m[order], height=barWidth, color=COLORS[1], edgecolor='white', label='male')\n",
    "        plt.barh(r1, supercategory_counts_f[order], height=barWidth, color=COLORS[0], edgecolor='white', label='female')\n",
    "        plt.yticks([r + (barWidth/2.) for r in range(len(r1))], np.array([dataset.supercategories_to_names[i+1] for i in range(len(r1))])[order], fontsize=fontsize)\n",
    "        plt.xticks(fontsize=fontsize)\n",
    "        plt.legend(loc='best', fontsize=fontsize)\n",
    "        plt.ylabel('Object Category', fontsize=fontsize, labelpad=20)\n",
    "        plt.xlabel('Fraction of Images that contain this Category', fontsize=fontsize, labelpad=20, x=.3)\n",
    "        plt.tight_layout()\n",
    "        plt.gcf().subplots_adjust(bottom=0.18)\n",
    "        plt.gcf().subplots_adjust(left=0.28)\n",
    "        if first_pass:\n",
    "            to_write[3] = ['(att_cnt) Distribution of object categories each gender appears with, sorted by ratio between the two.']\n",
    "            plt.savefig(\"results/{0}/{1}/1.png\".format(folder_name, save_loc))\n",
    "        plt.show()\n",
    "\n",
    "# Graphs the ratio of instance counts if they are statistically significant\n",
    "indices_to_keep = [i for i in range(len(instance_counts_f)) if categories[i] not in dataset.people_labels]\n",
    "instance_sig_f = norm_instance_counts_f[indices_to_keep]\n",
    "instance_sig_m = norm_instance_counts_m[indices_to_keep]\n",
    "xaxis_sig = np.array(xaxis)[indices_to_keep]\n",
    "pvalues_sig = np.array(p_values)[indices_to_keep]\n",
    "all_ratios = []\n",
    "\n",
    "for i in range(len(instance_sig_f)):\n",
    "    f_count = instance_sig_f[i]\n",
    "    m_count = instance_sig_m[i]\n",
    "    if f_count > m_count:\n",
    "        all_ratios.append(f_count / m_count)\n",
    "    else:\n",
    "        all_ratios.append(-m_count / f_count)\n",
    "\n",
    "    if np.absolute(all_ratios[-1]) <= 1:\n",
    "        all_ratios[-1] = 0\n",
    "\n",
    "all_ratios = np.array(all_ratios)\n",
    "\n",
    "def show_instance_ratios(sort_by, topn):\n",
    "    infinities = np.concatenate([np.where(all_ratios == -np.inf)[0], np.where(all_ratios == np.inf)[0]], axis=None)\n",
    "    \n",
    "    all_ratios[all_ratios == -np.inf] = 0\n",
    "    all_ratios[all_ratios == np.inf] = 0\n",
    "    \n",
    "    to_save = False\n",
    "    if topn is None:\n",
    "        topn = 5\n",
    "        to_save = True\n",
    "    \n",
    "    if sort_by == 'pvalue':\n",
    "        top_indices = np.argsort(pvalues_sig)[:topn][::-1]\n",
    "        if to_save:\n",
    "            for i in reversed(range(topn)):\n",
    "                if pvalues_sig[top_indices[i]] >= .05:\n",
    "                    top_indices.pop()\n",
    "            topn = len(top_indices)\n",
    "            if len(top_indices) == 0:\n",
    "                return\n",
    "                to_save = False\n",
    "    elif sort_by == 'ratio':\n",
    "        top_indices = np.argsort(np.absolute(all_ratios))[-topn:]\n",
    "    instance_sig_f_topn = instance_sig_f[top_indices]\n",
    "    instance_sig_m_topn = instance_sig_m[top_indices]\n",
    "    xaxis_sig_topn = xaxis_sig[top_indices]\n",
    "    all_ratios_topn = all_ratios[top_indices]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, max(2, topn // 3)))\n",
    "    fontsize = 10\n",
    "    less_indices = np.where(np.array(all_ratios_topn) < 0)[0]\n",
    "    pltbar = plt.barh(np.arange(topn), np.absolute(all_ratios_topn))\n",
    "    for ind in less_indices:\n",
    "        pltbar[ind].set_color('C1')\n",
    "    plt.yticks(np.arange(topn), xaxis_sig_topn[:topn], rotation='horizontal', fontsize=fontsize)\n",
    "    ax = plt.gca()\n",
    "    ax.tick_params(axis=\"x\", bottom=True, top=True, labelbottom=True, labeltop=True)\n",
    "    ax.tick_params(axis=\"y\", left=False, right=True, labelleft=False, labelright=True)\n",
    "    plt.ylabel('Categories', fontsize=fontsize)\n",
    "    plt.xlabel('Counts Ratio', fontsize=fontsize)\n",
    "    patch1 = mpatches.Patch(color='C0', label='female')\n",
    "    patch2 = mpatches.Patch(color='C1', label='male')\n",
    "    handles = [patch1, patch2]\n",
    "    fontP = FontProperties()\n",
    "    fontP.set_size(12)\n",
    "    lgd = plt.legend(handles=handles, prop=fontP, loc='best')\n",
    "    plt.tight_layout()\n",
    "    if to_save:\n",
    "        to_write[4] = ['(att_cnt) Objects that are most statistically significantly represented with one gender over the other.']\n",
    "        plt.savefig(\"results/{0}/{1}/2.png\".format(folder_name, save_loc))\n",
    "        plt.close()\n",
    "        return\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    if len(infinities) > 0:\n",
    "        print(\"Categories where one gender had 0 occurrences with that object:\\n\")\n",
    "        \n",
    "    # Categories where one gender had 0 occurrences with that object\n",
    "    for index in infinities:\n",
    "        print(\"{0}: F={1}, M={2}\\n\".format(xaxis_sig_topn[index], int(instance_sig_f_topn[index]*dataset.num_gender_images[0]), int(instance_sig_m_topn[index]*dataset.num_gender_images[1])))\n",
    "\n",
    "    print(\"Numbers for graph:\\n\")\n",
    "\n",
    "    for index in reversed(range(topn)):\n",
    "        print(\"{0}: F={1}, M={2}, rat: {3}\".format(xaxis_sig_topn[index], int(instance_sig_f_topn[index]*dataset.num_gender_images[0]), int(instance_sig_m_topn[index]*dataset.num_gender_images[1]), round(np.absolute(all_ratios_topn[index]), 4)))\n",
    "\n",
    "p_values = np.zeros_like(counts_mat_f)\n",
    "f_mi = np.zeros_like(counts_mat_f)\n",
    "m_mi = np.zeros_like(counts_mat_f)\n",
    "f_mi_wilson = np.zeros_like(counts_mat_f)\n",
    "m_mi_wilson = np.zeros_like(counts_mat_f)\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(categories)):\n",
    "        if categories[i] in dataset.people_labels or categories[j] in dataset.people_labels:\n",
    "            p_values[i][j] = -1\n",
    "        else:\n",
    "            len_f = instance_counts_f[j] + instance_counts_f[i] - counts_mat_f[i][j]\n",
    "            f = np.zeros(int(len_f))\n",
    "            f[:int(counts_mat_f[i][j])] = 1\n",
    "            len_m = instance_counts_m[j] + instance_counts_m[i] - counts_mat_m[i][j]\n",
    "            m = np.zeros(int(len_m))\n",
    "            m[:int(counts_mat_m[i][j])] = 1\n",
    "            f_mi[i][j] = np.mean(f)\n",
    "            m_mi[i][j] = np.mean(m)\n",
    "            f_mi_wilson[i][j] = wilson(np.mean(f), len_f)[0]\n",
    "            m_mi_wilson[i][j] = wilson(np.mean(m), len_m)[0]\n",
    "            p_values[i][j] = stats.ttest_ind(f, m)[1]\n",
    "\n",
    "flat_p = p_values.flatten()\n",
    "flat_p[flat_p!=flat_p] = float(\"inf\")\n",
    "flat_p[flat_p == -1] = float(\"inf\")\n",
    "normalized_indices = np.argsort(flat_p)\n",
    "\n",
    "def cooccurrence_counts_mi(topn):\n",
    "    print(\"Statistically significant mutual information:\\n\")\n",
    "    i, j = 0, 0\n",
    "    while j < topn:\n",
    "        index = normalized_indices[i]\n",
    "        a, b = index % len(categories), index // len(categories)\n",
    "        if a < b:\n",
    "            print(\"{0} - {1}: {2}\".format(names[categories[a]], names[categories[b]], '{:0.3e}'.format(flat_p[index])))\n",
    "            print(\"F: {0}, M: {1}\".format(round(f_mi.flatten()[index], 4), round(m_mi.flatten()[index], 4)))\n",
    "            print()\n",
    "            j += 1\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"att_cnt_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of object categories by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.group_mapping is not None:\n",
    "    supercategory_by_gender()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio between how often an object is represented with each gender (normalized), sorted by p-value or ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instanceratio_slider = widgets.IntSlider(min=5, max=50, step=1, value=10)\n",
    "instanceratio_sortby = widgets.Dropdown(options=['pvalue', 'ratio'], value='pvalue')\n",
    "\n",
    "if first_pass:\n",
    "    show_instance_ratios('pvalue', None)\n",
    "\n",
    "all_things = [instanceratio_sortby, instanceratio_slider]\n",
    "ui = HBox(all_things)\n",
    "out = widgets.interactive_output(show_instance_ratios, {'sort_by': instanceratio_sortby, 'topn': instanceratio_slider})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most statistically significant object cooccurrences, measured by mutual information, between the genders. The p-value is shown for the difference of the distributions, and the numbers for \"F\" and \"M\" indicate the mutual information for each gender on the object pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(cooccurrence_counts_mi, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# att_dis Metric : Distance from object as proxy for interaction\n",
    "<a id=\"att_dis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"att_dis_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide att_dis code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset.categories\n",
    "names = dataset.labels_to_names\n",
    "distances = pickle.load(open(\"results/{}/att_dis.pkl\".format(folder_name), \"rb\"))\n",
    "topn = 10\n",
    "\n",
    "f_all_dists = []\n",
    "m_all_dists = []\n",
    "f_distr_dists = []\n",
    "m_distr_dists = []\n",
    "p_values = []\n",
    "file_paths_f = []\n",
    "file_paths_m = []\n",
    "indices_f = []\n",
    "indices_m = []\n",
    "# Calculates the distances from person to each instance, and if it's a statistically significant difference\n",
    "for i in range(len(categories)):\n",
    "    f_dist = np.array([ (chunk[0] / np.sqrt(chunk[1]*chunk[2])) for chunk in distances[i][0] if chunk[2] != 0 and chunk[1] != 0])\n",
    "    m_dist = np.array([ (chunk[0] / np.sqrt(chunk[1]*chunk[2])) for chunk in distances[i][1] if chunk[2] != 0 and chunk[1] != 0])\n",
    "    f_file = np.array([ chunk[3] for chunk in distances[i][0] if chunk[2] != 0 and chunk[1] != 0])\n",
    "    m_file = np.array([ chunk[3] for chunk in distances[i][1] if chunk[2] != 0 and chunk[1] != 0])\n",
    "    f_index = np.array([ chunk[4] for chunk in distances[i][0] if chunk[2] != 0 and chunk[1] != 0])\n",
    "    m_index = np.array([ chunk[4] for chunk in distances[i][1] if chunk[2] != 0 and chunk[1] != 0])\n",
    "    t, p = stats.ttest_ind(f_dist, m_dist)\n",
    "    p_values.append(p)\n",
    "    f_distr_dists.append(f_dist)\n",
    "    m_distr_dists.append(m_dist)\n",
    "    f_all_dists.append(np.mean(f_dist))\n",
    "    m_all_dists.append(np.mean(m_dist))\n",
    "    file_paths_f.append(f_file)\n",
    "    file_paths_m.append(m_file)\n",
    "    indices_f.append(f_index)\n",
    "    indices_m.append(m_index)\n",
    "f_all_dists, m_all_dists = np.array(f_all_dists), np.array(m_all_dists)\n",
    "\n",
    "xaxis = [names[i] for i in categories]\n",
    "\n",
    "# Only retaining inforamtion for distances that are statistically significantly different, and are not people\n",
    "indices_to_keep = [i for i in range(len(f_all_dists)) if categories[i] not in dataset.people_labels]\n",
    "f_sig_dists = f_all_dists[indices_to_keep]\n",
    "m_sig_dists = m_all_dists[indices_to_keep]\n",
    "f_distr_dists = np.array(f_distr_dists)[indices_to_keep]\n",
    "m_distr_dists = np.array(m_distr_dists)[indices_to_keep]\n",
    "file_paths_f = np.array(file_paths_f)[indices_to_keep]\n",
    "file_paths_m = np.array(file_paths_m)[indices_to_keep]\n",
    "indices_f = np.array(indices_f)[indices_to_keep] \n",
    "indices_m = np.array(indices_m)[indices_to_keep] \n",
    "xaxis_sig = np.array(xaxis)[indices_to_keep]\n",
    "pvalues_sig = np.array(p_values)[indices_to_keep]\n",
    "\n",
    "all_ratios = []\n",
    "for i in range(len(f_sig_dists)):\n",
    "    f_count = f_sig_dists[i]\n",
    "    m_count = m_sig_dists[i]\n",
    "    if f_count > m_count:\n",
    "        all_ratios.append(f_count / m_count)\n",
    "    else:\n",
    "        all_ratios.append(m_count / -f_count)\n",
    "    if np.absolute(all_ratios[-1]) <= 1:\n",
    "        all_ratios[-1] = 0\n",
    "all_ratios = np.array(all_ratios)\n",
    "\n",
    "distance_ratio_options = [(\"{0}: p-value {1}, {2}x\".format(xaxis_sig[i], '{:0.2e}'.format(pvalues_sig[i]), round(all_ratios[i], 3)), i) for i in np.argsort(pvalues_sig)]\n",
    "\n",
    "def show_distance_ratios(sort_by, topn):\n",
    "\n",
    "    if sort_by == 'pvalue':\n",
    "        top_indices = np.argsort(pvalues_sig)[:topn][::-1]\n",
    "    elif sort_by == 'ratio':\n",
    "        top_indices = np.argsort(np.absolute(all_ratios))[-topn:]\n",
    "        \n",
    "    fontsize = 10\n",
    "    fig = plt.figure(figsize=(8, max(2, topn // 3)))\n",
    "    \n",
    "    all_ratios_topn = all_ratios[top_indices]\n",
    "    xaxis_sig_topn = xaxis_sig[top_indices]\n",
    "\n",
    "    negatives = np.where(all_ratios_topn > 0)[0]\n",
    "    pltbar = plt.barh(np.arange(topn), np.absolute(all_ratios_topn), tick_label=xaxis_sig_topn)\n",
    "    for neg in negatives:\n",
    "        pltbar[neg].set_color('C1')\n",
    "    patch1 = mpatches.Patch(color='C0', label='female')\n",
    "    patch2 = mpatches.Patch(color='C1', label='male')\n",
    "    handles = [patch1, patch2]\n",
    "    fontP = FontProperties()\n",
    "    fontP.set_size(12)\n",
    "    lgd = plt.legend(handles=handles, prop=fontP)\n",
    "    plt.xlabel('Categories', fontsize=fontsize)\n",
    "    plt.ylabel('Distance Ratio', fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.tight_layout()        \n",
    "    plt.show()\n",
    "\n",
    "distanceratio_slider = widgets.IntSlider(min=5, max=50, step=1, value=10)\n",
    "distanceratio_sortby = widgets.Dropdown(options=['pvalue', 'ratio'], value='pvalue')\n",
    "\n",
    "def set_box_color(bp, color):\n",
    "    plt.setp(bp['boxes'], color=color)\n",
    "    plt.setp(bp['whiskers'], color=color)\n",
    "    plt.setp(bp['caps'], color=color)\n",
    "    plt.setp(bp['medians'], color=color)\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "    save_box_images(bpf, f, file_paths_f[i*20:(i+1)*20], indices_f[i*20:(i+1)*20], \n",
    "        bpm, m, file_paths_m[i*20:(i+1)*20], indices_m[i*20:(i+1)*20],\n",
    "        xaxis_sig[i*20:(i+1)*20], boxfolder)\n",
    "\n",
    "# Saves qualitative images for each of the quartiles of distance\n",
    "def save_box_images(bpf, f_array, file_paths_f, indices_f, bpm, m_array, file_paths_m, indices_m, names, to_save=False):\n",
    "    a = bpf['whiskers'][0].get_data()[1][1]\n",
    "    b = bpf['whiskers'][0].get_data()[1][0]\n",
    "    c = bpf['medians'][0].get_data()[1][0]\n",
    "    d = bpf['whiskers'][1].get_data()[1][0]\n",
    "    e = bpf['whiskers'][1].get_data()[1][1]\n",
    "    box_nums_f = [a, b, c, d, e]\n",
    "    a = bpm['whiskers'][0].get_data()[1][1]\n",
    "    b = bpm['whiskers'][0].get_data()[1][0]\n",
    "    c = bpm['medians'][0].get_data()[1][0]\n",
    "    d = bpm['whiskers'][1].get_data()[1][0]\n",
    "    e = bpm['whiskers'][1].get_data()[1][1]\n",
    "    box_nums_m = [a, b, c, d, e]\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    for j in range(5):\n",
    "        for k in range(2):\n",
    "            if k == 0:\n",
    "                index = find_nearest(f_array, box_nums_f[j])\n",
    "                indices = indices_f\n",
    "                file_paths = file_paths_f\n",
    "                gender = 'f'\n",
    "            else:\n",
    "                index = find_nearest(m_array, box_nums_m[j])\n",
    "                indices = indices_m\n",
    "                file_paths = file_paths_m\n",
    "                gender = 'm'\n",
    "            file_path = file_paths[index]\n",
    "            ann_index = indices[index]\n",
    "            image, anns = dataset.from_path(file_path)\n",
    "            image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "            ann = anns[0][ann_index]['bbox']\n",
    "            ann_0 = (ann[0]*image.shape[1], ann[2]*image.shape[0])\n",
    "            ann_w = (ann[1]-ann[0])*image.shape[1]\n",
    "            ann_h = (ann[3]-ann[2])*image.shape[0]\n",
    "            per = anns[1][1]\n",
    "            per_0 = (per[0]*image.shape[1], per[2]*image.shape[0])\n",
    "            per_w = (per[1]-per[0])*image.shape[1]\n",
    "            per_h = (per[3]-per[2])*image.shape[0]\n",
    "\n",
    "            if k == 0:\n",
    "                ax = fig.add_subplot(2, 5, j+1)\n",
    "            else:\n",
    "                ax = fig.add_subplot(2, 5, j+6)\n",
    "            rect = patches.Rectangle(ann_0,ann_w, ann_h, linewidth=2,edgecolor='b',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            rect = patches.Rectangle(per_0,per_w, per_h, linewidth=2,edgecolor='r',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            title = 'min'\n",
    "            if j == 1:\n",
    "                title = 'lower quartile'\n",
    "            elif j == 2:\n",
    "                title = 'median'\n",
    "            elif j == 3:\n",
    "                title = 'upper quartile'\n",
    "            elif j == 4:\n",
    "                title = 'max'\n",
    "            ax.set_title(gender.upper() + ': ' + title, fontsize=22)\n",
    "            ax.axis(\"off\")\n",
    "            im = ax.imshow(image, alpha=.66)\n",
    "\n",
    "    if to_save:\n",
    "        if np.mean(f_array) < np.mean(m_array):\n",
    "            closer_sent = 'Females are closer than males.'\n",
    "        else:\n",
    "            closer_sent = 'Males are closer than females.'\n",
    "        to_write[5] = [\"(att_dis) Qualitative example of {0}, which has the biggest ratio in distance between object and person (which can be interpreted as a proxy for interaction) between the genders. \".format(names) + closer_sent + \" There is a red box around the person, and blue box around the object.\"]\n",
    "        plt.savefig(\"results/{0}/{1}/3.png\".format(folder_name, save_loc))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def qualitative_boxexamples(cat_index):\n",
    "    try: \n",
    "        to_save = False\n",
    "        if cat_index is None and first_pass:\n",
    "            to_save = True\n",
    "            cat_index = distance_ratio_options[0][1]\n",
    "\n",
    "        f = f_distr_dists[cat_index]\n",
    "        m = m_distr_dists[cat_index]\n",
    "\n",
    "        bpf = plt.boxplot(f, sym='', widths=0.6)\n",
    "        bpm = plt.boxplot(m, sym='', widths=0.6)\n",
    "        plt.close()\n",
    "\n",
    "        save_box_images(bpf, f, file_paths_f[cat_index], indices_f[cat_index], \n",
    "            bpm, m, file_paths_m[cat_index], indices_m[cat_index],\n",
    "            xaxis_sig[cat_index], to_save)\n",
    "    except (AttributeError, FileNotFoundError): \n",
    "        print('Some functionality not available for CoCoDatasetNoImages Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"att_dis_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of distance between an object and person by gender, sorted by p-value or ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_things = [distanceratio_sortby, distanceratio_slider]\n",
    "ui = HBox(all_things)\n",
    "out = widgets.interactive_output(show_distance_ratios, {'sort_by': distanceratio_sortby, 'topn': distanceratio_slider})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitative examples of person (red box) of each gender with the object (red box), representing the different quartiles of distance. The values in the dropdown menu represent the p-value for the independence of the female and male distance distributions, and the ratio is that of distance between male and female, where negative means it is closer for females, and positive means it is closer for males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_pass:\n",
    "    qualitative_boxexamples(None)\n",
    "interact(qualitative_boxexamples, cat_index=widgets.Dropdown(options=distance_ratio_options, layout=Layout(width='400px')));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# att_clu Metric: Linearly separable objects by gender\n",
    "<a id=\"att_clu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"att_clu_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide att_clu code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"results/{0}/4/\".format(folder_name)):\n",
    "    os.mkdir(\"results/{0}/4/\".format(folder_name))\n",
    "categories = dataset.categories\n",
    "names = dataset.labels_to_names\n",
    "stats_dict = pickle.load(open(\"results/{}/att_clu.pkl\".format(folder_name), \"rb\"))\n",
    "instances = stats_dict['instance']\n",
    "scenes = stats_dict['scene']\n",
    "scene_filepaths = stats_dict['scene_filepaths']\n",
    "\n",
    "file_name = 'categories_places365.txt'\n",
    "if not os.access(file_name, os.W_OK):\n",
    "    synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt'\n",
    "    os.system('wget ' + synset_url)\n",
    "classes = list()\n",
    "with open(file_name) as class_file:\n",
    "    for line in class_file:\n",
    "        classes.append(line.strip().split(' ')[0][3:])\n",
    "scene_classes = tuple(classes)\n",
    "\n",
    "topn = 15\n",
    "\n",
    "plot_kwds = {'alpha' : .8, 's' : 30, 'linewidths':0}\n",
    "\n",
    "instance_p_values = []\n",
    "scene_p_values = []\n",
    "\n",
    "file.write(\"SVM accuracies\\n\")\n",
    "\n",
    "if not os.path.exists(\"checkpoints/{}/att_clu.pkl\".format(folder_name)):\n",
    "    value_to_phrase = {}\n",
    "    value_to_scenephrase = {}\n",
    "    for i in range(len(categories)):\n",
    "        # SVM's to classify between an object's features for the genders\n",
    "        clf = svm.SVC(kernel='linear', probability=False)\n",
    "        clf_prob = svm.SVC(kernel='linear', probability=True)\n",
    "        if len(instances[i][0]) < 1 or len(instances[i][1]) < 1 or len(scenes[i][0]) < 1 or len(scenes[i][1]) < 1:\n",
    "            scene_p_values.append(float('inf'))\n",
    "            instance_p_values.append(float('inf'))\n",
    "            continue\n",
    "        features_instances = np.concatenate([instances[i][0], instances[i][1]], axis=0)\n",
    "        boundary_instances = len(instances[i][0])\n",
    "        features_scenes = np.concatenate([scenes[i][0], scenes[i][1]], axis=0)\n",
    "        boundary_scenes = len(scenes[i][0])\n",
    "\n",
    "        # Uncomment to visualize features of cropped object, saved as a png\n",
    "        # projection_instances = TSNE().fit_transform(features_instances)\n",
    "        # plt.scatter(*projection_instances.T, **plot_kwds, c=[COLORS[0] if i < boundary_instances else COLORS[1] for i in range(len(projection_instances))])\n",
    "        # plt.savefig(\"results/{0}/{1}/instances_{2}.png\".format(folder_name, 4, i))\n",
    "        # plt.close()\n",
    "\n",
    "        t, p = stats.ttest_ind(instances[i][0], instances[i][1])\n",
    "        instance_p_values.append(np.nanmean(p))\n",
    "\n",
    "        # Uncomment to visualize features of entire scene, saved as a png\n",
    "        # projection_scenes = TSNE().fit_transform(features_scenes)\n",
    "        # plt.scatter(*projection_scenes.T, **plot_kwds, c=[COLORS[0] if i < boundary_scenes else COLORS[1] for i in range(len(projection_scenes))])\n",
    "        # plt.savefig(\"results/{0}/{1}/scenes_{2}.png\".format(folder_name, 4, i))\n",
    "        # plt.close()\n",
    "\n",
    "        t, p = stats.ttest_ind(scenes[i][0], scenes[i][1])\n",
    "        scene_p_values.append(np.nanmean(p))\n",
    "        num_features = int(np.sqrt(len(features_scenes)))\n",
    "\n",
    "        labels = np.zeros(len(features_scenes))\n",
    "        labels[len(scenes[i][0]):] = 1\n",
    "        projected_features_scenes = StandardScaler().fit_transform(project(features_scenes, num_features))\n",
    "\n",
    "        clf.fit(projected_features_scenes, labels)\n",
    "        clf_prob.fit(projected_features_scenes, labels)\n",
    "        acc = clf.score(projected_features_scenes, labels)\n",
    "        probs = clf.decision_function(projected_features_scenes)\n",
    "        scaled_probs = clf_prob.predict_proba(projected_features_scenes)\n",
    "        a_probs = []\n",
    "        b_probs = []\n",
    "        scenes_per_gender = [[[], []] for i in range(len(scene_classes))]\n",
    "        for j in range(len(features_scenes)):\n",
    "\n",
    "            if j < len(scenes[i][0]):\n",
    "                a_probs.append(-probs[j])\n",
    "                this_scene = scene_filepaths[i][0][j][1]\n",
    "                scenes_per_gender[this_scene][0].append(np.absolute(scaled_probs[j][0]))\n",
    "            else:\n",
    "                b_probs.append(probs[j])\n",
    "                this_scene = scene_filepaths[i][1][j - len(scenes[i][0])][1]\n",
    "                scenes_per_gender[this_scene][1].append(np.absolute(scaled_probs[j][1]))\n",
    "        a_indices = np.argsort(np.array(a_probs))\n",
    "        b_indices = np.argsort(np.array(b_probs))\n",
    "\n",
    "        pickle.dump([a_indices, b_indices, scene_filepaths[i], a_probs, b_probs], open(\"results/{0}/att_clu/{1}_info.pkl\".format(folder_name, names[categories[i]]), \"wb\"))\n",
    "\n",
    "        base_acc, rand_acc, p_value = permutation_test_score(clf, projected_features_scenes, labels, scoring=\"accuracy\", n_permutations=100)\n",
    "        ratio = base_acc/np.mean(rand_acc)\n",
    "        \n",
    "        if p_value > 0.05 and ratio <= 1.2: # can tune as desired\n",
    "            continue\n",
    "            \n",
    "        amount = len(features_instances)\n",
    "        phrase = [ratio, names[categories[i]], acc, p_value, len(features_instances), num_features]\n",
    "        value_to_phrase[i] = phrase\n",
    "\n",
    "        for j in range(len(scene_classes)):\n",
    "            a_dists = scenes_per_gender[j][0]\n",
    "            b_dists = scenes_per_gender[j][1]\n",
    "            a = np.zeros(len(scenes[i][0]))\n",
    "            a[:len(a_dists)] = 1 #a_dists\n",
    "            b = np.zeros(len(scenes[i][1]))\n",
    "            b[:len(b_dists)] = 1 #b_dists\n",
    "            _, p = stats.ttest_ind(a, b)\n",
    "            if not np.isnan(p):\n",
    "                value_to_scenephrase[p] = [names[categories[i]], scene_classes[j], len(a_dists), len(a), len(b_dists), len(b)]\n",
    "    pickle.dump([value_to_phrase, value_to_scenephrase], open(\"checkpoints/{}/att_clu.pkl\".format(folder_name), 'wb'))\n",
    "else:\n",
    "    value_to_phrase, value_to_scenephrase = pickle.load(open(\"checkpoints/{}/att_clu.pkl\".format(folder_name), 'rb'))\n",
    "\n",
    "def label_svm_qual(category, num):\n",
    "    to_save = False\n",
    "    ratio, name, acc, p_value, num_examples, num_features = value_to_phrase[category]\n",
    "    a_indices, b_indices, scene_filepaths, a_probs, b_probs = pickle.load(open(\"results/{0}/att_clu/{1}_info.pkl\".format(folder_name, name), \"rb\"))\n",
    "    print_statement = \"{3}: Accuracy: {0}%, with p={1}, {2}x and {3} features\\n\".format(round(acc*100., 3), round(p_value, 3), round(ratio, 3), num_features, name)\n",
    "    if num is None and first_pass:\n",
    "        to_save = True\n",
    "        num = 5\n",
    "        to_write[6] = [\"(att_clu) To discern if there is an appearance difference in how females and males are imaged with an object, we extract scene-level features from each image, and fit a linear SVM to distinguish between the two.\\nAn example of the most linearly separable object between genders: {}\".format(name), print_statement]\n",
    "    else:\n",
    "        print(print_statement)\n",
    "    \n",
    "    the_indices = [a_indices, b_indices]\n",
    "    the_probs = [a_probs, b_probs]\n",
    "    \n",
    "    def display_chunk(b=0, correct=True, to_save=False, name=None):\n",
    "        this_filepaths = scene_filepaths[b]\n",
    "        this_indices = the_indices[b]\n",
    "        this_probs = the_probs[b]\n",
    "        collected_filepaths = []\n",
    "        \n",
    "        if correct:\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter = -1\n",
    "        while len(collected_filepaths) < num:\n",
    "            try:\n",
    "                index = this_indices[counter]\n",
    "            except:\n",
    "                break\n",
    "            file_path = this_filepaths[index][0]\n",
    "            if (this_probs[index] > 0 and correct) or (this_probs[index] < 0 and not correct):\n",
    "                collected_filepaths.append(file_path)\n",
    "            if correct:\n",
    "                counter += 1\n",
    "            else:\n",
    "                counter -= -1\n",
    "        if to_save and first_pass:\n",
    "            this_loc = \"results/{0}/{1}/att_clu_{2}.png\".format(folder_name, save_loc, name)\n",
    "            if len(collected_filepaths) > 0:\n",
    "                fig = plt.figure(figsize=(16, 8))\n",
    "                for i in range(num):\n",
    "                    ax = fig.add_subplot(1, num, i+1)\n",
    "                    ax.axis(\"off\")\n",
    "                    if i >= len(collected_filepaths):\n",
    "                        image = np.ones((3, 3, 3))\n",
    "                    else:\n",
    "                        image, _ = dataset.from_path(collected_filepaths[i])\n",
    "                        image = image.data.cpu().numpy().transpose(1, 2, 0)\n",
    "                    im = ax.imshow(image, extent=SAME_EXTENT)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(this_loc, bbox_inches = 'tight')\n",
    "                plt.close()\n",
    "            else:\n",
    "                os.system(\"cp util_files/no_images.png {}\".format(this_loc))\n",
    "        elif len(collected_filepaths) > 0:\n",
    "            display_filepaths(collected_filepaths, width = 800//len(collected_filepaths), height=800//len(collected_filepaths))\n",
    "        else:\n",
    "            print(\"No images in this category\")\n",
    "    if not to_save:\n",
    "        print(\"Female: Correct\")\n",
    "    else:\n",
    "        to_write[6].append(\"Female: Correct\")\n",
    "    display_chunk(0, True, to_save, 'a')\n",
    "    if not to_save:\n",
    "        print(\"Female: Incorrect\")\n",
    "    else:\n",
    "        to_write[6].append(\"Female: Incorrect\")\n",
    "    display_chunk(0, False, to_save, 'b')\n",
    "    if not to_save:\n",
    "        print(\"Male: Incorrect\")\n",
    "    else:\n",
    "        to_write[6].append(\"Male: Incorrect\")\n",
    "    display_chunk(1, False, to_save, 'c')\n",
    "    if not to_save:\n",
    "        print(\"Male: Correct\")\n",
    "    else:\n",
    "        to_write[6].append(\"Male: Correct\")\n",
    "    display_chunk(1, True, to_save, 'd')\n",
    "\n",
    "cat_svm_options = []\n",
    "most_different_cat_value = 1.2\n",
    "most_different_cat = None\n",
    "for index, phrase in sorted(value_to_phrase.items(), key=lambda kv: kv[1][0], reverse=True):\n",
    "    ratio, name, acc, p_value, num_examples, num_features = value_to_phrase[index]\n",
    "    if acc > .75 and ratio > most_different_cat_value and num_features > 5:\n",
    "        most_different_cat_value = ratio\n",
    "        most_different_cat = index\n",
    "    if num_features > 5:\n",
    "        cat_svm_options.append(('{0}: {1}% and {2}x'.format(name, round(100.*acc, 2), round(ratio, 3)), index))\n",
    "\n",
    "def instance_diff_by_scene(topn):\n",
    "    print(\"\\nInstance differences by scene between genders\\n\")\n",
    "    i = 0\n",
    "    for value, phrase in sorted(value_to_scenephrase.items(), key=lambda kv: kv[0], reverse=False):\n",
    "        print(\"{0} p-value, {1}: in {2} for {3} of {4} females, and {5} of {6} males\".format(dec_to_show(value), phrase[0], phrase[1], phrase[2], phrase[3], phrase[4], phrase[5]))\n",
    "        if i == topn:\n",
    "            break\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"att_clu_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitative examples of the most linearly separable objects once featurized, by gender. Permutation test p-values are given to check for random overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qual_widget = widgets.IntSlider(min=1, max=20, step=1, value=5)\n",
    "cat_choice_widget = widgets.Dropdown(options=cat_svm_options, layout=Layout(width='400px'))\n",
    "all_things = [widgets.Label('Category, p_value, acc',layout=Layout(padding='0px 0px 0px 5px', width='200px')), cat_choice_widget, widgets.Label('Num',layout=Layout(padding='0px 5px 0px 40px', width='80px')), num_qual_widget]\n",
    "\n",
    "if first_pass and most_different_cat is not None:\n",
    "    label_svm_qual(most_different_cat, None)\n",
    "ui = HBox(all_things)\n",
    "out = widgets.interactive_output(label_svm_qual, {'category': cat_choice_widget, 'num': num_qual_widget})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences in scene between gender for an object. Analyses on random sample of up to 500 for each gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(instance_diff_by_scene, topn=widgets.IntSlider(min=1, max=30, step=1, value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# att_scn Metric: Scenes\n",
    "<a id=\"att_scn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"att_scn_setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(for_next=True, toggle_text='Show/hide att_scn code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_info = pickle.load(open('results/{}/att_scn.pkl'.format(folder_name), 'rb'))\n",
    "scenes_per = stats_info['scenes_per']\n",
    "\n",
    "info = pickle.load(open('util_files/places_scene_info.pkl', 'rb'))\n",
    "idx_to_scene = info['idx_to_scene']\n",
    "idx_to_scenegroup = info['idx_to_scenegroup']\n",
    "sceneidx_to_scenegroupidx = info['sceneidx_to_scenegroupidx']\n",
    "\n",
    "xaxis = [idx_to_scenegroup[i] for i in range(len(idx_to_scenegroup))]\n",
    "xaxis = ['\\n'.join(textwrap.wrap(chunk, width=30)) for chunk in xaxis]\n",
    "barWidth = .4\n",
    "fontsize=10\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "r1 = np.arange(len(scenes_per))\n",
    "scenes_f = np.array([scenes_per[i][0] for i in range(len(r1))]) / dataset.num_gender_images[0]\n",
    "scenes_m = np.array([scenes_per[i][1] for i in range(len(r1))]) / dataset.num_gender_images[1]\n",
    "\n",
    "def show_scenes():\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    order = np.argsort(np.divide(scenes_m, scenes_f))\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    plt.barh(r2, scenes_m[order], height=barWidth, color=COLORS[1], edgecolor='white', label='male')\n",
    "    plt.barh(r1, scenes_f[order], height=barWidth, color=COLORS[0], edgecolor='white', label='female')\n",
    "    plt.yticks([r + (barWidth/2.) for r in range(len(r1))], np.array(xaxis)[order], fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.ylabel('Scene', fontsize=fontsize, labelpad=20)\n",
    "    plt.xlabel('Fraction of Images with this Scene', fontsize=fontsize, labelpad=20, x=.3)\n",
    "    plt.legend(loc='best', fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "    plt.gcf().subplots_adjust(bottom=0.15)\n",
    "    plt.gcf().subplots_adjust(left=0.5)\n",
    "    if first_pass:\n",
    "        to_write[7] = [\"(att_scn) Distribution of scenes that each gender appears in, sorted by ratio between the two.\"]\n",
    "        plt.savefig(\"results/{0}/{1}/5.png\".format(folder_name, save_loc))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "<a id=\"att_scn_analyses\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenes that each gender appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scenes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up summary pdf\n",
    "<a id=\"summarypdf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pass = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pdf(numbers):\n",
    "    for i in numbers:\n",
    "        if i in to_write.keys():\n",
    "            if i not in [1, 6]:\n",
    "                for sentence in to_write[i]:\n",
    "                    pdf.write(5, sentence)\n",
    "                    pdf.ln()\n",
    "            if i == 0:\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[1][0])\n",
    "                pdf.ln()\n",
    "            elif i == 1:\n",
    "                continue\n",
    "            elif i == 2:\n",
    "                pdf.image('results/{0}/{1}/0.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 3:\n",
    "                pdf.image('results/{0}/{1}/1.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 4:\n",
    "                pdf.image('results/{0}/{1}/2.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "            elif i == 5:\n",
    "                pdf.image('results/{0}/{1}/3.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            elif i == 6:\n",
    "                pdf.write(5, to_write[i][0])\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][1])\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][2])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/4_a.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][3])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/4_b.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][4])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/4_c.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "                pdf.write(5, to_write[i][5])\n",
    "                pdf.ln()\n",
    "                pdf.image('results/{0}/{1}/4_d.png'.format(folder_name, save_loc), w=160)\n",
    "                pdf.ln()\n",
    "            elif i == 7:\n",
    "                pdf.image('results/{0}/{1}/5.png'.format(folder_name, save_loc), h=80)\n",
    "                pdf.ln()\n",
    "            pdf.ln(h=3)\n",
    "            pdf.dashed_line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "            pdf.ln(h=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font('Arial', 'B', 16)\n",
    "pdf.write(5, \"Gender-Based Summary\")\n",
    "pdf.ln()\n",
    "pdf.ln()\n",
    "\n",
    "# Overview Statistics\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Overview Statistics\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "write_pdf([3, 7])\n",
    "\n",
    "# Interesting findings\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Sample Interesting Findings\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "write_pdf([0, 1, 2, 4, 5, 6])\n",
    "\n",
    "# Interesting findings\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "pdf.write(5, \"Some of the other metrics in the notebook\")\n",
    "pdf.ln()\n",
    "pdf.ln(h=3)\n",
    "pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
    "pdf.ln(h=3)\n",
    "pdf.set_font('Arial', '', 12)\n",
    "pdf.write(5, \"- (att_cnt) Cooccurrence differences of objects between genders\")\n",
    "pdf.ln()\n",
    "pdf.write(5, \"- (att_clu) Scene differences per object between genders\")\n",
    "pdf.ln()\n",
    "\n",
    "\n",
    "pdf.output('results/{0}/{1}/summary.pdf'.format(folder_name, save_loc), \"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
